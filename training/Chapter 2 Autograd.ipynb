{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from platform import python_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Introduction to Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd package is an engine to calculate derivatives which is Jacobian-vector product. It provides automatic differentiation for all operations on Tensors. It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are nothing more than composite mathematical functions that are delicately tweaked (trained) to output the required result. The tweaking or the training is done through a remarkable algorithm called backpropagation. Backpropagation is used to calculate the gradients of the loss with respect to the input weights to later update the weights and eventually reduce the loss. With autograd, we can skip all the steps to manually calculate our gradients. This can save us time and energy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to perform Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without autograd\n",
    "t_1 = torch.randn(5)\n",
    "print(f'Without autograd: {t_1}')\n",
    "\n",
    "# with autograd\n",
    "t_2 = torch.randn(5, requires_grad=True)\n",
    "print(f'With autograd: {t_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_add = t_1 + t_2\n",
    "print(f'Addition of tensor: {t_add}')\n",
    "print(f'Addition of tensor with autograd has attribute of requires_grad: {t_add.requires_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, grad_fn has attribute of `AddBackward` because our tensor operation is addition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sub = t_1 - t_2\n",
    "t_mul = t_1 * t_2\n",
    "t_mean = t_add.mean()\n",
    "t_sig = t_add.sigmoid()\n",
    "print(f'Subtraction of tensor with autograd has attribute of requires_grad: {t_sub.requires_grad}')\n",
    "print(f'Multiplication of tensor with autograd has attribute of requires_grad: {t_mul.requires_grad}')\n",
    "print(f'Mean of tensor with autograd has attribute of requires_grad: {t_mean.requires_grad}')\n",
    "print(f'Sigmoid of tensor with autograd has attribute of requires_grad: {t_sig.requires_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How grad is stored in tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient(s) are calculated automatically by calling `.backward()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mean.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Gradient of t_1 without autograd: {t_1.grad}')\n",
    "print(f'Gradient of t_2 with autograd: {t_2.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> grad can be implicitly created only for **_scalar_** outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically need to multiply with vector to produce scalar output, which is Jacobian product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to exclude gradient calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes during our training loop when we want to update our weights, then this operation should not be part of the gradient computation. Therefore, we need to exclude gradient calculation. We can do it with 3 ways:\n",
    "- `x.requires_grad_(False)`\n",
    "- `x.detach()`\n",
    "- `with torch.no_grad():`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1 = torch.randn(5, requires_grad=True)\n",
    "print(f'Autograd tensor: {t_1}')\n",
    "\n",
    "t_1.requires_grad_(False)\n",
    "print(f'Without autograd tensor: {t_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detach() will create new tensor with same value but it doesn't require the gradient\n",
    "\n",
    "t_2 = t_1.detach()\n",
    "print(f't_2: {t_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1 = torch.randn(5, requires_grad=True)\n",
    "print(f'Autograd tensor: {t_1}')\n",
    "\n",
    "t_1.detach_()\n",
    "print(f'Inplace detach tensor: {t_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_1 = torch.randn(5, requires_grad=True)\n",
    "print(f't_1: {t_1}')\n",
    "\n",
    "t_ans = t_1 + 2\n",
    "print(f't_ans: {t_ans}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    t_ans = t_1 + 2\n",
    "    print(f't_ans: {t_ans}')\n",
    "print(f'Final t_ans: {t_ans}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that whenever we call `.backward()`, the gradient for this tensor will be accumulated in `.grad` attribute. As the result their values will be summed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([1., 2., 3., 4., 5.], requires_grad=True)\n",
    "for epoch in range(5):\n",
    "    output = (weights*2).sum()\n",
    "    output.backward()\n",
    "    print(f'Epoch {epoch}: {weights.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the gradients are summed up and our weights or gradients are clearly incorrect. Before we do the next iteration step and optimization step, we must empty the gradient so we must call `.grad.zero_()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([1., 2., 3., 4., 5.], requires_grad=True)\n",
    "for epoch in range(5):\n",
    "    output = (weights*2).sum()\n",
    "    output.backward()\n",
    "    print(f'Epoch {epoch}: {weights.grad}')\n",
    "    weights.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Linear Regression Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General linear regression function: $$y = wX + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, let our function $f(x) = 4x$, for now just ignore bias $b$. First we will generate dummy data of $X$ and $y$. Below is the plot of our data which the function $f(x) = 4x$. We will use linear regression to find the weight/gradient which should be $w = 4$ and predict the $y$ value respective to $x$ value of 250 which should be $y = 1000$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([2, 5, 7, 10, 15], dtype=np.float32)\n",
    "y = np.array([8, 20, 28, 40, 60], dtype=np.float32)\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to do linear regression. Here we will show you 3 ways to do so which are:\n",
    "- Linear Regression using **NumPy** \n",
    "- Linear Regression using **PyTorch** without **_Autograd_**\n",
    "- Linear Regression using **PyTorch** with **_Autograd_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets initiate weight to zero and some of our hyperparameters at the beginning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 0.0\n",
    "learning_rate = 0.001\n",
    "n_iters = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `forward()` function to return model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X):\n",
    "    return w * X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `lossMSE()` for our loss function. In this case, we will use **Mean Square Error** for our loss function.\n",
    "$$MSE = \\frac {1}{N}(Y\\_Pred- Y)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossMSE(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `gradient()` function to return gradient of the loss with respect to our parameters.\n",
    "$$\\frac{dJ}{dw} = \\frac{1}{N}(2X)(Y\\_Pred-Y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(X, y, y_pred):\n",
    "    return np.dot(2*X, y_pred-y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we first make use the linear regression to predict $f(250)$ before the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Prediction before training: f(250) = {forward(250):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train our linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_iters):\n",
    "    # forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    loss = lossMSE(y, y_pred)\n",
    "    \n",
    "    # backward pass\n",
    "    dw = gradient(X, y, y_pred)\n",
    "    \n",
    "    # update weights\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: weight = {w:.5f}, loss = {loss:.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Prediction after training: f(250) = {forward(250):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using PyTorch without Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use PyTorch to train our linear regression model. First, we convert our dataset $X$, $y$ from numpy array to pytorch tensor using `torch.from_numpy()` and initiate tensor $w$ with zero using `torch.tensor()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y)\n",
    "w = torch.zeros(1, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define some functions for our linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X):\n",
    "    return w * X\n",
    "def lossMSE(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "def gradient(X, y, y_pred):\n",
    "    return torch.matmul(2*X, y_pred-y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define our linear regression function without autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegressionNoAutograd(X, y, w):\n",
    "    print(f'Prediction before training: f(250) = {forward(250).item():.3f}')\n",
    "    for epoch in range(n_iters):\n",
    "        y_pred = forward(X)\n",
    "        loss = lossMSE(y, y_pred)\n",
    "        dw = gradient(X, y, y_pred)\n",
    "        w -= learning_rate * dw\n",
    "        print(f'Epoch {epoch+1}: weight = {w.item():.5f}, loss = {loss.item():.10f}')\n",
    "    print(f'Prediction after training: f(250) = {forward(250).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearRegressionNoAutograd(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression using PyTorch with Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `autograd`, we no need to manually calculate gradient anymore with `.backward()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.zero_()\n",
    "w.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define our linear regression function with autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegressionAutograd(X, y, w):\n",
    "    print(f'Prediction before training: f(250) = {forward(250).item():.3f}')\n",
    "    for epoch in range(n_iters):\n",
    "        y_pred = forward(X)\n",
    "        loss = lossMSE(y, y_pred)\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            w -= learning_rate * w.grad\n",
    "        w.grad.zero_()\n",
    "        print(f'Epoch {epoch+1}: weight = {w.item():.5f}, loss = {loss.item():.10f}')\n",
    "    print(f'Prediction after training: f(250) = {forward(250).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearRegressionAutograd(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.backward()` back propagation function is not as exact as numerical gradient computation. So we may requires to tune our hyperparameters such as number of iterations and learning rates. Lets reset our weight to zero using `.storage()`. Note that we can't reset our $w$ using inplace operation.\n",
    "> PyTorch doesnâ€™t allow in-place operations on leaf variables that have `requires_grad=True` (such as parameters of your model) because the developers could not decide how such an operation should behave. If you want the operation to be differentiable, you can work around the limitation by cloning the leaf variable (or use a non-inplace version of the operator). Source: [PyTorch Forum](https://discuss.pytorch.org/t/leaf-variable-was-used-in-an-inplace-operation/308/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.005\n",
    "w.storage()[:] = 0\n",
    "linearRegressionAutograd(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Optimizer and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimizers** are algorithms or methods used to change the attributes of the neural network such as weights and learning rate to reduce the losses. Optimizers are used to solve optimization problems by minimizing the function. [Source](https://towardsdatascience.com/overview-of-various-optimizers-in-neural-networks-17c1be2df6d5)\n",
    "\n",
    "**Loss function** is a method of evaluating how well specific algorithm models the given data. If predictions deviates too much from actual results, loss function would cough up a very large number. Gradually, with the help of some optimization function, loss function learns to reduce the error in prediction. [Source](https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "lossMSE = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegressionAutogradOptimizerLoss(X, y, w):\n",
    "    print(f'Prediction before training: f(250) = {forward(250).item():.3f}')\n",
    "    for epoch in range(n_iters):\n",
    "        y_pred = forward(X)\n",
    "        loss = lossMSE(y, y_pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f'Epoch {epoch+1}: weight = {w.item():.5f}, loss = {loss.item():.10f}')\n",
    "    print(f'Prediction after training: f(250) = {forward(250).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.storage()[:] = 0\n",
    "linearRegressionAutogradOptimizerLoss(X, y, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=50, n_features=1, noise=2, random_state=428)\n",
    "X = torch.from_numpy(X).reshape(50, 1)\n",
    "y = torch.from_numpy(y).reshape(50, 1)\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Size of X: {X.shape}')\n",
    "print(f'Size of y: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Build a **linear regression model** to predict the $y$ value of 428 using **PyTorch** with autograd, optimizer and loss function.\n",
    "> **Challenge**: Convergence of model within 15 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.zeros(1, dtype=torch.float32, )\n",
    "learning_rate = 0\n",
    "n_iters = 0\n",
    "optimizer = None\n",
    "lossMSE = None\n",
    "def forward(X):\n",
    "    return None\n",
    "print(f'Prediction before training: f(428) = {forward(428).item():.3f}')\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = None\n",
    "    loss = None\n",
    "    loss.()\n",
    "    optimizer.()\n",
    "    optimizer.()\n",
    "    print(f'Epoch {epoch+1}: weight = {w.item():.5f}, loss = {loss.item():.10f}')\n",
    "print(f'Prediction after training: f(428) = {forward(428).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "weight = 29.835, loss = 3.494\n",
    "Prediction after training: f(428) = 12769.(approx)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
