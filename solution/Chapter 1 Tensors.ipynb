{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Introduction to Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensors** is the fundamental data structure, a multidimensional array in PyTorch. It is metaphorical relatable to the numpy array of Numpy. The difference between the two is that tensors are available in two modes: `CPU and GPU`. We will touch this mentioned feature at the end of this tutorial.\n",
    "<br>\n",
    "<br>\n",
    "There are a few ways to build a tensor:\n",
    "- `torch.arange()`\n",
    "- `torch.ones()`\n",
    "- `torch.zeros()`\n",
    "- `torch.rand()`\n",
    "- `torch.randint()`\n",
    "- `torch.fromnumpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.arange(n,m)` returns a 1-dimensional tensor with values ranging from $n$ to $m-1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(0,20)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.ones(dim_0,dim_1,...,dim_n)` returns a tensor of ones of the shape $ dim_{0} \\times dim_{1} \\times...\\times dim_{n} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones(3,4,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.zeros(dim_0,dim_1,...,dim_n)` returns a tensor of zeros of the shape $ dim_{0} \\times dim_{1} \\times...\\times dim_{n} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.zeros(3,4,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.rand(dim_0, dim_1,...,dim_m)` returns a tensor filled with random floats ranging from $0$ to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2285, 0.4276, 0.8271, 0.0772, 0.2831, 0.5630],\n",
      "         [0.7897, 0.6106, 0.0617, 0.0865, 0.3725, 0.1337],\n",
      "         [0.9258, 0.3545, 0.7063, 0.0228, 0.9724, 0.9281],\n",
      "         [0.7680, 0.3925, 0.1822, 0.3117, 0.4788, 0.1050]],\n",
      "\n",
      "        [[0.2714, 0.7694, 0.7631, 0.2606, 0.1142, 0.1005],\n",
      "         [0.8452, 0.9153, 0.8605, 0.1690, 0.1619, 0.7595],\n",
      "         [0.6912, 0.4550, 0.1193, 0.2934, 0.8134, 0.1963],\n",
      "         [0.0552, 0.2523, 0.2630, 0.9436, 0.2577, 0.6422]],\n",
      "\n",
      "        [[0.8944, 0.0695, 0.8053, 0.0021, 0.7835, 0.8281],\n",
      "         [0.2210, 0.2222, 0.9241, 0.9937, 0.8235, 0.8775],\n",
      "         [0.6407, 0.1401, 0.0781, 0.8441, 0.0758, 0.2169],\n",
      "         [0.0772, 0.0298, 0.2859, 0.7266, 0.5532, 0.6378]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand(3,4,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Mathematical Operations on Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Operations vs Inplace Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initialize $t_1$ and $t_2$ tensor and check their tensor storage location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: \n",
      "tensor([[0.3663, 0.2878],\n",
      "        [0.4093, 0.7040],\n",
      "        [0.7466, 0.4743]])\n",
      "t2: \n",
      "tensor([[0.8328, 0.8826],\n",
      "        [0.5961, 0.9508],\n",
      "        [0.1856, 0.5396]])\n",
      "\n",
      "Tensor storage location of t1: 1543902690176\n",
      "Tensor storage location of t2: 1543902686912\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand(3,2)\n",
    "t2 = torch.rand(3,2)\n",
    "print(\"t1: \\n\"+str(t1))\n",
    "print(\"t2: \\n\"+str(t2))\n",
    "\n",
    "loc_t1 = t1.data_ptr()\n",
    "loc_t2 = t2.data_ptr()\n",
    "print(\"\\nTensor storage location of t1: \" + str(loc_t1))\n",
    "print(\"Tensor storage location of t2: \" + str(loc_t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do normal addition on $t_1$ with $t_2$. Note that when you run the cell below everytime, the tensor storage location of $t_1$ will change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of normal addition: \n",
      "tensor([[1.1991, 1.1704],\n",
      "        [1.0054, 1.6549],\n",
      "        [0.9322, 1.0139]])\n",
      "Tensor storage location of t1: 1543902691840\n",
      "\n",
      "Is the tensor storage location of t1 before and after normal addition same: False\n"
     ]
    }
   ],
   "source": [
    "# Normal addition\n",
    "\n",
    "t1 = t1.add(t2)\n",
    "print(\"Value of normal addition: \\n\" + str(t1))\n",
    "loc_normalAdd = t1.data_ptr()\n",
    "print(\"Tensor storage location of t1: \" + str(loc_normalAdd))\n",
    "\n",
    "print(\"\\nIs the tensor storage location of t1 before and after normal addition same: \" + str(loc_normalAdd==loc_t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inplace Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will look into inplace operation. Inplace operations are always post-fixed with a trailing `_`. First of all, we wiil reinitialization $t_1$ and $t_2$ tensor and check their tensor storage location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: \n",
      "tensor([[0.6708, 0.8897],\n",
      "        [0.4758, 0.7601],\n",
      "        [0.7696, 0.8468]])\n",
      "t2: \n",
      "tensor([[0.1715, 0.2010],\n",
      "        [0.9651, 0.7610],\n",
      "        [0.9869, 0.6123]])\n",
      "\n",
      "Tensor storage location of t1: 1543902689728\n",
      "Tensor storage location of t2: 1543902690176\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand(3,2)\n",
    "t2 = torch.rand(3,2)\n",
    "print(\"t1: \\n\"+str(t1))\n",
    "print(\"t2: \\n\"+str(t2))\n",
    "\n",
    "loc_t1 = t1.data_ptr()\n",
    "loc_t2 = t2.data_ptr()\n",
    "print(\"\\nTensor storage location of t1: \" + str(loc_t1))\n",
    "print(\"Tensor storage location of t2: \" + str(loc_t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of normal addition: \n",
      "tensor([[0.8423, 1.0906],\n",
      "        [1.4409, 1.5211],\n",
      "        [1.7565, 1.4591]])\n",
      "Tensor storage location of t1: 1543902689728\n",
      "\n",
      "Is the tensor storage location of t1 before and after normal addition same: True\n"
     ]
    }
   ],
   "source": [
    "# Inplace addition\n",
    "\n",
    "t1 = t1.add_(t2)\n",
    "print(\"Value of normal addition: \\n\" + str(t1))\n",
    "loc_normalAdd = t1.data_ptr()\n",
    "print(\"Tensor storage location of t1: \" + str(loc_normalAdd))\n",
    "\n",
    "print(\"\\nIs the tensor storage location of t1 before and after normal addition same: \" + str(loc_normalAdd==loc_t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Mathematical Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's initialize two tensors which are $t_1$ and $t_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: \n",
      "tensor([[0.4112, 0.6493],\n",
      "        [0.6791, 0.3544],\n",
      "        [0.3705, 0.0969]])\n",
      "t2: \n",
      "tensor([[0.4592, 0.2616],\n",
      "        [0.9952, 0.9756],\n",
      "        [0.6004, 0.9060]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand(3,2)\n",
    "t2 = torch.rand(3,2)\n",
    "print(\"t1: \\n\"+str(t1))\n",
    "print(\"t2: \\n\"+str(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition of t1 and t2: \n",
      "tensor([[0.8704, 0.9109],\n",
      "        [1.6743, 1.3300],\n",
      "        [0.9709, 1.0030]])\n"
     ]
    }
   ],
   "source": [
    "tensorAdd = torch.add(t1, t2)\n",
    "print(\"Addition of t1 and t2: \\n\" + str(tensorAdd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtraction of t1 and t2: \n",
      "tensor([[-0.0480,  0.3876],\n",
      "        [-0.3161, -0.6212],\n",
      "        [-0.2299, -0.8091]])\n"
     ]
    }
   ],
   "source": [
    "tensorSub = torch.sub(t1, t2)\n",
    "print(\"Subtraction of t1 and t2: \\n\" + str(tensorSub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.mul(t1, t2)` - scalar/element-wise multiplication  <br /> \n",
    "`torch.mm(t1, t2)` - matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar multiplication required two tensors with same size. \n",
    ">Lets say tensor $t_1$ has $n \\times m$ dimension, then tensor $t_2$ should also have $n \\times m$ dimension in order to perform scalar multiplication. <br>The tensor size of scalar multiplication will be same as the tensor element size $n \\times m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of t1: torch.Size([3, 2])\n",
      "Size of t2: torch.Size([3, 2])\n",
      "\n",
      "Scalar multiplication of t1 and t2: \n",
      "tensor([[0.1888, 0.1699],\n",
      "        [0.6758, 0.3457],\n",
      "        [0.2225, 0.0878]])\n",
      "Size of tensorMul: torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of t1: \" + str(t1.shape))\n",
    "print(\"Size of t2: \" + str(t2.shape))\n",
    "tensorMul = torch.mul(t1, t2)\n",
    "print(\"\\nScalar multiplication of t1 and t2: \\n\" + str(tensorMul))\n",
    "print(\"Size of tensorMul: \" + str(tensorMul.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1615, 0.1385, 0.2428],\n",
      "        [0.4428, 0.3705, 0.6398],\n",
      "        [0.3831, 0.3640, 0.6676]])\n"
     ]
    }
   ],
   "source": [
    "mat_1 = torch.rand(3,3)\n",
    "mat_2 = torch.rand(3,3)\n",
    "print(mat_1.mm(mat_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication requires two tensors that fulfills the following conditions, where:  \n",
    ">Let's say tensor $t_1$ has $n \\times m$ dimension, then tensor $t_2$ must has $m \\times p$ dimension in order to perform matrix multiplication.<br>\n",
    "The resulting tensor size of matrix multiplication between $t_1$ and $t_2$ will be $n \\times p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of t1: torch.Size([3, 2])\n",
      "Size of t2: torch.Size([3, 2])\n",
      "size mismatch, m1: [3 x 2], m2: [3 x 2] at ..\\aten\\src\\TH/generic/THTensorMath.cpp:41\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of t1: \" + str(t1.shape))\n",
    "print(\"Size of t2: \" + str(t2.shape))\n",
    "try:\n",
    "    tensorMM = torch.mm(t1, t2)\n",
    "    print(\"\\nMatrix multiplication of t1 and t2: \\n\" + str(tensorMM))\n",
    "    print(\"Size of tensorMM: \" + str(tensorMM.shape))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error above shows `size mismatch`, because $t_2$ size does not match. In order to perform matrix multiplication, let's transpose t2 using `t2.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of t1: torch.Size([3, 2])\n",
      "Size of t2: torch.Size([2, 3])\n",
      "\n",
      "Matrix multiplication of t1 and t2: \n",
      "tensor([[0.3587, 1.0427, 0.8352],\n",
      "        [0.4046, 1.0215, 0.7288],\n",
      "        [0.1955, 0.4633, 0.3103]])\n",
      "Size of tensorMM: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "t2_T = t2.T\n",
    "print(\"Size of t1: \" + str(t1.shape))\n",
    "print(\"Size of t2: \" + str(t2_T.shape))\n",
    "tensorMM = torch.mm(t1, t2_T)\n",
    "print(\"\\nMatrix multiplication of t1 and t2: \\n\" + str(tensorMM))\n",
    "print(\"Size of tensorMM: \" + str(tensorMM.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every element x in the tensor, apply exponential function $e^{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential of t1: \n",
      "tensor([[1.5087, 1.9142],\n",
      "        [1.9721, 1.4253],\n",
      "        [1.4485, 1.1018]])\n"
     ]
    }
   ],
   "source": [
    "tensorExp = torch.exp(t1)\n",
    "print(\"Exponential of t1: \\n\" + str(tensorExp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every element x in the tensor, apply sigmoid function $\\frac{1}{1+e^{-x}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid of t1: \n",
      "tensor([[0.6014, 0.6568],\n",
      "        [0.6635, 0.5877],\n",
      "        [0.5916, 0.5242]])\n"
     ]
    }
   ],
   "source": [
    "tensorSig = torch.sigmoid(t1)\n",
    "print(\"Sigmoid of t1: \\n\" + str(tensorSig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: \n",
      "tensor([[0.5217, 0.4741],\n",
      "        [0.8540, 0.7760],\n",
      "        [0.5798, 0.4229]])\n",
      "t2: \n",
      "tensor([[0.0783, 0.4557],\n",
      "        [0.1646, 0.4775],\n",
      "        [0.5989, 0.4029]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand(3,2)\n",
    "t2 = torch.rand(3,2)\n",
    "print(\"t1: \\n\"+str(t1))\n",
    "print(\"t2: \\n\"+str(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the indices of the maximum value of all elements in the input tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no axix is specified, it will flatten the tensor and return the index of maximum value of all elements in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`axis=0` will return the indices of the maximum value of each elements in each coloumns in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(t1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`axis=1` will return the indices of the maximum value of each elements in each rows in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(t1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the sum of all elements in the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of the tensor t1: 3.628380537033081\n"
     ]
    }
   ],
   "source": [
    "tensor1Sum = torch.sum(t1)\n",
    "print(\"Sum of the tensor t1: \" + str(float(tensor1Sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of the tensor t2: 2.177807331085205\n"
     ]
    }
   ],
   "source": [
    "tensor2Sum = torch.sum(t2)\n",
    "print(\"Sum of the tensor t2: \" + str(float(tensor2Sum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Tensor Indexing, Slicing, Joining, Mutating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor indexing and slicing is similar to that of `numpy`:<br>\n",
    "\n",
    ">To get the element with indices $(x,y,z)$ in tensor $a$:<br> \n",
    "`element = a[x,y,z]`\n",
    "\n",
    ">To slice a part of the tensor $a$ in range $(x_1->x_2,y_1->y_2,z_1->z_2)$:<br> \n",
    "`slice = a[x_1:x_2+1,y_1:y_2+1,z_1:z_2+1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[87, 25, 56, 93, 64, 59, 22],\n",
      "         [28, 62, 44,  8, 74, 42, 72]],\n",
      "\n",
      "        [[63, 81, 70, 76, 20, 73, 20],\n",
      "         [27, 31, 47, 74, 13, 56,  7]]])\n"
     ]
    }
   ],
   "source": [
    "# Indexing and Slicing\n",
    "t_1 = torch.randint(100,(2,2,7))\n",
    "print(t_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to get the element with index $(1,1,3)$ from $t_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "print(int(t_1[1,1,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to slice out a portion of the tensor from $(0->1,1,3->6)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8, 74, 42, 72],\n",
      "        [74, 13, 56,  7]])\n"
     ]
    }
   ],
   "source": [
    "print(t_1[:,1,3:7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining and Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in Numpy arrays, PyTorch tensors are able to join with each other. There are a few tensor methods that allow joining:\n",
    "- `torch.cat((tensor_1,tensor_2), ndim)`\n",
    "- `torch.stack((tensor_1,tensor_2), ndim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's join $t_1$ and $t_2$ at all of their dimensions respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation at dimension 0: \n",
      "tensor([[[87, 25, 56, 93, 64, 59, 22],\n",
      "         [28, 62, 44,  8, 74, 42, 72]],\n",
      "\n",
      "        [[63, 81, 70, 76, 20, 73, 20],\n",
      "         [27, 31, 47, 74, 13, 56,  7]],\n",
      "\n",
      "        [[59, 29, 80,  9, 57, 59,  3],\n",
      "         [98, 82,  3, 64, 27, 10, 33]],\n",
      "\n",
      "        [[17, 21, 15, 97, 81, 78, 39],\n",
      "         [94, 52,  1, 20, 19, 25,  3]]])\n",
      "\n",
      "Concatenation at dimension 1: \n",
      "tensor([[[87, 25, 56, 93, 64, 59, 22],\n",
      "         [28, 62, 44,  8, 74, 42, 72],\n",
      "         [59, 29, 80,  9, 57, 59,  3],\n",
      "         [98, 82,  3, 64, 27, 10, 33]],\n",
      "\n",
      "        [[63, 81, 70, 76, 20, 73, 20],\n",
      "         [27, 31, 47, 74, 13, 56,  7],\n",
      "         [17, 21, 15, 97, 81, 78, 39],\n",
      "         [94, 52,  1, 20, 19, 25,  3]]])\n",
      "\n",
      "Concatenation at dimension 2: \n",
      "tensor([[[87, 25, 56, 93, 64, 59, 22, 59, 29, 80,  9, 57, 59,  3],\n",
      "         [28, 62, 44,  8, 74, 42, 72, 98, 82,  3, 64, 27, 10, 33]],\n",
      "\n",
      "        [[63, 81, 70, 76, 20, 73, 20, 17, 21, 15, 97, 81, 78, 39],\n",
      "         [27, 31, 47, 74, 13, 56,  7, 94, 52,  1, 20, 19, 25,  3]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining\n",
    "# tensor.cat()\n",
    "t_2 = torch.randint(100,(2,2,7))\n",
    "join_0 = torch.cat((t_1,t_2),0)\n",
    "join_1 = torch.cat((t_1,t_2),1)\n",
    "join_2 = torch.cat((t_1,t_2),2)\n",
    "print(\"Concatenation at dimension 0: \\n\"+ str(join_0), end=\"\\n\\n\")\n",
    "print(\"Concatenation at dimension 1: \\n\"+ str(join_1), end=\"\\n\\n\")\n",
    "print(\"Concatenation at dimension 2: \\n\"+ str(join_2), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><i>Note:</i> During concatenation using `torch.cat()`, all of the dimensions except the dimension that is subject to concatenation have to be of the same size \n",
    "<br><br> For example, if we have tensor $a$ and $b$, with shape $(3,2,4)$ and $(3,7,4)$ respectively, concatenation on dimension 1 is only valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[39, 65, 98, 24],\n",
      "         [38, 77, 58, 78],\n",
      "         [74, 43,  5, 30],\n",
      "         [55, 90, 57, 60],\n",
      "         [94, 62, 44, 26],\n",
      "         [28, 63, 35, 17],\n",
      "         [62, 42, 58, 79],\n",
      "         [74, 87, 26, 24],\n",
      "         [26, 12, 24, 23]],\n",
      "\n",
      "        [[24, 45, 62, 85],\n",
      "         [40, 48,  2, 12],\n",
      "         [62, 56, 97, 53],\n",
      "         [83, 80, 51, 20],\n",
      "         [78, 66, 38, 62],\n",
      "         [46, 76, 88, 56],\n",
      "         [38, 77, 44, 86],\n",
      "         [23, 44, 41, 36],\n",
      "         [83, 99, 68, 12]],\n",
      "\n",
      "        [[27, 73, 72, 71],\n",
      "         [66, 34, 31, 26],\n",
      "         [95, 16, 85,  2],\n",
      "         [ 9, 63, 57, 51],\n",
      "         [98, 71, 72,  5],\n",
      "         [33,  5, 48, 82],\n",
      "         [44, 78, 69, 90],\n",
      "         [33,  7, 17, 95],\n",
      "         [38, 55, 45, 17]]])\n"
     ]
    }
   ],
   "source": [
    "# Correct concatenation\n",
    "a = torch.randint(100,(3,2,4))\n",
    "b = torch.randint(100,(3,7,4))\n",
    "print(torch.cat((a,b),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes of tensors must match except in dimension 0. Got 2 and 7 in dimension 1 (The offending index is 1)\n"
     ]
    }
   ],
   "source": [
    "# Error in concatenation\n",
    "try:\n",
    "    print(torch.cat((a,b),0))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use `torch.stack()` to join two tensors. This function will stack two tensors on a new dimension with the condition that **both tensors are of the same shape**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[39, 65, 98, 24],\n",
      "          [38, 77, 58, 78]],\n",
      "\n",
      "         [[24, 45, 62, 85],\n",
      "          [40, 48,  2, 12]],\n",
      "\n",
      "         [[27, 73, 72, 71],\n",
      "          [66, 34, 31, 26]]],\n",
      "\n",
      "\n",
      "        [[[ 5, 12, 54, 68],\n",
      "          [19, 79, 24, 42]],\n",
      "\n",
      "         [[26, 25, 30, 65],\n",
      "          [70, 26,  3,  6]],\n",
      "\n",
      "         [[12, 27, 97, 16],\n",
      "          [37, 66, 35, 51]]]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.randint(100,(3,2,4))\n",
    "print(torch.stack((a,c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also provides ways to split up tensors on dimension specified, namely:\n",
    "- `torch.split(tensor, split_size_or_section_size, n_dim)`\n",
    "- `torch.chunk(tensor, number_of_chunks, n_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 1: \n",
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9]],\n",
      "\n",
      "        [[10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29]]])\n",
      "\n",
      "split 1: \n",
      "tensor([[[30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39]],\n",
      "\n",
      "        [[40, 41, 42, 43, 44],\n",
      "         [45, 46, 47, 48, 49]]])\n",
      "\n",
      "section 1:\n",
      "tensor([[[0, 1, 2, 3, 4],\n",
      "         [5, 6, 7, 8, 9]]])\n",
      "\n",
      "section 2:\n",
      "tensor([[[10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29]]])\n",
      "\n",
      "section 3:\n",
      "tensor([[[30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39]],\n",
      "\n",
      "        [[40, 41, 42, 43, 44],\n",
      "         [45, 46, 47, 48, 49]]])\n",
      "\n",
      "chunk1: \n",
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9]],\n",
      "\n",
      "        [[10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29]]])\n",
      "\n",
      "chunk2: \n",
      "tensor([[[30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39]],\n",
      "\n",
      "        [[40, 41, 42, 43, 44],\n",
      "         [45, 46, 47, 48, 49]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splitting with chunk size\n",
    "tensor = torch.arange(0,50).reshape(5,2,5)\n",
    "split1, split2 = torch.split(tensor,3,0)\n",
    "print(\"split 1: \\n\"+str(split1),end=\"\\n\\n\")\n",
    "print(\"split 1: \\n\"+str(split2),end=\"\\n\\n\")\n",
    "\n",
    "# splitting with section_size list [1,2,2]\n",
    "section1, section2, section3 = torch.split(tensor,[1,2,2],0)\n",
    "print(\"section 1:\\n\"+str(section1), end=\"\\n\\n\")\n",
    "print(\"section 2:\\n\"+str(section2), end=\"\\n\\n\")\n",
    "print(\"section 3:\\n\"+str(section3), end=\"\\n\\n\")\n",
    "\n",
    "# chunking\n",
    "split1, split2 = torch.chunk(tensor,2,0)\n",
    "print(\"chunk1: \\n\"+str(split1), end=\"\\n\\n\")\n",
    "print(\"chunk2: \\n\"+str(split2), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutating\n",
    "There are three main parts in tensor mutating:\n",
    "1. Transposing\n",
    "2. Squeezing\n",
    "3. Unsqueezing\n",
    "\n",
    "**Transposing** \n",
    "<br>Transposing could be performed through 3 ways:\n",
    "- `torch.t() or tensor.t()`            - Only workable on 2_d tensors\n",
    "- `tensor.T`                           - Returns the tensors with all its dimensions reversed (1,2,4)->(4,2,1)\n",
    "- `torch.transpose(tensor, dim_0, dim_1)`- Performs transpose on any two dimensions specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_2d = torch.arange(0,30).reshape(5,6)\n",
    "tensor_3d = tensor_2d.reshape(2,5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  6, 12, 18, 24],\n",
      "        [ 1,  7, 13, 19, 25],\n",
      "        [ 2,  8, 14, 20, 26],\n",
      "        [ 3,  9, 15, 21, 27],\n",
      "        [ 4, 10, 16, 22, 28],\n",
      "        [ 5, 11, 17, 23, 29]])\n",
      "tensor([[ 0,  6, 12, 18, 24],\n",
      "        [ 1,  7, 13, 19, 25],\n",
      "        [ 2,  8, 14, 20, 26],\n",
      "        [ 3,  9, 15, 21, 27],\n",
      "        [ 4, 10, 16, 22, 28],\n",
      "        [ 5, 11, 17, 23, 29]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.t(tensor_2d))\n",
    "print(tensor_2d.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0, 15],\n",
      "         [ 3, 18],\n",
      "         [ 6, 21],\n",
      "         [ 9, 24],\n",
      "         [12, 27]],\n",
      "\n",
      "        [[ 1, 16],\n",
      "         [ 4, 19],\n",
      "         [ 7, 22],\n",
      "         [10, 25],\n",
      "         [13, 28]],\n",
      "\n",
      "        [[ 2, 17],\n",
      "         [ 5, 20],\n",
      "         [ 8, 23],\n",
      "         [11, 26],\n",
      "         [14, 29]]])\n",
      "torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_3d.T)\n",
    "print(tensor_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  3,  6,  9, 12],\n",
      "         [ 1,  4,  7, 10, 13],\n",
      "         [ 2,  5,  8, 11, 14]],\n",
      "\n",
      "        [[15, 18, 21, 24, 27],\n",
      "         [16, 19, 22, 25, 28],\n",
      "         [17, 20, 23, 26, 29]]])\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "print(torch.transpose(tensor_3d, 2, 1))\n",
    "print(torch.transpose(tensor_3d, 2, 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Squeezing**\n",
    "<br>Just like what the name suggests, squeezing helps to remove redundant dimensions with value of one. The function is as follows:\n",
    "\n",
    "`torch.squeeze(tensor, dim)`\n",
    "\n",
    "The function removes all dimensions with value 1 if no dimension is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before squeezing: \n",
      "torch.Size([1, 5, 2, 1, 2])\n",
      "Shape after squeezing: \n",
      "torch.Size([5, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(0,20).reshape(1,5,2,1,2)\n",
    "print(\"Shape before squeezing: \")\n",
    "print(tensor.shape)\n",
    "tensor_squeezed = torch.squeeze(tensor)\n",
    "print(\"Shape after squeezing: \")\n",
    "print(tensor_squeezed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before squeezing: \n",
      "torch.Size([1, 5, 2, 1, 2])\n",
      "Shape after squeezing dimension 3: \n",
      "torch.Size([1, 5, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# squeezing only dimension 3\n",
    "tensor_squeezed = torch.squeeze(tensor,3)\n",
    "print(\"Shape before squeezing: \")\n",
    "print(tensor.shape)\n",
    "print(\"Shape after squeezing dimension 3: \")\n",
    "print(tensor_squeezed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsqueeze**\n",
    "<br>Sometimes we would like our tensor to match a certain dimension so that we could perform matrix operations like broadcasting. In this case we would like our tensors to be padded with extra dimensions.\n",
    "<br> What we could do is to \"unsqueeze\" the matrix. \n",
    "- `torch.unsqueeze(tensor,dim)`\n",
    "- `tensor.unsqueeze(dim)`\n",
    "- `tensor.unsqueeze_(dim)`- inplace-operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before unsqueeze:\n",
      "torch.Size([10])\n",
      "Shape after unsqueeze:\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(0,10)\n",
    "print(\"Shape before unsqueeze:\")\n",
    "print(tensor.shape)\n",
    "tensor.unsqueeze_(0)\n",
    "print(\"Shape after unsqueeze:\")\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specifying tensor dimension to unsqueeze\n",
    "tensor.unsqueeze_(2)\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Tensor objects methods\n",
    "\n",
    "There are a lot of useful methods provided by tensor objects. In this section, we will provide some examples on commonly used tensor object methods in deep learning.\n",
    "<br>\n",
    "If you wish to know more on tensor object methods, you can access it at [here](https://pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 1: \n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [8, 9]])\n",
      "Tensor 2: \n",
      " tensor([[5, 6],\n",
      "        [7, 8]])\n",
      "tensor([1, 2, 3, 4, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "# Initializing tensor\n",
    "tensor_1 = torch.tensor([[1,2], [3,4], [8,9]])\n",
    "tensor_2 = torch.tensor([[5,6], [7,8]])\n",
    "print(\"Tensor 1: \\n\", tensor_1)\n",
    "print(\"Tensor 2: \\n\", tensor_2)\n",
    "\n",
    "# We can flatten the tensor into single dimension by just calling tensor.flatten() \n",
    "flat = tensor_1.flatten()\n",
    "print(flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform matric multiplication, this could be done in two ways\n",
    "\n",
    "\n",
    "1.   `torch.mm(tensor1,tensor2)`\n",
    "2.   `tensor.mm(tensor2)`\n",
    "\n",
    "Either way will produce the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 19,  22],\n",
      "        [ 43,  50],\n",
      "        [103, 120]])\n",
      "tensor([[ 19,  22],\n",
      "        [ 43,  50],\n",
      "        [103, 120]])\n"
     ]
    }
   ],
   "source": [
    "torch_mm = torch.mm(tensor_1, tensor_2)\n",
    "print(torch_mm)\n",
    "\n",
    "tensor_mm = tensor_1.mm(tensor_2)\n",
    "print(tensor_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert Tensor into Numpy array just by simply calling `tensor.numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "tensor_to_numpy = tensor_1.numpy()\n",
    "print(type(tensor_1))\n",
    "print(type(tensor_to_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also convert Numpy array to tensor by calling `torch.from_numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "numpy_1 = np.arange(10)\n",
    "numpy_to_tensor = torch.from_numpy(numpy_1) \n",
    "print(type(numpy_1))\n",
    "print(type(numpy_to_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we have to reshape our tensor to certain shape to perform calculation.\n",
    "<br>To do so, torch has 2 options:\n",
    "\n",
    "1.   `tensor.view()`\n",
    "2.   `tensor.reshape()`\n",
    "\n",
    "For `tensor.view()`, the returned tensor will share the underlying data with the original tensor, therefore making changes in the reshaped tensor will also affect with the original tensor.  `tensor.view()` could only accept contiguous tensor, if non_contigous tensor(eg: transposed tensor) was passed, error will be prompted.\n",
    "<br>\n",
    "<br>\n",
    "For more detailed explaination, you could read it at [here](https://pytorch.org/docs/stable/tensor_view.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor storage location:  1543907307712\n",
      "View tensor storage location:  1543907307712\n",
      "Reshaped: \n",
      " tensor([[1, 2, 3, 4, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "# The reshaped tensor will use the same storage when using tensor.view()\n",
    "print(\"Original tensor storage location: \", tensor_1.data_ptr())\n",
    "view_tensor = tensor_1.view(1,6)\n",
    "print(\"View tensor storage location: \", view_tensor.data_ptr())\n",
    "print(\"Reshaped: \\n\", view_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed tensor storage location:  1543907307712\n",
      "Transposed: \n",
      " tensor([[1, 3, 8],\n",
      "        [2, 4, 9]])\n",
      "Error:\n",
      "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.\n"
     ]
    }
   ],
   "source": [
    "# Error will be prompted when we are trying to reshape using tensor.view() on non-contiguous tensor\n",
    "# Transposed tensor will share same storage as the original tensor\n",
    "transposed = tensor_1.T\n",
    "print(\"Transposed tensor storage location: \", transposed.data_ptr())\n",
    "print(\"Transposed: \\n\", transposed)\n",
    "\n",
    "# view() with error\n",
    "try:\n",
    "    transposed.view(1,6)\n",
    "except Exception as e:\n",
    "    print(\"Error:\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transposed.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use `tensor.view()`, another way to do so is simply calling `tensor.contiguous()`, which will create a new storage to convert the tensor into a contiguous tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New transposed tensor storage location:  1543907309888\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "transposed = tensor_1.T.contiguous()\n",
    "print(\"New transposed tensor storage location: \" ,transposed.data_ptr())\n",
    "print(transposed.is_contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 3, 8, 2, 4, 9]])\n"
     ]
    }
   ],
   "source": [
    "t_view = transposed.view(1,6)\n",
    "print(t_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we perform data modification on the reshaped tensor(t_view), the changes will be reflect on original tensor too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before assigning: \n",
      " tensor([[1, 3, 8],\n",
      "        [2, 4, 9]])\n",
      "After assigning: \n",
      " tensor([[1, 3, 0],\n",
      "        [2, 4, 9]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Before assigning: \\n\" ,transposed)\n",
    "t_view[0,2] = 0\n",
    "print(\"After assigning: \\n\",transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensor.reshape()` can operate on both contiguous and non-contiguous tensor. It will be a view of input if continguous input was passed, otherwise it will allocate a new memory storage to store the reshaped tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor storage location:  1543907307712\n",
      "Reshape tensor storage location:  1543907311552\n",
      "Reshaped: \n",
      " tensor([[1, 3, 8, 2, 4, 9]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Original tensor storage location: \" ,tensor_1.data_ptr())\n",
    "transposed = tensor_1.T\n",
    "t_reshape = transposed.reshape(1,6)\n",
    "print(\"Reshape tensor storage location: \" ,t_reshape.data_ptr())\n",
    "print(\"Reshaped: \\n\" ,t_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before assigning: \n",
      " tensor([[1, 3, 8],\n",
      "        [2, 4, 9]])\n",
      "After assigning: \n",
      " tensor([[1, 3, 8],\n",
      "        [2, 4, 9]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Before assigning: \\n\",transposed)\n",
    "t_reshape[0,2] = 10\n",
    "print(\"After assigning: \\n\" ,transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Tensors on CPU and GPU  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout our course we will be using PyTorch with both CPU and GPU capabilities (CUDA Toolkit), thus the need for us to show how to utilize both CPU and GPU.\n",
    "<br><br>\n",
    "Before we utillize the GPU, the norm is that we check if cuda is available, and declare a `torch.device(\"cuda:0\")` for later use.<br>Notation-wise, `cuda` refers to the cuda-enabled gpu and `:0` refers to the index of the GPU. This is useful when you have multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch allows us to seamlessly move data to and from our GPU to perform computations with the following methods:\n",
    "- `torch.rand((shape),device=\"cuda\")`- this method initializes the tensor in GPU memory.\n",
    "- `tensor.to(\"cuda\")`- this method returns a copy of the tensor in GPU memory\n",
    "- `tensor.cuda()`- this method returns a copy of the tensor in GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8761, 0.4067, 0.0178, 0.4623],\n",
      "         [0.9986, 0.6126, 0.9758, 0.1552],\n",
      "         [0.5249, 0.5470, 0.3420, 0.9898]],\n",
      "\n",
      "        [[0.6732, 0.4558, 0.0051, 0.1461],\n",
      "         [0.8312, 0.0735, 0.9342, 0.4310],\n",
      "         [0.4204, 0.2009, 0.1191, 0.9088]]], device='cuda:0')\n",
      "tensor([[[0.8761, 0.4067, 0.0178, 0.4623],\n",
      "         [0.9986, 0.6126, 0.9758, 0.1552],\n",
      "         [0.5249, 0.5470, 0.3420, 0.9898]],\n",
      "\n",
      "        [[0.6732, 0.4558, 0.0051, 0.1461],\n",
      "         [0.8312, 0.0735, 0.9342, 0.4310],\n",
      "         [0.4204, 0.2009, 0.1191, 0.9088]]], device='cuda:0')\n",
      "tensor([[[0.8761, 0.4067, 0.0178, 0.4623],\n",
      "         [0.9986, 0.6126, 0.9758, 0.1552],\n",
      "         [0.5249, 0.5470, 0.3420, 0.9898]],\n",
      "\n",
      "        [[0.6732, 0.4558, 0.0051, 0.1461],\n",
      "         [0.8312, 0.0735, 0.9342, 0.4310],\n",
      "         [0.4204, 0.2009, 0.1191, 0.9088]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand((2,3,4),device=device)\n",
    "print(tensor)\n",
    "tensor_1 = torch.rand((2,3,4))\n",
    "tensor_2 = tensor.to(\"cuda:0\")\n",
    "print(tensor_2)\n",
    "tensor_3 = tensor.cuda(device)\n",
    "# or just tensor.cuda() if there is only one GPU\n",
    "print(tensor_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these tensors are **stored in separate devices.** Hence, both separate tensors in respectively **CPU and GPU are not capable of performing operations with each other**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(tensor_1+tensor_2)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the type of device that the tensor is in, simply use `tensor.device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(tensor_1.device)\n",
    "print(tensor_2.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Initialize a tensor of random integers ranging from 0-100 with **dimensions(5,8,7)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[44, 39, 33, 60, 63, 79, 27],\n",
      "         [ 3, 97, 83,  1, 66, 56, 99],\n",
      "         [78, 76, 56, 68, 94, 33, 26],\n",
      "         [19, 91, 54, 24, 41, 69, 69],\n",
      "         [49, 80, 81, 12, 63, 60, 95],\n",
      "         [85, 22, 99, 11, 88, 78, 43],\n",
      "         [96, 89, 71, 57, 83, 95, 82],\n",
      "         [71, 40, 69, 73, 41, 11, 80]],\n",
      "\n",
      "        [[ 3,  6, 76, 27, 99, 26, 63],\n",
      "         [74, 75,  0, 18, 32, 68, 12],\n",
      "         [77, 45, 10, 80, 48, 21, 19],\n",
      "         [16, 81, 90, 82, 19, 44, 33],\n",
      "         [69, 63,  9, 33, 19, 78, 35],\n",
      "         [83, 22, 58, 35, 16, 46, 35],\n",
      "         [77, 12,  5, 46, 56, 15, 84],\n",
      "         [50,  8, 71, 47,  8, 75, 84]],\n",
      "\n",
      "        [[84, 48, 44, 34, 19, 60,  7],\n",
      "         [14, 75, 63, 13, 57, 33, 20],\n",
      "         [49, 89, 93, 11, 28, 31, 77],\n",
      "         [58, 84,  1, 70, 84, 90, 84],\n",
      "         [69, 27, 70, 10, 41, 84, 19],\n",
      "         [69,  5, 99, 72, 62, 42, 71],\n",
      "         [14, 39, 71, 11, 50, 73, 30],\n",
      "         [66, 87, 26, 19, 71,  6, 94]],\n",
      "\n",
      "        [[91, 20, 85, 42, 75, 18,  1],\n",
      "         [18, 61, 12, 78, 50, 43, 28],\n",
      "         [39, 82,  7, 60, 88, 78, 96],\n",
      "         [98, 15, 18, 66, 95, 72, 84],\n",
      "         [12, 77, 42, 84, 69, 18, 95],\n",
      "         [ 6, 74, 43, 33, 64,  0, 12],\n",
      "         [15, 51, 12, 78,  2, 44,  8],\n",
      "         [23, 36, 45, 78, 68, 40, 15]],\n",
      "\n",
      "        [[21,  3, 80, 18, 21, 41, 81],\n",
      "         [47, 32, 32, 92, 93, 33, 86],\n",
      "         [37, 88, 20, 65, 30, 13, 65],\n",
      "         [95, 33, 96, 52, 32, 35, 75],\n",
      "         [69, 74, 73,  3, 57, 43, 48],\n",
      "         [20, 47, 45, 38, 32, 94, 67],\n",
      "         [43, 22, 55, 45, 93, 69, 91],\n",
      "         [78, 40, 96, 37, 64, 67, 86]]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.randint(0,100,(5,8,7))\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "```\n",
    "tensor([[[44, 39, 33, 60, 63, 79, 27],\n",
    "         [ 3, 97, 83,  1, 66, 56, 99],\n",
    "         [78, 76, 56, 68, 94, 33, 26],\n",
    "         [19, 91, 54, 24, 41, 69, 69],\n",
    "         [49, 80, 81, 12, 63, 60, 95],\n",
    "         [85, 22, 99, 11, 88, 78, 43],\n",
    "         [96, 89, 71, 57, 83, 95, 82],\n",
    "         [71, 40, 69, 73, 41, 11, 80]],\n",
    "\n",
    "        [[ 3,  6, 76, 27, 99, 26, 63],\n",
    "         [74, 75,  0, 18, 32, 68, 12],\n",
    "         [77, 45, 10, 80, 48, 21, 19],\n",
    "         [16, 81, 90, 82, 19, 44, 33],\n",
    "         [69, 63,  9, 33, 19, 78, 35],\n",
    "         [83, 22, 58, 35, 16, 46, 35],\n",
    "         [77, 12,  5, 46, 56, 15, 84],\n",
    "         [50,  8, 71, 47,  8, 75, 84]],\n",
    "\n",
    "        [[84, 48, 44, 34, 19, 60,  7],\n",
    "         [14, 75, 63, 13, 57, 33, 20],\n",
    "         [49, 89, 93, 11, 28, 31, 77],\n",
    "         [58, 84,  1, 70, 84, 90, 84],\n",
    "         [69, 27, 70, 10, 41, 84, 19],\n",
    "         [69,  5, 99, 72, 62, 42, 71],\n",
    "         [14, 39, 71, 11, 50, 73, 30],\n",
    "         [66, 87, 26, 19, 71,  6, 94]],\n",
    "\n",
    "        [[91, 20, 85, 42, 75, 18,  1],\n",
    "         [18, 61, 12, 78, 50, 43, 28],\n",
    "         [39, 82,  7, 60, 88, 78, 96],\n",
    "         [98, 15, 18, 66, 95, 72, 84],\n",
    "         [12, 77, 42, 84, 69, 18, 95],\n",
    "         [ 6, 74, 43, 33, 64,  0, 12],\n",
    "         [15, 51, 12, 78,  2, 44,  8],\n",
    "         [23, 36, 45, 78, 68, 40, 15]],\n",
    "\n",
    "        [[21,  3, 80, 18, 21, 41, 81],\n",
    "         [47, 32, 32, 92, 93, 33, 86],\n",
    "         [37, 88, 20, 65, 30, 13, 65],\n",
    "         [95, 33, 96, 52, 32, 35, 75],\n",
    "         [69, 74, 73,  3, 57, 43, 48],\n",
    "         [20, 47, 45, 38, 32, 94, 67],\n",
    "         [43, 22, 55, 45, 93, 69, 91],\n",
    "         [78, 40, 96, 37, 64, 67, 86]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Slice out part of the tensor with dimensions $(0->4,3->6,1->6)$ and name it `slice_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[91, 54, 24, 41, 69, 69],\n",
      "         [80, 81, 12, 63, 60, 95],\n",
      "         [22, 99, 11, 88, 78, 43],\n",
      "         [89, 71, 57, 83, 95, 82]],\n",
      "\n",
      "        [[81, 90, 82, 19, 44, 33],\n",
      "         [63,  9, 33, 19, 78, 35],\n",
      "         [22, 58, 35, 16, 46, 35],\n",
      "         [12,  5, 46, 56, 15, 84]],\n",
      "\n",
      "        [[84,  1, 70, 84, 90, 84],\n",
      "         [27, 70, 10, 41, 84, 19],\n",
      "         [ 5, 99, 72, 62, 42, 71],\n",
      "         [39, 71, 11, 50, 73, 30]],\n",
      "\n",
      "        [[15, 18, 66, 95, 72, 84],\n",
      "         [77, 42, 84, 69, 18, 95],\n",
      "         [74, 43, 33, 64,  0, 12],\n",
      "         [51, 12, 78,  2, 44,  8]],\n",
      "\n",
      "        [[33, 96, 52, 32, 35, 75],\n",
      "         [74, 73,  3, 57, 43, 48],\n",
      "         [47, 45, 38, 32, 94, 67],\n",
      "         [22, 55, 45, 93, 69, 91]]])\n"
     ]
    }
   ],
   "source": [
    "slice_1 = tensor[0:5,3:7,1:7]\n",
    "print(slice_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "```\n",
    "tensor([[[91, 54, 24, 41, 69, 69],\n",
    "         [80, 81, 12, 63, 60, 95],\n",
    "         [22, 99, 11, 88, 78, 43],\n",
    "         [89, 71, 57, 83, 95, 82]],\n",
    "\n",
    "        [[81, 90, 82, 19, 44, 33],\n",
    "         [63,  9, 33, 19, 78, 35],\n",
    "         [22, 58, 35, 16, 46, 35],\n",
    "         [12,  5, 46, 56, 15, 84]],\n",
    "\n",
    "        [[84,  1, 70, 84, 90, 84],\n",
    "         [27, 70, 10, 41, 84, 19],\n",
    "         [ 5, 99, 72, 62, 42, 71],\n",
    "         [39, 71, 11, 50, 73, 30]],\n",
    "\n",
    "        [[15, 18, 66, 95, 72, 84],\n",
    "         [77, 42, 84, 69, 18, 95],\n",
    "         [74, 43, 33, 64,  0, 12],\n",
    "         [51, 12, 78,  2, 44,  8]],\n",
    "\n",
    "        [[33, 96, 52, 32, 35, 75],\n",
    "         [74, 73,  3, 57, 43, 48],\n",
    "         [47, 45, 38, 32, 94, 67],\n",
    "         [22, 55, 45, 93, 69, 91]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Produce 5 chunks of tensors where $dim = 0$ from `slice_1` and put it in a list: `list_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[91, 54, 24, 41, 69, 69],\n",
      "         [80, 81, 12, 63, 60, 95],\n",
      "         [22, 99, 11, 88, 78, 43],\n",
      "         [89, 71, 57, 83, 95, 82]]])\n",
      "tensor([[[81, 90, 82, 19, 44, 33],\n",
      "         [63,  9, 33, 19, 78, 35],\n",
      "         [22, 58, 35, 16, 46, 35],\n",
      "         [12,  5, 46, 56, 15, 84]]])\n",
      "tensor([[[84,  1, 70, 84, 90, 84],\n",
      "         [27, 70, 10, 41, 84, 19],\n",
      "         [ 5, 99, 72, 62, 42, 71],\n",
      "         [39, 71, 11, 50, 73, 30]]])\n",
      "tensor([[[15, 18, 66, 95, 72, 84],\n",
      "         [77, 42, 84, 69, 18, 95],\n",
      "         [74, 43, 33, 64,  0, 12],\n",
      "         [51, 12, 78,  2, 44,  8]]])\n",
      "tensor([[[33, 96, 52, 32, 35, 75],\n",
      "         [74, 73,  3, 57, 43, 48],\n",
      "         [47, 45, 38, 32, 94, 67],\n",
      "         [22, 55, 45, 93, 69, 91]]])\n"
     ]
    }
   ],
   "source": [
    "list_1 = slice_1.chunk(5)\n",
    "for chunk in list_1:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "```\n",
    "tensor([[[91, 54, 24, 41, 69, 69],\n",
    "         [80, 81, 12, 63, 60, 95],\n",
    "         [22, 99, 11, 88, 78, 43],\n",
    "         [89, 71, 57, 83, 95, 82]]])\n",
    "tensor([[[81, 90, 82, 19, 44, 33],\n",
    "         [63,  9, 33, 19, 78, 35],\n",
    "         [22, 58, 35, 16, 46, 35],\n",
    "         [12,  5, 46, 56, 15, 84]]])\n",
    "tensor([[[84,  1, 70, 84, 90, 84],\n",
    "         [27, 70, 10, 41, 84, 19],\n",
    "         [ 5, 99, 72, 62, 42, 71],\n",
    "         [39, 71, 11, 50, 73, 30]]])\n",
    "tensor([[[15, 18, 66, 95, 72, 84],\n",
    "         [77, 42, 84, 69, 18, 95],\n",
    "         [74, 43, 33, 64,  0, 12],\n",
    "         [51, 12, 78,  2, 44,  8]]])\n",
    "tensor([[[33, 96, 52, 32, 35, 75],\n",
    "         [74, 73,  3, 57, 43, 48],\n",
    "         [47, 45, 38, 32, 94, 67],\n",
    "         [22, 55, 45, 93, 69, 91]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Concatenate the first and third chunk together at dimension 2 and name it `cat_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[91, 54, 24, 41, 69, 69, 84,  1, 70, 84, 90, 84],\n",
      "         [80, 81, 12, 63, 60, 95, 27, 70, 10, 41, 84, 19],\n",
      "         [22, 99, 11, 88, 78, 43,  5, 99, 72, 62, 42, 71],\n",
      "         [89, 71, 57, 83, 95, 82, 39, 71, 11, 50, 73, 30]]])\n"
     ]
    }
   ],
   "source": [
    "cat_1 = torch.cat((list_1[0],list_1[2]),2)\n",
    "print(cat_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "```\n",
    "tensor([[[91, 54, 24, 41, 69, 69, 84,  1, 70, 84, 90, 84],\n",
    "         [80, 81, 12, 63, 60, 95, 27, 70, 10, 41, 84, 19],\n",
    "         [22, 99, 11, 88, 78, 43,  5, 99, 72, 62, 42, 71],\n",
    "         [89, 71, 57, 83, 95, 82, 39, 71, 11, 50, 73, 30]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Add `tensor_1` and `tensor_2` five times and store the result in the same storage location of `tensor_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = torch.tensor([1, 0, -1, -1, 0, 1])\n",
    "tensor_2 = torch.tensor([[21, 45, 68, 32, 1, 0],\n",
    "                        [93, 32, 33, 20, 5, 72]])\n",
    "tensor_ans = torch.empty(0)\n",
    "tensor_ans = tensor_2.add_(tensor_1.mul(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_ans: \n",
      "tensor([[26, 45, 63, 27,  1,  5],\n",
      "        [98, 32, 28, 15,  5, 77]])\n",
      "\n",
      "Is tensor storage location of tensor_ans same as tensor storage location of tensor_2: True\n"
     ]
    }
   ],
   "source": [
    "loc_tensor_1 = tensor_1.data_ptr()\n",
    "loc_tensor_2 = tensor_2.data_ptr()\n",
    "loc_tensor_ans = tensor_ans.data_ptr()\n",
    "print(\"tensor_ans: \\n\" + str(tensor_ans))\n",
    "print(\"\\nIs tensor storage location of tensor_ans same as tensor storage location of tensor_2: \" + str(loc_tensor_ans == loc_tensor_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "```\n",
    "tensor_ans: \n",
    "tensor([[26, 45, 63, 27,  1,  5],\n",
    "        [98, 32, 28, 15,  5, 77]])\n",
    "\n",
    "Is tensor storage location of tensor_ans same as tensor storage location of tensor_2: True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Lets say right now we want to have **two** 8-bit RGB image with the size of 228 x 228, try to randomly initialize the tensors with the detail given \n",
    "(height, width, channel).\n",
    "\n",
    "*Hint* : Use `torch.randint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 values for tensor 1 :  tensor([239, 194, 220, 114, 159,  42, 211, 211, 207, 222])\n",
      "First 10 values for tensor 2 :  tensor([111, 155,  17, 170, 220, 237,  19,  18, 127, 185])\n"
     ]
    }
   ],
   "source": [
    "# Intialize the tensors\n",
    "tensor_1 = torch.randint(0,256,(228,228,3))\n",
    "tensor_2 = torch.randint(0,256,(228,228,3))\n",
    "\n",
    "print(\"First 10 values for tensor 1 : \", tensor_1[0:10,0,0])\n",
    "print(\"First 10 values for tensor 2 : \", tensor_2[0:10,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "``` \n",
    "First 10 values for tensor 1 :  tensor([239, 194, 220, 114, 159,  42, 211, 211, 207, 222])\n",
    "First 10 values for tensor 2 :  tensor([111, 155,  17, 170, 220, 237,  19,  18, 127, 185])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is impossible to perform matric multiplication on a 3D tensor, therefore we have to reshape it into a 2D tensor.\n",
    "\n",
    "**_TASK_**: It this case, we would like to change the order of dimension of the tensor to shape$(channel, height, width)$ so that when we reshape, it would become $(channel, height \\times width)$\n",
    "\n",
    "*hint* : Use `tensor.permute`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 228, 228])\n",
      "torch.Size([3, 228, 228])\n"
     ]
    }
   ],
   "source": [
    "# Change the order of the tensor dimension\n",
    "tensor_1 = tensor_1.permute(2,0,1)\n",
    "tensor_2 = tensor_2.permute(2,0,1)\n",
    "\n",
    "print(tensor_1.shape)\n",
    "print(tensor_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "``` \n",
    "torch.Size([3, 228, 228])\n",
    "torch.Size([3, 228, 228])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Perform reshape on the tensor\n",
    "\n",
    "*hint* : Use `tensor.view`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 51984])\n",
      "torch.Size([3, 51984])\n"
     ]
    }
   ],
   "source": [
    "# Reshape the tensor\n",
    "tensor_1 = tensor_1.view(3,-1)\n",
    "tensor_2 = tensor_1.view(3,-1)\n",
    "\n",
    "print(tensor_1.shape)\n",
    "print(tensor_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "``` \n",
    "torch.Size([3, 51984])\n",
    "torch.Size([3, 51984])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Perform matric multiplication between the tensors, take note that you might need to transpose to match the shape (3x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1138473070,  850800576,  852055266],\n",
      "        [ 850800576, 1130950560,  846511247],\n",
      "        [ 852055266,  846511247, 1131113481]])\n"
     ]
    }
   ],
   "source": [
    "# Perform matric multiplication\n",
    "tensor_mm = torch.mm(tensor_1,tensor_2.t())\n",
    "\n",
    "print(tensor_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "``` \n",
    "tensor([[845716185, 842557652, 845981041],\n",
    "        [853949685, 846237767, 849568376],\n",
    "        [848178623, 842627404, 849479321]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Initialize a **gpu device.** Then, initialize a random tensor with any size and store it in the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3990, 0.5167, 0.0249, 0.9401, 0.9459, 0.7967],\n",
      "         [0.4150, 0.8203, 0.2290, 0.9096, 0.1183, 0.0752],\n",
      "         [0.4092, 0.9601, 0.2093, 0.1940, 0.8909, 0.4387]],\n",
      "\n",
      "        [[0.3570, 0.5454, 0.8299, 0.2099, 0.7684, 0.4290],\n",
      "         [0.2117, 0.6606, 0.1654, 0.4250, 0.9927, 0.6964],\n",
      "         [0.2472, 0.7028, 0.7494, 0.9303, 0.0494, 0.0750]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# initialize cuda device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tensor = torch.rand((2,3,6), device=device)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "``` \n",
    "torch.Size([3, 51984])\n",
    "torch.Size([3, 51984])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
