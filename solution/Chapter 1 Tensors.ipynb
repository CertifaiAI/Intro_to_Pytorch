{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Introduction to Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensors** is the fundamental data structure, a multidimensional array in PyTorch. It is metaphorical relatable to the numpy array of Numpy. The difference between the two is that tensors are available in two modes: `CPU and GPU`. We will touch this mentioned feature at the end of this tutorial.\n",
    "<br>\n",
    "<br>\n",
    "There are a few ways to build a tensor:\n",
    "- `torch.arange()`\n",
    "- `torch.ones()`\n",
    "- `torch.zeros()`\n",
    "- `torch.rand()`\n",
    "- `torch.randint()`\n",
    "- `torch.fromnumpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.arange(n,m)` returns a 1-dimensional tensor with values ranging from $n$ to $m-1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(0,20)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.ones(dim_0,dim_1,...,dim_n)` returns a tensor of ones of the shape $ dim_{0} \\times dim_{1} \\times...\\times dim_{n} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.ones(3,4,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.zeros(dim_0,dim_1,...,dim_n)` returns a tensor of zeros of the shape $ dim_{0} \\times dim_{1} \\times...\\times dim_{n} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.zeros(3,4,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.rand(dim_0, dim_1,...,dim_m)` returns a tensor filled with random floats ranging from $0$ to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.2859, 0.6775, 0.5071, 0.5461, 0.7330, 0.3735],\n",
      "         [0.3047, 0.1286, 0.3429, 0.8701, 0.8186, 0.2171],\n",
      "         [0.4222, 0.0883, 0.9961, 0.3317, 0.4389, 0.6991],\n",
      "         [0.4125, 0.9386, 0.4705, 0.1007, 0.6774, 0.8190]],\n",
      "\n",
      "        [[0.8590, 0.5544, 0.5610, 0.0102, 0.8174, 0.0764],\n",
      "         [0.0913, 0.2260, 0.4347, 0.4061, 0.5499, 0.8606],\n",
      "         [0.7268, 0.5759, 0.9317, 0.0726, 0.4304, 0.8007],\n",
      "         [0.3994, 0.0900, 0.2424, 0.3720, 0.7988, 0.8187]],\n",
      "\n",
      "        [[0.5969, 0.0384, 0.0908, 0.7887, 0.2292, 0.0524],\n",
      "         [0.1801, 0.1501, 0.7646, 0.7797, 0.1290, 0.6000],\n",
      "         [0.8332, 0.8725, 0.9735, 0.7468, 0.8632, 0.9013],\n",
      "         [0.0145, 0.6645, 0.5157, 0.1906, 0.3781, 0.9433]]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand(3,4,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Mathematical Operations on Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Mathematical Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's initialize two tensors which are $t_1$ and $t_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: \n",
      "tensor([[0.3382, 0.7931],\n",
      "        [0.6499, 0.3060],\n",
      "        [0.0039, 0.5766]])\n",
      "t2: \n",
      "tensor([[0.9197, 0.0988],\n",
      "        [0.7919, 0.2058],\n",
      "        [0.0935, 0.3464]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand(3,2)\n",
    "t2 = torch.rand(3,2)\n",
    "print(\"t1: \\n\"+str(t1))\n",
    "print(\"t2: \\n\"+str(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition of t1 and t2: \n",
      "tensor([[1.2579, 0.8919],\n",
      "        [1.4417, 0.5118],\n",
      "        [0.0974, 0.9230]])\n"
     ]
    }
   ],
   "source": [
    "tensorAdd = torch.add(t1, t2)\n",
    "print(\"Addition of t1 and t2: \\n\" + str(tensorAdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition of t1 and t2 using '+' operator:\n",
      "tensor([[1.2579, 0.8919],\n",
      "        [1.4417, 0.5118],\n",
      "        [0.0974, 0.9230]])\n"
     ]
    }
   ],
   "source": [
    "tensorAdd_2 = t1 + t2\n",
    "print(\"Addition of t1 and t2 using '+' operator:\\n\"+str(tensorAdd_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subtraction of t1 and t2: \n",
      "tensor([[-0.5815,  0.6943],\n",
      "        [-0.1420,  0.1003],\n",
      "        [-0.0897,  0.2303]])\n"
     ]
    }
   ],
   "source": [
    "tensorSub = torch.sub(t1, t2)\n",
    "print(\"Subtraction of t1 and t2: \\n\" + str(tensorSub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition of t1 and t2 using '-' operator:\n",
      "tensor([[-0.5815,  0.6943],\n",
      "        [-0.1420,  0.1003],\n",
      "        [-0.0897,  0.2303]])\n"
     ]
    }
   ],
   "source": [
    "tensorSub_2 = t1 - t2\n",
    "print(\"Addition of t1 and t2 using '-' operator:\\n\"+str(tensorSub_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.mul(t1, t2)` - element-wise multiplication and scalar multiplication  <br /> \n",
    "`torch.mm(t1, t2)` - matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Element-wise multiplication required two tensors with same size. \n",
    ">Lets say tensor $t_1$ has $n \\times m$ dimension, then tensor $t_2$ should also have $n \\times m$ dimension in order to perform element-wise multiplication. <br>The tensor size of element-wise multiplication will be same as the tensor element size $n \\times m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of t1: torch.Size([3, 2])\n",
      "Size of t2: torch.Size([3, 2])\n",
      "\n",
      "Element-wise multiplication of t1 and t2: \n",
      "tensor([[3.1107e-01, 7.8367e-02],\n",
      "        [5.1461e-01, 6.2964e-02],\n",
      "        [3.6094e-04, 1.9973e-01]])\n",
      "Size of tensorMul: torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of t1: \" + str(t1.shape))\n",
    "print(\"Size of t2: \" + str(t2.shape))\n",
    "tensorMul = torch.mul(t1, t2)\n",
    "print(\"\\nElement-wise multiplication of t1 and t2: \\n\" + str(tensorMul))\n",
    "print(\"Size of tensorMul: \" + str(tensorMul.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of t1: torch.Size([3, 2])\n",
      "Size of t2: torch.Size([3, 2])\n",
      "\n",
      "Element-wise multiplication of t1 and t2 using '*' operator: \n",
      "tensor([[3.1107e-01, 7.8367e-02],\n",
      "        [5.1461e-01, 6.2964e-02],\n",
      "        [3.6094e-04, 1.9973e-01]])\n",
      "Size of tensorMul_2: torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of t1: \" + str(t1.shape))\n",
    "print(\"Size of t2: \" + str(t2.shape))\n",
    "tensorMul_2 = t1 * t2\n",
    "print(\"\\nElement-wise multiplication of t1 and t2 using '*' operator: \\n\" + str(tensorMul_2))\n",
    "print(\"Size of tensorMul_2: \" + str(tensorMul_2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0966, 1.2884, 0.7292],\n",
      "        [1.3622, 1.6019, 1.2856],\n",
      "        [0.9331, 1.1699, 1.1666]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "mat_1 = torch.rand(3,3)\n",
    "mat_2 = torch.rand(3,3)\n",
    "print(mat_1.mm(mat_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication requires two tensors that fulfills the following conditions, where:  \n",
    ">Let's say tensor $t_1$ has $n \\times m$ dimension, then tensor $t_2$ must has $m \\times p$ dimension in order to perform matrix multiplication.<br>\n",
    "The resulting tensor size of matrix multiplication between $t_1$ and $t_2$ will be $n \\times p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of t1: torch.Size([3, 2])\n",
      "Size of t2: torch.Size([3, 2])\n",
      "size mismatch, m1: [3 x 2], m2: [3 x 2] at ..\\aten\\src\\TH/generic/THTensorMath.cpp:41\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of t1: \" + str(t1.shape))\n",
    "print(\"Size of t2: \" + str(t2.shape))\n",
    "try:\n",
    "    tensorMM = torch.mm(t1, t2)\n",
    "    print(\"\\nMatrix multiplication of t1 and t2: \\n\" + str(tensorMM))\n",
    "    print(\"Size of tensorMM: \" + str(tensorMM.shape))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error above shows `size mismatch`, because $t_2$ size does not match. In order to perform matrix multiplication, let's transpose t2 using `t2.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of t1: torch.Size([3, 2])\n",
      "Size of t2: torch.Size([2, 3])\n",
      "\n",
      "Matrix multiplication of t1 and t2: \n",
      "tensor([[0.3894, 0.4310, 0.3063],\n",
      "        [0.6279, 0.5776, 0.1668],\n",
      "        [0.0605, 0.1217, 0.2001]])\n",
      "Size of tensorMM: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "t2_T = t2.T\n",
    "print(\"Size of t1: \" + str(t1.shape))\n",
    "print(\"Size of t2: \" + str(t2_T.shape))\n",
    "tensorMM = torch.mm(t1, t2_T)\n",
    "print(\"\\nMatrix multiplication of t1 and t2: \\n\" + str(tensorMM))\n",
    "print(\"Size of tensorMM: \" + str(tensorMM.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like element-wise multiplication, division between matrices requires both matrices be of the same size and results in a tensor of the same size as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of t1: torch.Size([3, 2])\n",
      "Size of t2: torch.Size([3, 2])\n",
      "\n",
      "Element-wise division of t1 and t2: \n",
      "tensor([[0.3677, 8.0269],\n",
      "        [0.8207, 1.4873],\n",
      "        [0.0413, 1.6648]])\n",
      "Size of tensorDiv: torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of t1: \" + str(t1.shape))\n",
    "print(\"Size of t2: \" + str(t2.shape))\n",
    "tensorDiv = torch.div(t1, t2)\n",
    "print(\"\\nElement-wise division of t1 and t2: \\n\" + str(tensorDiv))\n",
    "print(\"Size of tensorDiv: \" + str(tensorDiv.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element-wise division of t1 and t2 with '/' operator: \n",
      "tensor([[0.3677, 8.0269],\n",
      "        [0.8207, 1.4873],\n",
      "        [0.0413, 1.6648]])\n",
      "Size of tensorDiv: torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "tensorDiv_1 = t1 / t2\n",
    "print(\"Element-wise division of t1 and t2 with '/' operator: \\n\" + str(tensorDiv_1))\n",
    "print(\"Size of tensorDiv: \" + str(tensorDiv_1.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every element x in the tensor, apply exponential function $e^{x}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exponential of t1: \n",
      "tensor([[1.4024, 2.2103],\n",
      "        [1.9153, 1.3580],\n",
      "        [1.0039, 1.7800]])\n"
     ]
    }
   ],
   "source": [
    "tensorExp = torch.exp(t1)\n",
    "print(\"Exponential of t1: \\n\" + str(tensorExp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every element x in the tensor, apply sigmoid function $\\frac{1}{1+e^{-x}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid of t1: \n",
      "tensor([[0.5838, 0.6885],\n",
      "        [0.6570, 0.5759],\n",
      "        [0.5010, 0.6403]])\n"
     ]
    }
   ],
   "source": [
    "tensorSig = torch.sigmoid(t1)\n",
    "print(\"Sigmoid of t1: \\n\" + str(tensorSig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: \n",
      "tensor([[0.3922, 0.5664],\n",
      "        [0.8503, 0.2480],\n",
      "        [0.2495, 0.9991]])\n",
      "t2: \n",
      "tensor([[0.8323, 0.2853],\n",
      "        [0.6682, 0.7370],\n",
      "        [0.2295, 0.1381]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.rand(3,2)\n",
    "t2 = torch.rand(3,2)\n",
    "print(\"t1: \\n\"+str(t1))\n",
    "print(\"t2: \\n\"+str(t2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the indices of the maximum value of all elements in the input tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no axis is specified, it will flatten the tensor and return the index of maximum value of all elements in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`axis=0` will return the indices of the maximum value of each elements in each coloumns in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(t1, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`axis=1` will return the indices of the maximum value of each elements in each rows in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(t1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the sum of all elements in the input tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of the tensor t1: 3.305593967437744\n"
     ]
    }
   ],
   "source": [
    "tensor1Sum = torch.sum(t1)\n",
    "print(\"Sum of the tensor t1: \" + str(float(tensor1Sum)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of the tensor t2: 2.8902862071990967\n"
     ]
    }
   ],
   "source": [
    "tensor2Sum = torch.sum(t2)\n",
    "print(\"Sum of the tensor t2: \" + str(float(tensor2Sum)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Operations vs Inplace Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first initialize $t_1$ and $t_2$ tensor and check their tensor storage location. We will perform both normal and inplace additions and assign it to a variable $t_3$ and later on compare their values to see their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Operation:\n",
      "t1: \n",
      "tensor([1, 1, 1])\n",
      "t2: \n",
      "tensor([2, 2, 2])\n",
      "t3: \n",
      "tensor([3, 3, 3])\n",
      "\n",
      "Tensor storage location of t1: 2035840346304\n",
      "Tensor storage location of t2: 2035840352384\n",
      "Tensor storage location of t3: 2035840350336\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1,1,1])\n",
    "t2 = torch.tensor([2,2,2])\n",
    "t3 = t1.add(t2)\n",
    "print(\"Normal Operation:\")\n",
    "print(\"t1: \\n\"+str(t1))\n",
    "print(\"t2: \\n\"+str(t2))\n",
    "print(\"t3: \\n\"+str(t3))\n",
    "\n",
    "print(\"\\nTensor storage location of t1: \" + str(t1.data_ptr()))\n",
    "print(\"Tensor storage location of t2: \" + str(t2.data_ptr()))\n",
    "print(\"Tensor storage location of t3: \" + str(t3.data_ptr()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In-place Operation:\n",
      "t1: \n",
      "tensor([3, 3, 3])\n",
      "t2: \n",
      "tensor([2, 2, 2])\n",
      "t3: \n",
      "tensor([3, 3, 3])\n",
      "\n",
      "Tensor storage location of t1: 2035840350592\n",
      "Tensor storage location of t2: 2035840345536\n",
      "Tensor storage location of t3: 2035840350592\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1,1,1])\n",
    "t2 = torch.tensor([2,2,2])\n",
    "t3 = t1.add_(t2)\n",
    "print(\"In-place Operation:\")\n",
    "print(\"t1: \\n\"+str(t1))\n",
    "print(\"t2: \\n\"+str(t2))\n",
    "print(\"t3: \\n\"+str(t3))\n",
    "\n",
    "print(\"\\nTensor storage location of t1: \" + str(t1.data_ptr()))\n",
    "print(\"Tensor storage location of t2: \" + str(t2.data_ptr()))\n",
    "print(\"Tensor storage location of t3: \" + str(t3.data_ptr()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in normal operations, the result of the operation is only assigned to $t_3$.<br> \n",
    "Whereas in in-place operations, the result of the operation is directly stored in the original tensor where the in-place operations are called on. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we called `t1.add_()`, which means the data in $t_1$ tensor will change into the result of the addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Tensor Indexing, Slicing, Joining, Mutating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing and Slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor indexing and slicing is similar to that of `numpy`:<br>\n",
    "\n",
    ">To get the element with indices $(x,y,z)$ in tensor $a$:<br> \n",
    "`element = a[x,y,z]`\n",
    "\n",
    ">To slice a part of the tensor $a$ in range $(x_1\\Rightarrow x_2,y_1\\Rightarrow y_2,z_1\\Rightarrow z_2)$:<br> \n",
    "`slice = a[x_1:x_2+1,y_1:y_2+1,z_1:z_2+1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10, 11],\n",
      "        [12, 13, 14, 15, 16, 17],\n",
      "        [18, 19, 20, 21, 22, 23]])\n"
     ]
    }
   ],
   "source": [
    "# Indexing and Slicing\n",
    "t_1 = torch.arange(0,24).reshape(-1,6)\n",
    "print(t_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= ../data/indexing.png width=300 height=300>\n",
    "\n",
    "Let's try to get number $7$ out of the $t_1$.<br>\n",
    "$7$ will have the index $(1,3)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(int(t_1[1,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= ../data/slicing.png width=300 height=300>\n",
    "\n",
    "Let's try to slice out the above portion from $t_1$<br>\n",
    "This will have index range of $(0\\Rightarrow 2,2\\Rightarrow 4)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2,  3,  4],\n",
      "        [ 8,  9, 10],\n",
      "        [14, 15, 16]])\n"
     ]
    }
   ],
   "source": [
    "print(t_1[:3,2:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining and Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in Numpy arrays, PyTorch tensors are able to join with each other. There are a few tensor methods that allow joining:\n",
    "- `torch.cat((tensor_1,tensor_2), ndim)`\n",
    "- `torch.stack((tensor_1,tensor_2), ndim)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's join $t_1$ and $t_2$ at all of their dimensions respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_1:\n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]]])\n",
      "t_2:\n",
      "tensor([[[0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0.]]])\n",
      "Concatenation at dimension 0: \n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0.]]])\n",
      "\n",
      "Concatenation at dimension 1: \n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0.]]])\n",
      "\n",
      "Concatenation at dimension 2: \n",
      "tensor([[[1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining\n",
    "# tensor.cat()\n",
    "t_1 = torch.ones((2,2,7))\n",
    "t_2 = torch.zeros((2,2,7))\n",
    "join_0 = torch.cat((t_1,t_2),0)\n",
    "join_1 = torch.cat((t_1,t_2),1)\n",
    "join_2 = torch.cat((t_1,t_2),2)\n",
    "print(\"t_1:\",t_1,sep=\"\\n\")\n",
    "print(\"t_2:\",t_2,sep=\"\\n\")\n",
    "print(\"Concatenation at dimension 0: \\n\"+ str(join_0), end=\"\\n\\n\")\n",
    "print(\"Concatenation at dimension 1: \\n\"+ str(join_1), end=\"\\n\\n\")\n",
    "print(\"Concatenation at dimension 2: \\n\"+ str(join_2), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "><i>Note:</i> During concatenation using `torch.cat()`, all of the dimensions except the dimension that is subject to concatenation have to be of the same size \n",
    "<br><br> For example, if we have tensor $a$ and $b$, with shape $(3,2,4)$ and $(3,7,4)$ respectively, concatenation on dimension 1 is only valid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# Correct concatenation\n",
    "a = torch.ones((3,2,4))\n",
    "b = torch.zeros((3,7,4))\n",
    "print(torch.cat((a,b),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes of tensors must match except in dimension 0. Got 2 and 7 in dimension 1 (The offending index is 1)\n"
     ]
    }
   ],
   "source": [
    "# Error in concatenation\n",
    "try:\n",
    "    print(torch.cat((a,b),0))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use `torch.stack()` to join two tensors. This function will stack two tensors on a new dimension with the condition that **both tensors are of the same shape**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1., 1.],\n",
      "          [1., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]],\n",
      "\n",
      "         [[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.zeros((3,2,4))\n",
    "print(torch.stack((a,c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch also provides ways to split up tensors on dimension specified, namely:\n",
    "- `torch.split(tensor, split_size_or_section_size, n_dim)`\n",
    "- `torch.chunk(tensor, number_of_chunks, n_dim)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tensor: \n",
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9]],\n",
      "\n",
      "        [[10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29]],\n",
      "\n",
      "        [[30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39]],\n",
      "\n",
      "        [[40, 41, 42, 43, 44],\n",
      "         [45, 46, 47, 48, 49]]])\n",
      "split 1: \n",
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9]],\n",
      "\n",
      "        [[10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29]]])\n",
      "\n",
      "split 2: \n",
      "tensor([[[30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39]],\n",
      "\n",
      "        [[40, 41, 42, 43, 44],\n",
      "         [45, 46, 47, 48, 49]]])\n",
      "\n",
      "section 1:\n",
      "tensor([[[0, 1, 2, 3, 4],\n",
      "         [5, 6, 7, 8, 9]]])\n",
      "\n",
      "section 2:\n",
      "tensor([[[10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29]]])\n",
      "\n",
      "section 3:\n",
      "tensor([[[30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39]],\n",
      "\n",
      "        [[40, 41, 42, 43, 44],\n",
      "         [45, 46, 47, 48, 49]]])\n",
      "\n",
      "chunk1: \n",
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9]],\n",
      "\n",
      "        [[10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29]]])\n",
      "\n",
      "chunk2: \n",
      "tensor([[[30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39]],\n",
      "\n",
      "        [[40, 41, 42, 43, 44],\n",
      "         [45, 46, 47, 48, 49]]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(0,50).reshape(5,2,5)\n",
    "print(\"original tensor: \\n\"+ str(tensor))\n",
    "# splitting with chunk size\n",
    "\n",
    "split1, split2 = torch.split(tensor,3,0)\n",
    "print(\"split 1: \\n\"+str(split1),end=\"\\n\\n\")\n",
    "print(\"split 2: \\n\"+str(split2),end=\"\\n\\n\")\n",
    "\n",
    "# splitting with section_size list [1,2,2]\n",
    "section1, section2, section3 = torch.split(tensor,[1,2,2],0)\n",
    "print(\"section 1:\\n\"+str(section1), end=\"\\n\\n\")\n",
    "print(\"section 2:\\n\"+str(section2), end=\"\\n\\n\")\n",
    "print(\"section 3:\\n\"+str(section3), end=\"\\n\\n\")\n",
    "\n",
    "# chunking\n",
    "split1, split2 = torch.chunk(tensor,2,0)\n",
    "print(\"chunk1: \\n\"+str(split1), end=\"\\n\\n\")\n",
    "print(\"chunk2: \\n\"+str(split2), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutating\n",
    "There are three main parts in tensor mutating:\n",
    "1. Transposing\n",
    "2. Squeezing\n",
    "3. Unsqueezing\n",
    "\n",
    "**Transposing** \n",
    "<br>Transposing could be performed through 3 ways:\n",
    "- `torch.t() or tensor.t()`            - Only workable on 2_d tensors\n",
    "- `tensor.T`                           - Returns the tensors with all its dimensions reversed (1,2,4)->(4,2,1)\n",
    "- `torch.transpose(tensor, dim_0, dim_1)`- Performs transpose on any two dimensions specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_2d = torch.arange(0,30).reshape(5,6)\n",
    "tensor_3d = tensor_2d.reshape(2,5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  6, 12, 18, 24],\n",
      "        [ 1,  7, 13, 19, 25],\n",
      "        [ 2,  8, 14, 20, 26],\n",
      "        [ 3,  9, 15, 21, 27],\n",
      "        [ 4, 10, 16, 22, 28],\n",
      "        [ 5, 11, 17, 23, 29]])\n",
      "tensor([[ 0,  6, 12, 18, 24],\n",
      "        [ 1,  7, 13, 19, 25],\n",
      "        [ 2,  8, 14, 20, 26],\n",
      "        [ 3,  9, 15, 21, 27],\n",
      "        [ 4, 10, 16, 22, 28],\n",
      "        [ 5, 11, 17, 23, 29]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.t(tensor_2d))\n",
    "print(tensor_2d.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0, 15],\n",
      "         [ 3, 18],\n",
      "         [ 6, 21],\n",
      "         [ 9, 24],\n",
      "         [12, 27]],\n",
      "\n",
      "        [[ 1, 16],\n",
      "         [ 4, 19],\n",
      "         [ 7, 22],\n",
      "         [10, 25],\n",
      "         [13, 28]],\n",
      "\n",
      "        [[ 2, 17],\n",
      "         [ 5, 20],\n",
      "         [ 8, 23],\n",
      "         [11, 26],\n",
      "         [14, 29]]])\n",
      "torch.Size([2, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_3d.T)\n",
    "print(tensor_3d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  3,  6,  9, 12],\n",
      "         [ 1,  4,  7, 10, 13],\n",
      "         [ 2,  5,  8, 11, 14]],\n",
      "\n",
      "        [[15, 18, 21, 24, 27],\n",
      "         [16, 19, 22, 25, 28],\n",
      "         [17, 20, 23, 26, 29]]])\n",
      "torch.Size([2, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "print(torch.transpose(tensor_3d, 2, 1))\n",
    "print(torch.transpose(tensor_3d, 2, 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Squeezing**\n",
    "<br>Just like what the name suggests, squeezing helps to remove redundant dimensions with value of one. The function is as follows:\n",
    "\n",
    "`torch.squeeze(tensor, dim)`\n",
    "\n",
    "The function removes all dimensions with value 1 if no dimension is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before squeezing: \n",
      "torch.Size([1, 5, 2, 1, 2])\n",
      "Shape after squeezing: \n",
      "torch.Size([5, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(0,20).reshape(1,5,2,1,2)\n",
    "print(\"Shape before squeezing: \")\n",
    "print(tensor.shape)\n",
    "tensor_squeezed = torch.squeeze(tensor)\n",
    "print(\"Shape after squeezing: \")\n",
    "print(tensor_squeezed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before squeezing: \n",
      "torch.Size([1, 5, 2, 1, 2])\n",
      "Shape after squeezing dimension 3: \n",
      "torch.Size([1, 5, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# squeezing only dimension 3\n",
    "tensor_squeezed = torch.squeeze(tensor,3)\n",
    "print(\"Shape before squeezing: \")\n",
    "print(tensor.shape)\n",
    "print(\"Shape after squeezing dimension 3: \")\n",
    "print(tensor_squeezed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unsqueeze**\n",
    "<br>Sometimes we would like our tensor to match a certain dimension so that we could perform matrix operations like broadcasting. In this case we would like our tensors to be padded with extra dimensions.\n",
    "<br> What we could do is to \"unsqueeze\" the matrix. \n",
    "- `torch.unsqueeze(tensor,dim)`\n",
    "- `tensor.unsqueeze(dim)`\n",
    "- `tensor.unsqueeze_(dim)`- inplace-operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before unsqueeze:\n",
      "torch.Size([10])\n",
      "Shape after unsqueeze:\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.arange(0,10)\n",
    "print(\"Shape before unsqueeze:\")\n",
    "print(tensor.shape)\n",
    "tensor.unsqueeze_(0)\n",
    "print(\"Shape after unsqueeze:\")\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specifying tensor dimension to unsqueeze\n",
    "tensor.unsqueeze_(2)\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Tensor objects methods\n",
    "\n",
    "There are a lot of useful methods provided by tensor objects. In this section, we will provide some examples on commonly used tensor object methods in deep learning.\n",
    "<br>\n",
    "If you wish to know more on tensor object methods, you can access it at [here](https://pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 1: \n",
      " tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [8, 9]])\n",
      "Tensor 2: \n",
      " tensor([[5, 6],\n",
      "        [7, 8]])\n",
      "tensor([1, 2, 3, 4, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "# Initializing tensor\n",
    "tensor_1 = torch.tensor([[1,2], [3,4], [8,9]])\n",
    "tensor_2 = torch.tensor([[5,6], [7,8]])\n",
    "print(\"Tensor 1: \\n\", tensor_1)\n",
    "print(\"Tensor 2: \\n\", tensor_2)\n",
    "\n",
    "# We can flatten the tensor into single dimension by just calling tensor.flatten() \n",
    "flat = tensor_1.flatten()\n",
    "print(flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform matric multiplication, this could be done in two ways\n",
    "\n",
    "\n",
    "1.   `torch.mm(tensor1,tensor2)`\n",
    "2.   `tensor.mm(tensor2)`\n",
    "\n",
    "Either way will produce the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 19,  22],\n",
      "        [ 43,  50],\n",
      "        [103, 120]])\n",
      "tensor([[ 19,  22],\n",
      "        [ 43,  50],\n",
      "        [103, 120]])\n"
     ]
    }
   ],
   "source": [
    "torch_mm = torch.mm(tensor_1, tensor_2)\n",
    "print(torch_mm)\n",
    "\n",
    "tensor_mm = tensor_1.mm(tensor_2)\n",
    "print(tensor_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert Tensor into Numpy array just by simply calling `tensor.numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "tensor_to_numpy = tensor_1.numpy()\n",
    "print(type(tensor_1))\n",
    "print(type(tensor_to_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also convert Numpy array to tensor by calling `torch.from_numpy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "numpy_1 = np.arange(10)\n",
    "numpy_to_tensor = torch.from_numpy(numpy_1) \n",
    "print(type(numpy_1))\n",
    "print(type(numpy_to_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we have to reshape our tensor to certain shape to perform calculation.\n",
    "<br>To do so, torch has 2 options:\n",
    "\n",
    "1.   `tensor.view()`\n",
    "2.   `tensor.reshape()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `tensor.view()` function returns a tensor with a new shape but _shares the same underlying data with the original tensor._ What it does is actually change the metadata of the original tensor called `stride`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `stride` tells PyTorch how to read the underlying memory with respect to the tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[5, 7, 4],\n",
      "        [1, 3, 2],\n",
      "        [7, 3, 8]])\n",
      "Original tensor shape: torch.Size([3, 3])\n",
      "Original tensor stride: (3, 1)\n"
     ]
    }
   ],
   "source": [
    "tensor_1 = torch.tensor([[5,7,4],[1,3,2],[7,3,8]])\n",
    "print(\"Original tensor:\\n\"+str(tensor_1))\n",
    "print(\"Original tensor shape: \"+str(tensor_1.shape))\n",
    "print(\"Original tensor stride: \"+str(tensor_1.stride()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In tensor_1 (original tensor), its stride can be accesed through the `stride()` method, and as you can see it returns a tuple of (2,1).\n",
    "\n",
    "> Each value represents how many memory locations to traverse/skip until the next index in its respective dimensions. \n",
    "\n",
    "Based on that, the first value will represent $dim_0$, second value will represent $dim_1$ and so on. Referring to the above exmaple, the first value (2) shows the next index on the 0th dimension of this tensor is located after 2 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=../data/stride.png width=500 height=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped view tensor:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Reshaped view tensor shape:  torch.Size([1, 9])\n",
      "Stride of reshaped view tensor: (9, 1)\n"
     ]
    }
   ],
   "source": [
    "view_tensor = tensor_1.view(-1,9)\n",
    "print(\"Reshaped view tensor:\\n\"+str(view_tensor))\n",
    "print(\"Reshaped view tensor shape: \", str(view_tensor.shape))\n",
    "print(\"Stride of reshaped view tensor: \"+ str(view_tensor.stride()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, `view()` of a tensor with a different shape has a different stride as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we mentioned before, the tensor obtained after performing `view()` shares the same underlying data with the original tensor, thus any changes made in the original tensor is reflected in the obtained tensor as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# fill tensor_1 with ones\n",
    "tensor_1 = tensor_1.fill_(1)\n",
    "print(view_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the values of `view_tensor` changes as the values of `tensor_1` changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to note is that `view()` only accepts _contiguous_ tensors. \n",
    ">The term **'contiguous'** refers to continuity between data stored in the tensor's memory layout. \n",
    "\n",
    "A great example of a non-contiguous tensor will be a transposed tensor.\n",
    "Let's change the `tensor_1` to array ranges from `0-8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [3, 4, 5],\n",
       "        [6, 7, 8]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_1 = torch.arange(0,9) #init\n",
    "tensor_1 = tensor_1.view(3,3) #reshape\n",
    "tensor_1 #the memory that stores the tensor becomes [0,1,2,3,4,5,6,7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed tensor: \n",
      "tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n",
      "Transposed tensor stride: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Transposed tensor: \\n\"+ str(tensor_1.T)) #after transpose, the memory that stores the tensor becomes [0,3,6,1,4,7,2,5,8]\n",
    "print(\"Transposed tensor stride: \"+ str(tensor_1.T.stride()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-116-88ffa4340e32>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtensor_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "tensor_1.T.view(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_1.T.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are a few ways to tackle non-contiguous tensors. \n",
    "\n",
    "One of them is to call `tensor.contiguous()` to make them contiguous. PyTorch will make a new copy of the tensor in another memory location contiguously.\n",
    "\n",
    "The other one is to call `reshape()` which we will talk in a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed Tensor:\n",
      "tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n",
      "Contiguity of Tensor: True\n"
     ]
    }
   ],
   "source": [
    "transposed = tensor_1.T.contiguous()\n",
    "print(\"Transposed Tensor:\\n\" +str(transposed))\n",
    "print(\"Contiguity of Tensor: \" + str(transposed.is_contiguous()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 3, 6, 1, 4, 7, 2, 5, 8]])\n"
     ]
    }
   ],
   "source": [
    "t_view = transposed.view(1,9)\n",
    "print(t_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we perform data modification on the reshaped tensor(t_view), the changes will be reflect on original tensor too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before assigning: \n",
      " tensor([[0, 3, 0],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n",
      "After assigning: \n",
      " tensor([[  0,   3, 100],\n",
      "        [  1,   4,   7],\n",
      "        [  2,   5,   8]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Before assigning: \\n\" ,transposed)\n",
    "t_view[0,2] = 100\n",
    "print(\"After assigning: \\n\",transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tensor.reshape()` can operate on both contiguous and non-contiguous tensor. \n",
    "\n",
    "It will be a view of input if continguous input was passed, otherwise it will allocate a new memory storage to store the reshaped tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transposed tensor:\n",
      "tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n",
      "Contiguity of Transposed tensor: False\n",
      "Reshaped tensor:\n",
      "tensor([[0, 3, 6, 1, 4, 7, 2, 5, 8]])\n",
      "Contiguity of Rehsaped tensor: True\n"
     ]
    }
   ],
   "source": [
    "transposed = tensor_1.T\n",
    "print(\"Transposed tensor:\", transposed, sep=\"\\n\")\n",
    "print(\"Contiguity of Transposed tensor:\", transposed.is_contiguous())\n",
    "t_reshape = transposed.reshape(1,9)\n",
    "print(\"Reshaped tensor:\" , t_reshape, sep=\"\\n\")\n",
    "print(\"Contiguity of Rehsaped tensor:\", t_reshape.is_contiguous())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `transposed` tensor is non-contiguous, calling `transposed.reshape()` will return a copy of it that is contiguous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before assigning: \n",
      " tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n",
      "After assigning: \n",
      " tensor([[0, 3, 6],\n",
      "        [1, 4, 7],\n",
      "        [2, 5, 8]])\n"
     ]
    }
   ],
   "source": [
    "# changing the value of t_reshape will not affect the values of transposed \n",
    "print(\"Before assigning: \\n\",transposed)\n",
    "t_reshape[0,2] = 10\n",
    "print(\"After assigning: \\n\" ,transposed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Tensors on CPU and GPU  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout our course we will be using PyTorch with both CPU and GPU capabilities (CUDA Toolkit), thus the need for us to show how to utilize both CPU and GPU.\n",
    "<br><br>\n",
    "Before we utillize the GPU, the norm is that we check if cuda is available, and declare a `torch.device(\"cuda:0\")` for later use.<br>Notation-wise, `cuda` refers to the cuda-enabled gpu and `:0` refers to the index of the GPU. This is useful when you have multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch allows us to seamlessly move data to and from our GPU to perform computations with the following methods:\n",
    "- `torch.rand((shape),device=\"cuda\")`- this method initializes the tensor in GPU memory.\n",
    "- `tensor.to(\"cuda\")`- this method returns a copy of the tensor in GPU memory\n",
    "- `tensor.cuda()`- this method returns a copy of the tensor in GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4398, 0.9223, 0.9426, 0.0987],\n",
      "         [0.7932, 0.4384, 0.6645, 0.6505],\n",
      "         [0.7295, 0.3098, 0.0493, 0.2784]],\n",
      "\n",
      "        [[0.6656, 0.9662, 0.6077, 0.5527],\n",
      "         [0.3950, 0.9753, 0.4863, 0.7287],\n",
      "         [0.9289, 0.6358, 0.4757, 0.0803]]], device='cuda:0')\n",
      "tensor([[[0.4398, 0.9223, 0.9426, 0.0987],\n",
      "         [0.7932, 0.4384, 0.6645, 0.6505],\n",
      "         [0.7295, 0.3098, 0.0493, 0.2784]],\n",
      "\n",
      "        [[0.6656, 0.9662, 0.6077, 0.5527],\n",
      "         [0.3950, 0.9753, 0.4863, 0.7287],\n",
      "         [0.9289, 0.6358, 0.4757, 0.0803]]], device='cuda:0')\n",
      "tensor([[[0.4398, 0.9223, 0.9426, 0.0987],\n",
      "         [0.7932, 0.4384, 0.6645, 0.6505],\n",
      "         [0.7295, 0.3098, 0.0493, 0.2784]],\n",
      "\n",
      "        [[0.6656, 0.9662, 0.6077, 0.5527],\n",
      "         [0.3950, 0.9753, 0.4863, 0.7287],\n",
      "         [0.9289, 0.6358, 0.4757, 0.0803]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.rand((2,3,4),device=device)\n",
    "print(tensor)\n",
    "tensor_1 = torch.rand((2,3,4))\n",
    "tensor_2 = tensor.to(\"cuda:0\")\n",
    "print(tensor_2)\n",
    "tensor_3 = tensor.cuda(device)\n",
    "# or just tensor.cuda() if there is only one GPU\n",
    "print(tensor_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that these tensors are **stored in separate devices.** Hence, both separate tensors in respectively **CPU and GPU are not capable of performing operations with each other**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(tensor_1+tensor_2)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the type of device that the tensor is in, simply use `tensor.device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(tensor_1.device)\n",
    "print(tensor_2.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Initialize a tensor of random integers ranging from 0-100 with **dimensions(5,8,7)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[44, 39, 33, 60, 63, 79, 27],\n",
      "         [ 3, 97, 83,  1, 66, 56, 99],\n",
      "         [78, 76, 56, 68, 94, 33, 26],\n",
      "         [19, 91, 54, 24, 41, 69, 69],\n",
      "         [49, 80, 81, 12, 63, 60, 95],\n",
      "         [85, 22, 99, 11, 88, 78, 43],\n",
      "         [96, 89, 71, 57, 83, 95, 82],\n",
      "         [71, 40, 69, 73, 41, 11, 80]],\n",
      "\n",
      "        [[ 3,  6, 76, 27, 99, 26, 63],\n",
      "         [74, 75,  0, 18, 32, 68, 12],\n",
      "         [77, 45, 10, 80, 48, 21, 19],\n",
      "         [16, 81, 90, 82, 19, 44, 33],\n",
      "         [69, 63,  9, 33, 19, 78, 35],\n",
      "         [83, 22, 58, 35, 16, 46, 35],\n",
      "         [77, 12,  5, 46, 56, 15, 84],\n",
      "         [50,  8, 71, 47,  8, 75, 84]],\n",
      "\n",
      "        [[84, 48, 44, 34, 19, 60,  7],\n",
      "         [14, 75, 63, 13, 57, 33, 20],\n",
      "         [49, 89, 93, 11, 28, 31, 77],\n",
      "         [58, 84,  1, 70, 84, 90, 84],\n",
      "         [69, 27, 70, 10, 41, 84, 19],\n",
      "         [69,  5, 99, 72, 62, 42, 71],\n",
      "         [14, 39, 71, 11, 50, 73, 30],\n",
      "         [66, 87, 26, 19, 71,  6, 94]],\n",
      "\n",
      "        [[91, 20, 85, 42, 75, 18,  1],\n",
      "         [18, 61, 12, 78, 50, 43, 28],\n",
      "         [39, 82,  7, 60, 88, 78, 96],\n",
      "         [98, 15, 18, 66, 95, 72, 84],\n",
      "         [12, 77, 42, 84, 69, 18, 95],\n",
      "         [ 6, 74, 43, 33, 64,  0, 12],\n",
      "         [15, 51, 12, 78,  2, 44,  8],\n",
      "         [23, 36, 45, 78, 68, 40, 15]],\n",
      "\n",
      "        [[21,  3, 80, 18, 21, 41, 81],\n",
      "         [47, 32, 32, 92, 93, 33, 86],\n",
      "         [37, 88, 20, 65, 30, 13, 65],\n",
      "         [95, 33, 96, 52, 32, 35, 75],\n",
      "         [69, 74, 73,  3, 57, 43, 48],\n",
      "         [20, 47, 45, 38, 32, 94, 67],\n",
      "         [43, 22, 55, 45, 93, 69, 91],\n",
      "         [78, 40, 96, 37, 64, 67, 86]]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "# Your code\n",
    "tensor = torch.randint(0,100,(5,8,7))\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "```\n",
    "tensor([[[44, 39, 33, 60, 63, 79, 27],\n",
    "         [ 3, 97, 83,  1, 66, 56, 99],\n",
    "         [78, 76, 56, 68, 94, 33, 26],\n",
    "         [19, 91, 54, 24, 41, 69, 69],\n",
    "         [49, 80, 81, 12, 63, 60, 95],\n",
    "         [85, 22, 99, 11, 88, 78, 43],\n",
    "         [96, 89, 71, 57, 83, 95, 82],\n",
    "         [71, 40, 69, 73, 41, 11, 80]],\n",
    "\n",
    "        [[ 3,  6, 76, 27, 99, 26, 63],\n",
    "         [74, 75,  0, 18, 32, 68, 12],\n",
    "         [77, 45, 10, 80, 48, 21, 19],\n",
    "         [16, 81, 90, 82, 19, 44, 33],\n",
    "         [69, 63,  9, 33, 19, 78, 35],\n",
    "         [83, 22, 58, 35, 16, 46, 35],\n",
    "         [77, 12,  5, 46, 56, 15, 84],\n",
    "         [50,  8, 71, 47,  8, 75, 84]],\n",
    "\n",
    "        [[84, 48, 44, 34, 19, 60,  7],\n",
    "         [14, 75, 63, 13, 57, 33, 20],\n",
    "         [49, 89, 93, 11, 28, 31, 77],\n",
    "         [58, 84,  1, 70, 84, 90, 84],\n",
    "         [69, 27, 70, 10, 41, 84, 19],\n",
    "         [69,  5, 99, 72, 62, 42, 71],\n",
    "         [14, 39, 71, 11, 50, 73, 30],\n",
    "         [66, 87, 26, 19, 71,  6, 94]],\n",
    "\n",
    "        [[91, 20, 85, 42, 75, 18,  1],\n",
    "         [18, 61, 12, 78, 50, 43, 28],\n",
    "         [39, 82,  7, 60, 88, 78, 96],\n",
    "         [98, 15, 18, 66, 95, 72, 84],\n",
    "         [12, 77, 42, 84, 69, 18, 95],\n",
    "         [ 6, 74, 43, 33, 64,  0, 12],\n",
    "         [15, 51, 12, 78,  2, 44,  8],\n",
    "         [23, 36, 45, 78, 68, 40, 15]],\n",
    "\n",
    "        [[21,  3, 80, 18, 21, 41, 81],\n",
    "         [47, 32, 32, 92, 93, 33, 86],\n",
    "         [37, 88, 20, 65, 30, 13, 65],\n",
    "         [95, 33, 96, 52, 32, 35, 75],\n",
    "         [69, 74, 73,  3, 57, 43, 48],\n",
    "         [20, 47, 45, 38, 32, 94, 67],\n",
    "         [43, 22, 55, 45, 93, 69, 91],\n",
    "         [78, 40, 96, 37, 64, 67, 86]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Slice out part of the tensor with indices $(0\\rightarrow 4,3\\rightarrow 6,1\\rightarrow 6)$ and name it `slice_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[91, 54, 24, 41, 69, 69],\n",
      "         [80, 81, 12, 63, 60, 95],\n",
      "         [22, 99, 11, 88, 78, 43],\n",
      "         [89, 71, 57, 83, 95, 82]],\n",
      "\n",
      "        [[81, 90, 82, 19, 44, 33],\n",
      "         [63,  9, 33, 19, 78, 35],\n",
      "         [22, 58, 35, 16, 46, 35],\n",
      "         [12,  5, 46, 56, 15, 84]],\n",
      "\n",
      "        [[84,  1, 70, 84, 90, 84],\n",
      "         [27, 70, 10, 41, 84, 19],\n",
      "         [ 5, 99, 72, 62, 42, 71],\n",
      "         [39, 71, 11, 50, 73, 30]],\n",
      "\n",
      "        [[15, 18, 66, 95, 72, 84],\n",
      "         [77, 42, 84, 69, 18, 95],\n",
      "         [74, 43, 33, 64,  0, 12],\n",
      "         [51, 12, 78,  2, 44,  8]],\n",
      "\n",
      "        [[33, 96, 52, 32, 35, 75],\n",
      "         [74, 73,  3, 57, 43, 48],\n",
      "         [47, 45, 38, 32, 94, 67],\n",
      "         [22, 55, 45, 93, 69, 91]]])\n"
     ]
    }
   ],
   "source": [
    "slice_1 = tensor[0:5,3:7,1:7]\n",
    "print(slice_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "```\n",
    "tensor([[[91, 54, 24, 41, 69, 69],\n",
    "         [80, 81, 12, 63, 60, 95],\n",
    "         [22, 99, 11, 88, 78, 43],\n",
    "         [89, 71, 57, 83, 95, 82]],\n",
    "\n",
    "        [[81, 90, 82, 19, 44, 33],\n",
    "         [63,  9, 33, 19, 78, 35],\n",
    "         [22, 58, 35, 16, 46, 35],\n",
    "         [12,  5, 46, 56, 15, 84]],\n",
    "\n",
    "        [[84,  1, 70, 84, 90, 84],\n",
    "         [27, 70, 10, 41, 84, 19],\n",
    "         [ 5, 99, 72, 62, 42, 71],\n",
    "         [39, 71, 11, 50, 73, 30]],\n",
    "\n",
    "        [[15, 18, 66, 95, 72, 84],\n",
    "         [77, 42, 84, 69, 18, 95],\n",
    "         [74, 43, 33, 64,  0, 12],\n",
    "         [51, 12, 78,  2, 44,  8]],\n",
    "\n",
    "        [[33, 96, 52, 32, 35, 75],\n",
    "         [74, 73,  3, 57, 43, 48],\n",
    "         [47, 45, 38, 32, 94, 67],\n",
    "         [22, 55, 45, 93, 69, 91]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Produce 5 chunks of tensors where $dim = 0$ from `slice_1` and put it in a list: `list_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[91, 54, 24, 41, 69, 69],\n",
      "         [80, 81, 12, 63, 60, 95],\n",
      "         [22, 99, 11, 88, 78, 43],\n",
      "         [89, 71, 57, 83, 95, 82]]])\n",
      "tensor([[[81, 90, 82, 19, 44, 33],\n",
      "         [63,  9, 33, 19, 78, 35],\n",
      "         [22, 58, 35, 16, 46, 35],\n",
      "         [12,  5, 46, 56, 15, 84]]])\n",
      "tensor([[[84,  1, 70, 84, 90, 84],\n",
      "         [27, 70, 10, 41, 84, 19],\n",
      "         [ 5, 99, 72, 62, 42, 71],\n",
      "         [39, 71, 11, 50, 73, 30]]])\n",
      "tensor([[[15, 18, 66, 95, 72, 84],\n",
      "         [77, 42, 84, 69, 18, 95],\n",
      "         [74, 43, 33, 64,  0, 12],\n",
      "         [51, 12, 78,  2, 44,  8]]])\n",
      "tensor([[[33, 96, 52, 32, 35, 75],\n",
      "         [74, 73,  3, 57, 43, 48],\n",
      "         [47, 45, 38, 32, 94, 67],\n",
      "         [22, 55, 45, 93, 69, 91]]])\n"
     ]
    }
   ],
   "source": [
    "list_1 = slice_1.chunk(5)\n",
    "for chunk in list_1:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "```\n",
    "tensor([[[91, 54, 24, 41, 69, 69],\n",
    "         [80, 81, 12, 63, 60, 95],\n",
    "         [22, 99, 11, 88, 78, 43],\n",
    "         [89, 71, 57, 83, 95, 82]]])\n",
    "tensor([[[81, 90, 82, 19, 44, 33],\n",
    "         [63,  9, 33, 19, 78, 35],\n",
    "         [22, 58, 35, 16, 46, 35],\n",
    "         [12,  5, 46, 56, 15, 84]]])\n",
    "tensor([[[84,  1, 70, 84, 90, 84],\n",
    "         [27, 70, 10, 41, 84, 19],\n",
    "         [ 5, 99, 72, 62, 42, 71],\n",
    "         [39, 71, 11, 50, 73, 30]]])\n",
    "tensor([[[15, 18, 66, 95, 72, 84],\n",
    "         [77, 42, 84, 69, 18, 95],\n",
    "         [74, 43, 33, 64,  0, 12],\n",
    "         [51, 12, 78,  2, 44,  8]]])\n",
    "tensor([[[33, 96, 52, 32, 35, 75],\n",
    "         [74, 73,  3, 57, 43, 48],\n",
    "         [47, 45, 38, 32, 94, 67],\n",
    "         [22, 55, 45, 93, 69, 91]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Concatenate the first and third chunk together at dimension 2 and name it `cat_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[91, 54, 24, 41, 69, 69, 84,  1, 70, 84, 90, 84],\n",
      "         [80, 81, 12, 63, 60, 95, 27, 70, 10, 41, 84, 19],\n",
      "         [22, 99, 11, 88, 78, 43,  5, 99, 72, 62, 42, 71],\n",
      "         [89, 71, 57, 83, 95, 82, 39, 71, 11, 50, 73, 30]]])\n"
     ]
    }
   ],
   "source": [
    "cat_1 = torch.cat((list_1[0],list_1[2]),2)\n",
    "print(cat_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "```\n",
    "tensor([[[91, 54, 24, 41, 69, 69, 84,  1, 70, 84, 90, 84],\n",
    "         [80, 81, 12, 63, 60, 95, 27, 70, 10, 41, 84, 19],\n",
    "         [22, 99, 11, 88, 78, 43,  5, 99, 72, 62, 42, 71],\n",
    "         [89, 71, 57, 83, 95, 82, 39, 71, 11, 50, 73, 30]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Add `tensor_1` and `tensor_2` five times and store the result in the same storage location of `tensor_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = torch.tensor([1, 0, -1, -1, 0, 1])\n",
    "tensor_2 = torch.tensor([[21, 45, 68, 32, 1, 0],\n",
    "                        [93, 32, 33, 20, 5, 72]])\n",
    "tensor_ans = torch.empty(0)\n",
    "tensor_ans = tensor_2.add_(tensor_1.mul(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_ans: \n",
      "tensor([[26, 45, 63, 27,  1,  5],\n",
      "        [98, 32, 28, 15,  5, 77]])\n",
      "\n",
      "Is tensor storage location of tensor_ans same as tensor storage location of tensor_2: True\n"
     ]
    }
   ],
   "source": [
    "loc_tensor_1 = tensor_1.data_ptr()\n",
    "loc_tensor_2 = tensor_2.data_ptr()\n",
    "loc_tensor_ans = tensor_ans.data_ptr()\n",
    "print(\"tensor_ans: \\n\" + str(tensor_ans))\n",
    "print(\"\\nIs tensor storage location of tensor_ans same as tensor storage location of tensor_2: \" + str(loc_tensor_ans == loc_tensor_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "```\n",
    "tensor_ans: \n",
    "tensor([[26, 45, 63, 27,  1,  5],\n",
    "        [98, 32, 28, 15,  5, 77]])\n",
    "\n",
    "Is tensor storage location of tensor_ans same as tensor storage location of tensor_2: True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Lets say right now we want to have **two** 8-bit RGB image with the size of 228 x 228, try to randomly initialize the tensors with the detail given \n",
    "(height, width, channel).\n",
    "\n",
    "*Hint* : Use `torch.randint`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 values for tensor 1 :  tensor([239, 194, 220, 114, 159,  42, 211, 211, 207, 222])\n",
      "First 10 values for tensor 2 :  tensor([111, 155,  17, 170, 220, 237,  19,  18, 127, 185])\n"
     ]
    }
   ],
   "source": [
    "# Intialize the tensors\n",
    "tensor_1 = torch.randint(0,256,(228,228,3))\n",
    "tensor_2 = torch.randint(0,256,(228,228,3))\n",
    "\n",
    "print(\"First 10 values for tensor 1 : \", tensor_1[0:10,0,0])\n",
    "print(\"First 10 values for tensor 2 : \", tensor_2[0:10,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "``` \n",
    "First 10 values for tensor 1 :  tensor([239, 194, 220, 114, 159,  42, 211, 211, 207, 222])\n",
    "First 10 values for tensor 2 :  tensor([111, 155,  17, 170, 220, 237,  19,  18, 127, 185])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is impossible to perform matric multiplication on a 3D tensor, therefore we have to reshape it into a 2D tensor.\n",
    "\n",
    "**_TASK_**: It this case, we would like to change the order of dimension of the tensor to shape$(channel, height, width)$ so that when we reshape, it would become $(channel, height \\times width)$\n",
    "\n",
    "*hint* : Use `tensor.permute`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 228, 228])\n",
      "torch.Size([3, 228, 228])\n"
     ]
    }
   ],
   "source": [
    "# Change the order of the tensor dimension\n",
    "tensor_1 = tensor_1.permute(2,0,1)\n",
    "tensor_2 = tensor_2.permute(2,0,1)\n",
    "\n",
    "print(tensor_1.shape)\n",
    "print(tensor_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "``` \n",
    "torch.Size([3, 228, 228])\n",
    "torch.Size([3, 228, 228])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Perform reshape on the tensor\n",
    "\n",
    "*hint* : Use `tensor.view`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 51984])\n",
      "torch.Size([3, 51984])\n"
     ]
    }
   ],
   "source": [
    "# Reshape the tensor\n",
    "tensor_1 = tensor_1.view(3,-1)\n",
    "tensor_2 = tensor_2.view(3,-1)\n",
    "\n",
    "print(tensor_1.shape)\n",
    "print(tensor_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "``` \n",
    "torch.Size([3, 51984])\n",
    "torch.Size([3, 51984])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Perform matric multiplication between the tensors, take note that you might need to transpose to match the shape (3x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[846356565, 849680903, 849140043],\n",
      "        [842706582, 849543264, 846890670],\n",
      "        [840797074, 846055878, 845897280]])\n"
     ]
    }
   ],
   "source": [
    "# Perform matric multiplication\n",
    "tensor_mm = torch.mm(tensor_1,tensor_2.t())\n",
    "\n",
    "print(tensor_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "``` \n",
    "tensor([[846356565, 849680903, 849140043],\n",
    "        [842706582, 849543264, 846890670],\n",
    "        [840797074, 846055878, 845897280]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_TASK_**: Initialize a **gpu device.** Then, initialize a random tensor with any size and store it in the device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3990, 0.5167, 0.0249, 0.9401, 0.9459, 0.7967],\n",
      "         [0.4150, 0.8203, 0.2290, 0.9096, 0.1183, 0.0752],\n",
      "         [0.4092, 0.9601, 0.2093, 0.1940, 0.8909, 0.4387]],\n",
      "\n",
      "        [[0.3570, 0.5454, 0.8299, 0.2099, 0.7684, 0.4290],\n",
      "         [0.2117, 0.6606, 0.1654, 0.4250, 0.9927, 0.6964],\n",
      "         [0.2472, 0.7028, 0.7494, 0.9303, 0.0494, 0.0750]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# initialize cuda device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tensor = torch.rand((2,3,6), device=device)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected Output :\n",
    "``` \n",
    "tensor([[[0.3990, 0.5167, 0.0249, 0.9401, 0.9459, 0.7967],\n",
    "         [0.4150, 0.8203, 0.2290, 0.9096, 0.1183, 0.0752],\n",
    "         [0.4092, 0.9601, 0.2093, 0.1940, 0.8909, 0.4387]],\n",
    "\n",
    "        [[0.3570, 0.5454, 0.8299, 0.2099, 0.7684, 0.4290],\n",
    "         [0.2117, 0.6606, 0.1654, 0.4250, 0.9927, 0.6964],\n",
    "         [0.2472, 0.7028, 0.7494, 0.9303, 0.0494, 0.0750]]], device='cuda:0')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
