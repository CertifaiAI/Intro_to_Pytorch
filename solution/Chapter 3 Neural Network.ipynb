{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Dataloader\n",
    "### What is Dataloader\n",
    "Dataloader is a class that helps with shuffling and organizing the data in minibatches. We can import this class from `torch.utils.data`.\n",
    "\n",
    "The job of a data loader is to sample minibatches from a dataset, giving us the flexibility to choose the size of our minibatch to be use for training in each iteration. The constructor takes a `Dataset` object as input, along with `batch_size` and a `shuffle` boolean variable that indicates whether the data needs to be shuffled at the beginning of each epoch.\n",
    "\n",
    "In this chapter, we are going to do classification task based on Fashion MNIST dataset. Fashion MNIST dataset could be directly imported and downloaded from `torchvision.datasets.FashionMNIST`. Pytorch has collected several datasets (CIFAR, COCO, Cityscapes, etc..) in the `torchvision` library, you may have a look of the full list of datasets at [here](https://pytorch.org/docs/stable/torchvision/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required library\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading/Downloading the FashionMNIST dataset, download might takes some time \n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = '../data',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor()\n",
    "    )\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root = '../data',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset into the `DataLoader` and input your desired batch size for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size = 32, shuffle = True)\n",
    "test_loader = DataLoader(test_set, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# A view of the DataLoader\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch\n",
    "\n",
    "# Output the size of each batch\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each images are assigned to one of the following labels:\n",
    "\n",
    "- 0 T-shirt/top\n",
    "- 1 Trouser\n",
    "- 2 Pullover\n",
    "- 3 Dress\n",
    "- 4 Coat\n",
    "- 5 Sandal\n",
    "- 6 Shirt\n",
    "- 7 Sneaker\n",
    "- 8 Bag\n",
    "- 9 Ankle boot\n",
    "\n",
    "Let us plot the image out to have a look on how does the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting numeric labels to text label\n",
    "\n",
    "def labelsText(labels):\n",
    "    labelDict = {\n",
    "                 0: \"T-shirt/Top\",\n",
    "                 1: \"Trouser\",\n",
    "                 2: \"Pullover\",\n",
    "                 3: \"Dress\",\n",
    "                 4: \"Coat\", \n",
    "                 5: \"Sandal\", \n",
    "                 6: \"Shirt\",\n",
    "                 7: \"Sneaker\",\n",
    "                 8: \"Bag\",\n",
    "                 9: \"Ankle Boot\"\n",
    "                 }\n",
    "    label = (labels.item() if type(labels) == torch.Tensor else labels)\n",
    "    return labelDict[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: \n",
      "Sandal, Shirt, Coat, Pullover, Shirt, Shirt, Dress, Trouser, Bag, T-shirt/Top, "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting out the images in the dataset\n",
    "\n",
    "grid = torchvision.utils.make_grid(images[0:10], nrow = 10)\n",
    "\n",
    "plt.figure(figsize = (15, 15))\n",
    "plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "\n",
    "print(\"Labels: \")\n",
    "for i in labels[0:10]:\n",
    "    print(labelsText(i) + \", \", end = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Build your first Neural Network (Subclassing nn.Module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Model Training\n",
    "We had loaded our dataset into training and testing set, now let us build a simple Feedfoward Neural Network to perform classification on this dataset.\n",
    "\n",
    "PyTorch has a whole submodule dedicated to neural networks, called `torch.nn`. It contains the building blocks needed to create all sorts of neural network architectures.\n",
    "\n",
    "To build a Neural Network, it could be done in two ways :\n",
    "- Calling the `nn.Sequential()` for fast implementation of the network\n",
    "- Subclassing `nn.Module` to have more flexibility on designing the network, eg: writing the your own `foward()` method\n",
    "\n",
    "\n",
    "Now let us start building the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to build a 4 layers neural network with ReLU activation function. Apply dropout with 20% probability to reduce the effect of overfitting. Let us try build our model using `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential()\n",
    "torch.manual_seed(0)\n",
    "model_sequential = nn.Sequential(nn.Linear(784,256),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(256,128),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(128,64),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(64,10),\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a wrapper function for our training called `training`. This wrapper function will take on parameters:\n",
    "- n_epochs\n",
    "- optimizer\n",
    "- model\n",
    "- loss_fn\n",
    "- train_loader\n",
    "- writer (Instance of Summary Writer to use TensorBoard for visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch does support TensorBoard which provides the visualization and tooling needed for machine learning experimentation. It is a useful tool that we can use during our training. Now let's define our training loop and implement some of the TensorBoard methods. \n",
    "\n",
    "If you wish to know more on TensorBoard, you can access it at [here](https://pytorch.org/docs/stable/tensorboard.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def training(n_epochs, optimizer, model, loss_fn, train_loader, writer):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            # Clearing gradient from previous mini-batch gradient computation  \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Reshape the tensor so that it fits the dimension of our input layer\n",
    "            # Get predictions output from the model\n",
    "            outputs = model(imgs.view(-1, 784))\n",
    "            \n",
    "            # Calculate the loss for curernt batch\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Calculating the gradient\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updating the weights and biases using optimizer.step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Summing up the loss over each epoch\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "            # Calculating the accuracy\n",
    "            predictions = torch.max(outputs, 1)[1]\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += len(labels)\n",
    "\n",
    "        accuracy = correct * 100 / total\n",
    "        writer.add_scalar('Loss ', loss_train / len(train_loader), epoch)\n",
    "        writer.add_scalar('Accuracy ', accuracy, epoch)\n",
    "        print('Epoch {}, Training loss {} , Accuracy {:.2f} %'.format(epoch, loss_train / len(train_loader), accuracy))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can open our TensorBoard in the terminal with the command of `tensorboard --logdir=runs`. Do remember change to the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready for training. Let's use `SGD` as our optimizer and `CrossEntropy` as loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 2.2894855969746906 , Accuracy 18.28 %\n",
      "Epoch 2, Training loss 2.2401039816538493 , Accuracy 28.62 %\n",
      "Epoch 3, Training loss 2.0709991912841796 , Accuracy 29.36 %\n",
      "Epoch 4, Training loss 1.6963001889546712 , Accuracy 35.94 %\n",
      "Epoch 5, Training loss 1.406517707824707 , Accuracy 46.59 %\n",
      "Epoch 6, Training loss 1.2153509410858154 , Accuracy 52.72 %\n",
      "Epoch 7, Training loss 1.103756807899475 , Accuracy 56.64 %\n",
      "Epoch 8, Training loss 1.0318541956583658 , Accuracy 59.33 %\n",
      "Epoch 9, Training loss 0.9726073985735575 , Accuracy 62.38 %\n",
      "Epoch 10, Training loss 0.931044387404124 , Accuracy 64.05 %\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model_SGD = model_sequential \n",
    "optimizer = optim.SGD(model_SGD.parameters(), lr = 1e-3) \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(comment = 'SGD')\n",
    "training(\n",
    "    n_epochs = 10,\n",
    "    optimizer = optimizer,\n",
    "    model = model_SGD,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    writer = writer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build another model which we set log softmax as the activation function at the output layer and uses Negative log-likelihood loss function. Compare the results for both of these setting. This time we are going to build by subclassing `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclassing nn.Module\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(784, 256)\n",
    "        self.act_1 = nn.ReLU()\n",
    "        self.fc_2 = nn.Linear(256, 128)\n",
    "        self.act_2 = nn.ReLU()\n",
    "        self.fc_3 = nn.Linear(128, 64)\n",
    "        self.act_3 = nn.ReLU()\n",
    "        self.fc_4 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.dropout(self.act_1(self.fc_1(x)))\n",
    "        out = self.dropout(self.act_2(self.fc_2(out)))\n",
    "        out = self.dropout(self.act_3(self.fc_3(out)))\n",
    "        # adding in softmax\n",
    "        out = F.log_softmax(self.fc_4(out), dim = 1)\n",
    "        return out\n",
    "    \n",
    "# Or you can use the Pytorch provided functional API when defining the forward method. Both of these are the same.\n",
    "\n",
    "class Classifier_F(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(784, 256)\n",
    "        self.fc_2 = nn.Linear(256, 128)\n",
    "        self.fc_3 = nn.Linear(128, 64)\n",
    "        self.fc_4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.dropout(F.relu(self.fc_1(x)), p = 0.2)\n",
    "        out = F.dropout(F.relu(self.fc_2(out)), p = 0.2)\n",
    "        out = F.dropout(F.relu(self.fc_3(out)), p = 0.2)\n",
    "        out = F.log_softmax(self.fc_4(out), dim = 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 2.2894890218098958 , Accuracy 18.39 %\n",
      "Epoch 2, Training loss 2.2399076170603434 , Accuracy 28.73 %\n",
      "Epoch 3, Training loss 2.068951116498311 , Accuracy 29.45 %\n",
      "Epoch 4, Training loss 1.695164651552836 , Accuracy 36.16 %\n",
      "Epoch 5, Training loss 1.4096814838409424 , Accuracy 46.69 %\n",
      "Epoch 6, Training loss 1.2168791191418966 , Accuracy 52.60 %\n",
      "Epoch 7, Training loss 1.1041425074577331 , Accuracy 56.25 %\n",
      "Epoch 8, Training loss 1.0339518047332763 , Accuracy 59.37 %\n",
      "Epoch 9, Training loss 0.9758181870142619 , Accuracy 62.02 %\n",
      "Epoch 10, Training loss 0.9312916868527731 , Accuracy 63.92 %\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model_SGD = Classifier() \n",
    "optimizer = optim.SGD(model_SGD.parameters(), lr = 1e-3) \n",
    "loss_fn = nn.NLLLoss()\n",
    "writer = SummaryWriter(comment = 'SGD')\n",
    "training(\n",
    "    n_epochs = 10,\n",
    "    optimizer = optimizer,\n",
    "    model = model_SGD,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    writer = writer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossEntropy is actually performing log softmax and negative log likelihood at the same time. Therefore during the construction of our model we could neglect the declaration of activation function at the output layer and save some memory during the backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try using other optimizer `Adam` to do our training. Optimizer is one of the hyperparameters that we can tune on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.5945944479823112 , Accuracy 78.32 %\n",
      "Epoch 2, Training loss 0.423241344755888 , Accuracy 84.78 %\n",
      "Epoch 3, Training loss 0.38519719421068827 , Accuracy 86.15 %\n",
      "Epoch 4, Training loss 0.36408053546349206 , Accuracy 86.94 %\n",
      "Epoch 5, Training loss 0.35000673046310743 , Accuracy 87.39 %\n",
      "Epoch 6, Training loss 0.3385574172397455 , Accuracy 87.74 %\n",
      "Epoch 7, Training loss 0.32801985016465185 , Accuracy 88.09 %\n",
      "Epoch 8, Training loss 0.3184917394856612 , Accuracy 88.41 %\n",
      "Epoch 9, Training loss 0.31102090905706087 , Accuracy 88.59 %\n",
      "Epoch 10, Training loss 0.3041634604026874 , Accuracy 88.89 %\n"
     ]
    }
   ],
   "source": [
    "model_Adam = Classifier() \n",
    "optimizer = optim.Adam(model_Adam.parameters(), lr = 1e-3) \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(comment = 'Adam')\n",
    "training(\n",
    "    n_epochs = 10,\n",
    "    optimizer = optimizer,\n",
    "    model = model_Adam,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    writer = writer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that `Adam` is performing better than the `SGD` with the same setting. Hyperparameter tuning is very important in order to obtain desired result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Model Saving\n",
    "After training the model, we would like to save it for future usages. There are some pretty useful functions you might need to familar with:\n",
    "\n",
    "- `torch.save`: It serialize the object to save to your machine. Models, tensors, and dictionaries of all kinds of objects can be saved using this function.\n",
    "- `torch.load`: This function uses pickle’s unpickling facilities to deserialize pickled object files to memory.\n",
    "- `torch.nn.Module.load_state_dict`: Loads a model’s parameter dictionary using a deserialized state_dict.\n",
    "\n",
    "If you wish to know more on model saving, you can access it at [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving only the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('../generated_model'):\n",
    "    os.mkdir('../generated_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the weights only of the model\n",
    "torch.save(model_Adam.state_dict(),  '../generated_model/mnist_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To load the state_dict, you must have an instance of the model\n",
    "modelLoad = Classifier()\n",
    "modelLoad.load_state_dict(torch.load('../generated_model/mnist_state_dict.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the entire model\n",
    "torch.save(model_Adam, '../generated_model/mnist_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "modelLoad = torch.load('../generated_model/mnist_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add-ons: Saving Model in ONNX format\n",
    "Pytorch also support saving model as ONNX (Open Neural Network Exchange) file type, which is a open format built to represent machine learning models. Let's see how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input : Float(32:784, 784:1),\n",
      "      %fc_1.weight : Float(256:784, 784:1),\n",
      "      %fc_1.bias : Float(256:1),\n",
      "      %fc_2.weight : Float(128:256, 256:1),\n",
      "      %fc_2.bias : Float(128:1),\n",
      "      %fc_3.weight : Float(64:128, 128:1),\n",
      "      %fc_3.bias : Float(64:1),\n",
      "      %fc_4.weight : Float(10:64, 64:1),\n",
      "      %fc_4.bias : Float(10:1)):\n",
      "  %9 : Float(32:256, 256:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%input, %fc_1.weight, %fc_1.bias) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1674:0\n",
      "  %10 : Float(32:256, 256:1) = onnx::Relu(%9) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:973:0\n",
      "  %11 : Float(32:128, 128:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%10, %fc_2.weight, %fc_2.bias) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1674:0\n",
      "  %12 : Float(32:128, 128:1) = onnx::Relu(%11) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:973:0\n",
      "  %13 : Float(32:64, 64:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%12, %fc_3.weight, %fc_3.bias) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1674:0\n",
      "  %14 : Float(32:64, 64:1) = onnx::Relu(%13) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:973:0\n",
      "  %15 : Float(32:10, 10:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%14, %fc_4.weight, %fc_4.bias) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1674:0\n",
      "  %output : Float(32:10, 10:1) = onnx::LogSoftmax[axis=1](%15) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1591:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx \n",
    "dummy_input = torch.randn(32, 784, requires_grad = True)\n",
    "torch.onnx.export(model_Adam, dummy_input, '../generated_model/model.onnx', verbose = True, input_names = ['input'], output_names = ['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "#loading the onnx format model\n",
    "model = onnx.load('../generated_model/model.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Inference\n",
    "Sometimes, we would like to inference on the trained model to evaluate the performance. `model.eval()` will set the model to evaluation(inference) mode to set dropout, batch normalization layers, etc.. to evaluation mode. Evaluation mode will disable the usage of dropout and batch normalization during the `foward` method as it is not required during the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc_1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (act_1): ReLU()\n",
       "  (fc_2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (act_2): ReLU()\n",
       "  (fc_3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (act_3): ReLU()\n",
       "  (fc_4): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using previous loaded model\n",
    "modelLoad.eval()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting it to inference mode, we could pass in test data with the setting of \n",
    "```python \n",
    "with torch.no_grad():\n",
    "``` \n",
    "as we do not have to calculate the gradient during the inference, this can help us save some memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 88.09 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = modelLoad(images.view(-1, 784))\n",
    "        predictions = torch.max(outputs, 1)[1]\n",
    "        correct += (predictions == labels).sum()\n",
    "        total += len(labels)\n",
    "    accuracy_test = correct.item() * 100 / total\n",
    "print(\"Test Accuracy : {:.2f} %\".format(accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Build your first Neural Network (Sequential Model)\n",
    "### 3.3.1 Model Training\n",
    "\n",
    "Altough there are many other machine learning techniques to tackle multi-variate linear regression, it would be interesting for us to tackle it using deep learning for learning purposes.\n",
    "<br>In this sub-section, we will try to perform said regression using PyTorch `SequentialModel` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Real Estate dataset from the `realEstate.csv` for our linear regression example. \n",
    "\n",
    "Description of data:\n",
    "- House Age\n",
    "- Distance from the unit to MRT station\n",
    "- The number of Convenience Stores around the unit\n",
    "- House Unit Price per 1000 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use pandas to load in the csv.<br>\n",
    "Note that in this dataset there are a total of $3$ features and $1$ label.<br>\n",
    "Thus from the data we will use `.iloc[]` to distinguish the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/Regression/realEstate.csv\", header = 0)\n",
    "n_features = 3\n",
    "X = data.iloc[:, 0:3].values\n",
    "y = data.iloc[:, 3].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, we split our dataset into 70/30 train/test ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, shuffle = True, random_state = 1022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform feature scaling onto `X_train` and `X_test` using `StandardScaler` from `scikit-learn`.<br>\n",
    "*Note: only fit the train_set but transform both train and test sets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 3.1, we've touch on how Dataloaders are initialized and used in model training. It was simple, which is to pass in whatever `Dataset` we need into the Dataloader initializer. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using a custom dataset from a csv file as compared to the previous one which was prepared readily from torchvision. Thus in this case, we will have to build our own by subclassing from `torch.utils.data.Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst subclassing `Dataset`, PyTorch [documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) notes that we have to override the `__getitem__()` method and optionally the `__len__()` method.<br>\n",
    "We will mainly have three methods in this `Dataset` class:\n",
    "- `__init__(self, data, label)`: helps us pass in the feature and labels into the dataset\n",
    "- `__len__(self)`:allows the dataset to know how many instances of data there is \n",
    "- `__getitem__(self, idx)`:allows the dataset to get items from the data and labels by indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype = torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype  = torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After feature scaling, we initialize our custom datasets and put them into `Dataloader` constructor and our data is prepared. The next step will be modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Custom_Dataset(X_train, y_train)\n",
    "test_dataset = Custom_Dataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 128 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we previously stated, there are two approaches of modeling.\n",
    "- Subclassing `nn.Module` \n",
    "- Calling the `nn.Sequential()` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.Sequential` is a simple function that accepts a list of `nn.Modules` and returns a model with all the sequential layers. We will be implementing these few layers:\n",
    "1. nn.Linear(3,50)\n",
    "2. nn.ReLU()\n",
    "3. nn.Linear(50,25)\n",
    "4. nn.ReLU()\n",
    "5. nn.Linear(25,10)\n",
    "6. nn.ReLU()\n",
    "7. nn.Linear(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model_sequential = nn.Sequential(nn.Linear(n_features, 50),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(50, 25),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(25, 10),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(10, 1)\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this regression probelm, the loss/criterion we will use is Mean-Squared-Error loss, which in PyTorch is `nn.MSELoss()`<br>\n",
    "We will also choose to use `Adam` as our optimizer.<br> Remember, `torch.optim.*any_optimizer*` accepts `model.parameters()` to keep track of the model's parameters, hence we should always initialize our model first before our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_sequential.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our modeling is done, let's commence our training with using the training loop that defined previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a wrapper function for our training called `train_model`. This wrapper function will take on parameters:\n",
    "- model\n",
    "- loader\n",
    "- loss_function/criterion\n",
    "- optimizer\n",
    "- number_of_epochs (optional)\n",
    "- iteration_check (optional): *if False is passed in, losses of each iteration per epoch will not be printed>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below will be an overall workings an explaination of our train_model function:\n",
    "1. In each epoch, each minibatch starts with `optimizer.zero_grad()`. This is to clear previously computed gradients from previous minibatches.\n",
    "2. We get the features and labels by indexing our minibatch.\n",
    "3. Compute forward propagation by calling `model(features)` and assigning it to a variable `prediction`\n",
    "4. Compute the loss by calling `criterion(prediction, torch.unsqueeze(labels, dim=1))`\n",
    "    - the reason we unsqueeze is to make sure the shape of the labels are the same as the predictions, which is (batch_size,1) \n",
    "5. Compute backward propagation by calling `loss.backward()`\n",
    "6. Update the parameters(learning rate etc.) of the model by calling `optimizer.step()`\n",
    "7. Increment our `running_loss` with the loss of our current batch\n",
    "8. At the end of each epoch, compute the accuracy by dividing the accumulated loss and the amount of data samples, and finally zero the `running_loss` for the next epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, criterion, optimizer,epochs=5000):\n",
    "#   this running_loss will keep track of the losses of every epoch from each respective iteration\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for i, data in enumerate(loader):\n",
    "#           zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            features, labels = data[0],data[1]\n",
    "            prediction = model(features)\n",
    "            loss = criterion(prediction, torch.unsqueeze(labels,dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if (epoch % 100 == 0 or epoch == 1):\n",
    "            print(f\"Epoch {epoch} Loss: {running_loss / len(loader)}\")     \n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1559.314471435547\n",
      "Epoch 100 Loss: 61.67083594799042\n",
      "Epoch 200 Loss: 57.53920102566481\n",
      "Epoch 300 Loss: 54.614624582976106\n",
      "Epoch 400 Loss: 51.69376365095377\n",
      "Epoch 500 Loss: 49.110941734910014\n",
      "Epoch 600 Loss: 44.46782956123352\n",
      "Epoch 700 Loss: 45.49254035949707\n",
      "Epoch 800 Loss: 45.39475156664848\n",
      "Epoch 900 Loss: 43.348855590820314\n",
      "Epoch 1000 Loss: 42.04828781485558\n",
      "Epoch 1100 Loss: 39.37081394195557\n",
      "Epoch 1200 Loss: 42.60350239276886\n",
      "Epoch 1300 Loss: 38.945985350012776\n",
      "Epoch 1400 Loss: 39.63016664907336\n",
      "Epoch 1500 Loss: 36.81087758541107\n",
      "Epoch 1600 Loss: 34.936926842236424\n",
      "Epoch 1700 Loss: 35.42953658103943\n",
      "Epoch 1800 Loss: 32.789571383502334\n",
      "Epoch 1900 Loss: 34.93219475212682\n",
      "Epoch 2000 Loss: 33.54853103160858\n",
      "Epoch 2100 Loss: 28.336665666103364\n",
      "Epoch 2200 Loss: 25.664763996377587\n",
      "Epoch 2300 Loss: 24.103572607040405\n",
      "Epoch 2400 Loss: 17.353846311569214\n",
      "Epoch 2500 Loss: 15.863344663381577\n",
      "Epoch 2600 Loss: 13.111431193351745\n",
      "Epoch 2700 Loss: 12.318226540088654\n",
      "Epoch 2800 Loss: 19.141652542352677\n",
      "Epoch 2900 Loss: 17.75134304985404\n",
      "Epoch 3000 Loss: 16.94328822637908\n",
      "Epoch 3100 Loss: 18.66891082525253\n",
      "Epoch 3200 Loss: 17.63850952475368\n",
      "Epoch 3300 Loss: 14.46680794209242\n",
      "Epoch 3400 Loss: 21.239836806058882\n",
      "Epoch 3500 Loss: 20.83810586631298\n",
      "Epoch 3600 Loss: 15.835954087972642\n",
      "Epoch 3700 Loss: 22.814937913417815\n",
      "Epoch 3800 Loss: 18.055061160423794\n",
      "Epoch 3900 Loss: 18.024778324365617\n",
      "Epoch 4000 Loss: 19.730439281463624\n",
      "Epoch 4100 Loss: 16.410921066999435\n",
      "Epoch 4200 Loss: 14.806035457924008\n",
      "Epoch 4300 Loss: 12.136985358595847\n",
      "Epoch 4400 Loss: 20.251971996575595\n",
      "Epoch 4500 Loss: 17.372333994880318\n",
      "Epoch 4600 Loss: 18.757385206222533\n",
      "Epoch 4700 Loss: 18.26095001846552\n",
      "Epoch 4800 Loss: 19.056522417068482\n",
      "Epoch 4900 Loss: 22.027928829193115\n",
      "Epoch 5000 Loss: 16.290424835681915\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "train_model(model_sequential, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Inference\n",
    "\n",
    "Now let's evaluate our model. Use `model.eval()` to set the model to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=25, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=25, out_features=10, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sequential.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say your house age is 10, distance to MRT is 100 meters, and there are 6 convenience stores around the unit, could you predict your house price? Let's use our trained model to find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for your house price is : 54032.859802246094\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inference = torch.tensor([[10, 100, 6]])\n",
    "    inference = torch.from_numpy(scaler.transform(inference))\n",
    "    predict = model_sequential.forward(inference.float())\n",
    "        \n",
    "print(\"The prediction for your house price is :\", predict.item() * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will try to build a classifier for our MNIST Handwriting dataset.\n",
    "\n",
    "Construct transform with the following transforms:\n",
    "- coverting to tensor\n",
    "- normalize the tensor with mean=0.15 and std=0.3081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.15,), (0.3081,))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the MNIST dataset from `torchvision.datasets`. Load them into respective `Dataloaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "train = MNIST(\"../data\", download = True, transform = transform, train = True)\n",
    "test = MNIST(\"../data\", download = True, transform = transform, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, 100, shuffle = True, num_workers = 0)\n",
    "test_loader = DataLoader(test, 100, shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare `SummaryWriter` for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Model with the following layers:\n",
    "- 4 linear/dense layers\n",
    "- First 3 with ReLU activation functions\n",
    "\n",
    "*Note: Remember to resize the incoming tensor first*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features = 28 * 28, out_features = 1000)\n",
    "        self.fc2 = nn.Linear(in_features = 1000, out_features = 500)\n",
    "        self.fc3 = nn.Linear(in_features = 500, out_features = 100)\n",
    "        self.fc4 = nn.Linear(in_features = 100, out_features = 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model and load it to our **GPU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize criterion: `CrossEntropyLoss` and optimizer `Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a wrapper function `train_model` to train the model using `CUDA`. `add_scalar` which shows a loss against epoch graph on TensorBoard.<br>\n",
    "Here is a checklist for you to keep check what to do:\n",
    "1. For each iteration in each epoch, zero the gradients of the parameters\n",
    "2. Forward propagate\n",
    "3. Calculate loss\n",
    "4. Write the loss and train to TensorBoard\n",
    "5. Back propagate\n",
    "6. Update the parameters\n",
    "7. For each epoch, calculate the accuracy on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \n",
      "Iteration:1 \n",
      "Loss:2.3120129108428955\n",
      "Epoch:1 \n",
      "Iteration:2 \n",
      "Loss:2.243009090423584\n",
      "Epoch:1 \n",
      "Iteration:3 \n",
      "Loss:2.103304862976074\n",
      "Epoch:1 \n",
      "Iteration:4 \n",
      "Loss:1.938184380531311\n",
      "Epoch:1 \n",
      "Iteration:5 \n",
      "Loss:1.7360073328018188\n",
      "Epoch:1 \n",
      "Iteration:6 \n",
      "Loss:1.4185400009155273\n",
      "Epoch:1 \n",
      "Iteration:7 \n",
      "Loss:1.3077017068862915\n",
      "Epoch:1 \n",
      "Iteration:8 \n",
      "Loss:0.979144811630249\n",
      "Epoch:1 \n",
      "Iteration:9 \n",
      "Loss:0.8673616051673889\n",
      "Epoch:1 \n",
      "Iteration:10 \n",
      "Loss:0.7848854660987854\n",
      "Epoch:1 \n",
      "Iteration:11 \n",
      "Loss:0.9053862690925598\n",
      "Epoch:1 \n",
      "Iteration:12 \n",
      "Loss:0.7195755243301392\n",
      "Epoch:1 \n",
      "Iteration:13 \n",
      "Loss:0.8481418490409851\n",
      "Epoch:1 \n",
      "Iteration:14 \n",
      "Loss:0.7561290860176086\n",
      "Epoch:1 \n",
      "Iteration:15 \n",
      "Loss:0.6766089797019958\n",
      "Epoch:1 \n",
      "Iteration:16 \n",
      "Loss:0.5629891157150269\n",
      "Epoch:1 \n",
      "Iteration:17 \n",
      "Loss:0.6399906277656555\n",
      "Epoch:1 \n",
      "Iteration:18 \n",
      "Loss:0.6289631128311157\n",
      "Epoch:1 \n",
      "Iteration:19 \n",
      "Loss:0.7176056504249573\n",
      "Epoch:1 \n",
      "Iteration:20 \n",
      "Loss:0.6771294474601746\n",
      "Epoch:1 \n",
      "Iteration:21 \n",
      "Loss:0.43189147114753723\n",
      "Epoch:1 \n",
      "Iteration:22 \n",
      "Loss:0.5592374801635742\n",
      "Epoch:1 \n",
      "Iteration:23 \n",
      "Loss:0.6598607897758484\n",
      "Epoch:1 \n",
      "Iteration:24 \n",
      "Loss:0.5911179184913635\n",
      "Epoch:1 \n",
      "Iteration:25 \n",
      "Loss:0.5268451571464539\n",
      "Epoch:1 \n",
      "Iteration:26 \n",
      "Loss:0.34881362318992615\n",
      "Epoch:1 \n",
      "Iteration:27 \n",
      "Loss:0.4407460689544678\n",
      "Epoch:1 \n",
      "Iteration:28 \n",
      "Loss:0.3689185380935669\n",
      "Epoch:1 \n",
      "Iteration:29 \n",
      "Loss:0.4429815411567688\n",
      "Epoch:1 \n",
      "Iteration:30 \n",
      "Loss:0.6116948127746582\n",
      "Epoch:1 \n",
      "Iteration:31 \n",
      "Loss:0.6002848148345947\n",
      "Epoch:1 \n",
      "Iteration:32 \n",
      "Loss:0.5611258149147034\n",
      "Epoch:1 \n",
      "Iteration:33 \n",
      "Loss:0.666802167892456\n",
      "Epoch:1 \n",
      "Iteration:34 \n",
      "Loss:0.42656680941581726\n",
      "Epoch:1 \n",
      "Iteration:35 \n",
      "Loss:0.5240041017532349\n",
      "Epoch:1 \n",
      "Iteration:36 \n",
      "Loss:0.29298585653305054\n",
      "Epoch:1 \n",
      "Iteration:37 \n",
      "Loss:0.508000910282135\n",
      "Epoch:1 \n",
      "Iteration:38 \n",
      "Loss:0.46748197078704834\n",
      "Epoch:1 \n",
      "Iteration:39 \n",
      "Loss:0.33440831303596497\n",
      "Epoch:1 \n",
      "Iteration:40 \n",
      "Loss:0.6263558864593506\n",
      "Epoch:1 \n",
      "Iteration:41 \n",
      "Loss:0.3841659128665924\n",
      "Epoch:1 \n",
      "Iteration:42 \n",
      "Loss:0.3341437876224518\n",
      "Epoch:1 \n",
      "Iteration:43 \n",
      "Loss:0.45079100131988525\n",
      "Epoch:1 \n",
      "Iteration:44 \n",
      "Loss:0.44888055324554443\n",
      "Epoch:1 \n",
      "Iteration:45 \n",
      "Loss:0.46881726384162903\n",
      "Epoch:1 \n",
      "Iteration:46 \n",
      "Loss:0.31650739908218384\n",
      "Epoch:1 \n",
      "Iteration:47 \n",
      "Loss:0.39410457015037537\n",
      "Epoch:1 \n",
      "Iteration:48 \n",
      "Loss:0.41094696521759033\n",
      "Epoch:1 \n",
      "Iteration:49 \n",
      "Loss:0.5384355783462524\n",
      "Epoch:1 \n",
      "Iteration:50 \n",
      "Loss:0.333896279335022\n",
      "Epoch:1 \n",
      "Iteration:51 \n",
      "Loss:0.40745270252227783\n",
      "Epoch:1 \n",
      "Iteration:52 \n",
      "Loss:0.37681883573532104\n",
      "Epoch:1 \n",
      "Iteration:53 \n",
      "Loss:0.30641573667526245\n",
      "Epoch:1 \n",
      "Iteration:54 \n",
      "Loss:0.255819708108902\n",
      "Epoch:1 \n",
      "Iteration:55 \n",
      "Loss:0.34836751222610474\n",
      "Epoch:1 \n",
      "Iteration:56 \n",
      "Loss:0.19757182896137238\n",
      "Epoch:1 \n",
      "Iteration:57 \n",
      "Loss:0.3527303636074066\n",
      "Epoch:1 \n",
      "Iteration:58 \n",
      "Loss:0.26464834809303284\n",
      "Epoch:1 \n",
      "Iteration:59 \n",
      "Loss:0.369119256734848\n",
      "Epoch:1 \n",
      "Iteration:60 \n",
      "Loss:0.2571631073951721\n",
      "Epoch:1 \n",
      "Iteration:61 \n",
      "Loss:0.28121277689933777\n",
      "Epoch:1 \n",
      "Iteration:62 \n",
      "Loss:0.2682133913040161\n",
      "Epoch:1 \n",
      "Iteration:63 \n",
      "Loss:0.29948434233665466\n",
      "Epoch:1 \n",
      "Iteration:64 \n",
      "Loss:0.2584401071071625\n",
      "Epoch:1 \n",
      "Iteration:65 \n",
      "Loss:0.1802312135696411\n",
      "Epoch:1 \n",
      "Iteration:66 \n",
      "Loss:0.3833845555782318\n",
      "Epoch:1 \n",
      "Iteration:67 \n",
      "Loss:0.19513483345508575\n",
      "Epoch:1 \n",
      "Iteration:68 \n",
      "Loss:0.271013081073761\n",
      "Epoch:1 \n",
      "Iteration:69 \n",
      "Loss:0.3728995621204376\n",
      "Epoch:1 \n",
      "Iteration:70 \n",
      "Loss:0.33933544158935547\n",
      "Epoch:1 \n",
      "Iteration:71 \n",
      "Loss:0.24964481592178345\n",
      "Epoch:1 \n",
      "Iteration:72 \n",
      "Loss:0.1458766907453537\n",
      "Epoch:1 \n",
      "Iteration:73 \n",
      "Loss:0.2318328619003296\n",
      "Epoch:1 \n",
      "Iteration:74 \n",
      "Loss:0.35721245408058167\n",
      "Epoch:1 \n",
      "Iteration:75 \n",
      "Loss:0.27163660526275635\n",
      "Epoch:1 \n",
      "Iteration:76 \n",
      "Loss:0.2832474410533905\n",
      "Epoch:1 \n",
      "Iteration:77 \n",
      "Loss:0.28073111176490784\n",
      "Epoch:1 \n",
      "Iteration:78 \n",
      "Loss:0.18299219012260437\n",
      "Epoch:1 \n",
      "Iteration:79 \n",
      "Loss:0.19779279828071594\n",
      "Epoch:1 \n",
      "Iteration:80 \n",
      "Loss:0.32478073239326477\n",
      "Epoch:1 \n",
      "Iteration:81 \n",
      "Loss:0.5900135636329651\n",
      "Epoch:1 \n",
      "Iteration:82 \n",
      "Loss:0.25531479716300964\n",
      "Epoch:1 \n",
      "Iteration:83 \n",
      "Loss:0.1387094408273697\n",
      "Epoch:1 \n",
      "Iteration:84 \n",
      "Loss:0.24132877588272095\n",
      "Epoch:1 \n",
      "Iteration:85 \n",
      "Loss:0.30440443754196167\n",
      "Epoch:1 \n",
      "Iteration:86 \n",
      "Loss:0.36016371846199036\n",
      "Epoch:1 \n",
      "Iteration:87 \n",
      "Loss:0.2922019362449646\n",
      "Epoch:1 \n",
      "Iteration:88 \n",
      "Loss:0.19534306228160858\n",
      "Epoch:1 \n",
      "Iteration:89 \n",
      "Loss:0.25497016310691833\n",
      "Epoch:1 \n",
      "Iteration:90 \n",
      "Loss:0.31121373176574707\n",
      "Epoch:1 \n",
      "Iteration:91 \n",
      "Loss:0.19418592751026154\n",
      "Epoch:1 \n",
      "Iteration:92 \n",
      "Loss:0.31320226192474365\n",
      "Epoch:1 \n",
      "Iteration:93 \n",
      "Loss:0.17160598933696747\n",
      "Epoch:1 \n",
      "Iteration:94 \n",
      "Loss:0.3011602759361267\n",
      "Epoch:1 \n",
      "Iteration:95 \n",
      "Loss:0.2664591073989868\n",
      "Epoch:1 \n",
      "Iteration:96 \n",
      "Loss:0.34600746631622314\n",
      "Epoch:1 \n",
      "Iteration:97 \n",
      "Loss:0.15365272760391235\n",
      "Epoch:1 \n",
      "Iteration:98 \n",
      "Loss:0.3775344789028168\n",
      "Epoch:1 \n",
      "Iteration:99 \n",
      "Loss:0.2457958608865738\n",
      "Epoch:1 \n",
      "Iteration:100 \n",
      "Loss:0.20113806426525116\n",
      "Epoch:1 \n",
      "Iteration:101 \n",
      "Loss:0.2597142457962036\n",
      "Epoch:1 \n",
      "Iteration:102 \n",
      "Loss:0.3785651922225952\n",
      "Epoch:1 \n",
      "Iteration:103 \n",
      "Loss:0.2910976707935333\n",
      "Epoch:1 \n",
      "Iteration:104 \n",
      "Loss:0.32003748416900635\n",
      "Epoch:1 \n",
      "Iteration:105 \n",
      "Loss:0.3702780604362488\n",
      "Epoch:1 \n",
      "Iteration:106 \n",
      "Loss:0.2612571716308594\n",
      "Epoch:1 \n",
      "Iteration:107 \n",
      "Loss:0.20184503495693207\n",
      "Epoch:1 \n",
      "Iteration:108 \n",
      "Loss:0.30470114946365356\n",
      "Epoch:1 \n",
      "Iteration:109 \n",
      "Loss:0.2566049098968506\n",
      "Epoch:1 \n",
      "Iteration:110 \n",
      "Loss:0.30411121249198914\n",
      "Epoch:1 \n",
      "Iteration:111 \n",
      "Loss:0.28380855917930603\n",
      "Epoch:1 \n",
      "Iteration:112 \n",
      "Loss:0.3514578342437744\n",
      "Epoch:1 \n",
      "Iteration:113 \n",
      "Loss:0.3024035096168518\n",
      "Epoch:1 \n",
      "Iteration:114 \n",
      "Loss:0.3007325232028961\n",
      "Epoch:1 \n",
      "Iteration:115 \n",
      "Loss:0.23290877044200897\n",
      "Epoch:1 \n",
      "Iteration:116 \n",
      "Loss:0.3148707449436188\n",
      "Epoch:1 \n",
      "Iteration:117 \n",
      "Loss:0.1848905235528946\n",
      "Epoch:1 \n",
      "Iteration:118 \n",
      "Loss:0.15175089240074158\n",
      "Epoch:1 \n",
      "Iteration:119 \n",
      "Loss:0.21671690046787262\n",
      "Epoch:1 \n",
      "Iteration:120 \n",
      "Loss:0.2821066379547119\n",
      "Epoch:1 \n",
      "Iteration:121 \n",
      "Loss:0.25291892886161804\n",
      "Epoch:1 \n",
      "Iteration:122 \n",
      "Loss:0.21602274477481842\n",
      "Epoch:1 \n",
      "Iteration:123 \n",
      "Loss:0.1897137463092804\n",
      "Epoch:1 \n",
      "Iteration:124 \n",
      "Loss:0.18207958340644836\n",
      "Epoch:1 \n",
      "Iteration:125 \n",
      "Loss:0.22877010703086853\n",
      "Epoch:1 \n",
      "Iteration:126 \n",
      "Loss:0.24930299818515778\n",
      "Epoch:1 \n",
      "Iteration:127 \n",
      "Loss:0.2413063496351242\n",
      "Epoch:1 \n",
      "Iteration:128 \n",
      "Loss:0.24214984476566315\n",
      "Epoch:1 \n",
      "Iteration:129 \n",
      "Loss:0.30584508180618286\n",
      "Epoch:1 \n",
      "Iteration:130 \n",
      "Loss:0.13950319588184357\n",
      "Epoch:1 \n",
      "Iteration:131 \n",
      "Loss:0.3294107913970947\n",
      "Epoch:1 \n",
      "Iteration:132 \n",
      "Loss:0.28691741824150085\n",
      "Epoch:1 \n",
      "Iteration:133 \n",
      "Loss:0.13431206345558167\n",
      "Epoch:1 \n",
      "Iteration:134 \n",
      "Loss:0.22785593569278717\n",
      "Epoch:1 \n",
      "Iteration:135 \n",
      "Loss:0.2721596360206604\n",
      "Epoch:1 \n",
      "Iteration:136 \n",
      "Loss:0.13127829134464264\n",
      "Epoch:1 \n",
      "Iteration:137 \n",
      "Loss:0.252728670835495\n",
      "Epoch:1 \n",
      "Iteration:138 \n",
      "Loss:0.2905021011829376\n",
      "Epoch:1 \n",
      "Iteration:139 \n",
      "Loss:0.22047342360019684\n",
      "Epoch:1 \n",
      "Iteration:140 \n",
      "Loss:0.48403823375701904\n",
      "Epoch:1 \n",
      "Iteration:141 \n",
      "Loss:0.15702365338802338\n",
      "Epoch:1 \n",
      "Iteration:142 \n",
      "Loss:0.2312907576560974\n",
      "Epoch:1 \n",
      "Iteration:143 \n",
      "Loss:0.319017618894577\n",
      "Epoch:1 \n",
      "Iteration:144 \n",
      "Loss:0.30859729647636414\n",
      "Epoch:1 \n",
      "Iteration:145 \n",
      "Loss:0.255501389503479\n",
      "Epoch:1 \n",
      "Iteration:146 \n",
      "Loss:0.19951361417770386\n",
      "Epoch:1 \n",
      "Iteration:147 \n",
      "Loss:0.15566644072532654\n",
      "Epoch:1 \n",
      "Iteration:148 \n",
      "Loss:0.291554719209671\n",
      "Epoch:1 \n",
      "Iteration:149 \n",
      "Loss:0.2254704236984253\n",
      "Epoch:1 \n",
      "Iteration:150 \n",
      "Loss:0.3103751838207245\n",
      "Epoch:1 \n",
      "Iteration:151 \n",
      "Loss:0.23330779373645782\n",
      "Epoch:1 \n",
      "Iteration:152 \n",
      "Loss:0.19241328537464142\n",
      "Epoch:1 \n",
      "Iteration:153 \n",
      "Loss:0.275493860244751\n",
      "Epoch:1 \n",
      "Iteration:154 \n",
      "Loss:0.16837461292743683\n",
      "Epoch:1 \n",
      "Iteration:155 \n",
      "Loss:0.19802793860435486\n",
      "Epoch:1 \n",
      "Iteration:156 \n",
      "Loss:0.3491751551628113\n",
      "Epoch:1 \n",
      "Iteration:157 \n",
      "Loss:0.43757039308547974\n",
      "Epoch:1 \n",
      "Iteration:158 \n",
      "Loss:0.1347665637731552\n",
      "Epoch:1 \n",
      "Iteration:159 \n",
      "Loss:0.06288599222898483\n",
      "Epoch:1 \n",
      "Iteration:160 \n",
      "Loss:0.3957083523273468\n",
      "Epoch:1 \n",
      "Iteration:161 \n",
      "Loss:0.3086799383163452\n",
      "Epoch:1 \n",
      "Iteration:162 \n",
      "Loss:0.12891536951065063\n",
      "Epoch:1 \n",
      "Iteration:163 \n",
      "Loss:0.1177622377872467\n",
      "Epoch:1 \n",
      "Iteration:164 \n",
      "Loss:0.08659529685974121\n",
      "Epoch:1 \n",
      "Iteration:165 \n",
      "Loss:0.21118642389774323\n",
      "Epoch:1 \n",
      "Iteration:166 \n",
      "Loss:0.2700453996658325\n",
      "Epoch:1 \n",
      "Iteration:167 \n",
      "Loss:0.31985050439834595\n",
      "Epoch:1 \n",
      "Iteration:168 \n",
      "Loss:0.19454088807106018\n",
      "Epoch:1 \n",
      "Iteration:169 \n",
      "Loss:0.19190970063209534\n",
      "Epoch:1 \n",
      "Iteration:170 \n",
      "Loss:0.26456591486930847\n",
      "Epoch:1 \n",
      "Iteration:171 \n",
      "Loss:0.27670493721961975\n",
      "Epoch:1 \n",
      "Iteration:172 \n",
      "Loss:0.31067776679992676\n",
      "Epoch:1 \n",
      "Iteration:173 \n",
      "Loss:0.2188132256269455\n",
      "Epoch:1 \n",
      "Iteration:174 \n",
      "Loss:0.23364071547985077\n",
      "Epoch:1 \n",
      "Iteration:175 \n",
      "Loss:0.1508840024471283\n",
      "Epoch:1 \n",
      "Iteration:176 \n",
      "Loss:0.23261654376983643\n",
      "Epoch:1 \n",
      "Iteration:177 \n",
      "Loss:0.23869295418262482\n",
      "Epoch:1 \n",
      "Iteration:178 \n",
      "Loss:0.18907514214515686\n",
      "Epoch:1 \n",
      "Iteration:179 \n",
      "Loss:0.16858088970184326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \n",
      "Iteration:180 \n",
      "Loss:0.3811591863632202\n",
      "Epoch:1 \n",
      "Iteration:181 \n",
      "Loss:0.124379463493824\n",
      "Epoch:1 \n",
      "Iteration:182 \n",
      "Loss:0.31516680121421814\n",
      "Epoch:1 \n",
      "Iteration:183 \n",
      "Loss:0.31521129608154297\n",
      "Epoch:1 \n",
      "Iteration:184 \n",
      "Loss:0.1377878487110138\n",
      "Epoch:1 \n",
      "Iteration:185 \n",
      "Loss:0.18656201660633087\n",
      "Epoch:1 \n",
      "Iteration:186 \n",
      "Loss:0.1332310140132904\n",
      "Epoch:1 \n",
      "Iteration:187 \n",
      "Loss:0.19603176414966583\n",
      "Epoch:1 \n",
      "Iteration:188 \n",
      "Loss:0.19425024092197418\n",
      "Epoch:1 \n",
      "Iteration:189 \n",
      "Loss:0.13017690181732178\n",
      "Epoch:1 \n",
      "Iteration:190 \n",
      "Loss:0.13372930884361267\n",
      "Epoch:1 \n",
      "Iteration:191 \n",
      "Loss:0.1804359406232834\n",
      "Epoch:1 \n",
      "Iteration:192 \n",
      "Loss:0.2930707633495331\n",
      "Epoch:1 \n",
      "Iteration:193 \n",
      "Loss:0.14517799019813538\n",
      "Epoch:1 \n",
      "Iteration:194 \n",
      "Loss:0.32116764783859253\n",
      "Epoch:1 \n",
      "Iteration:195 \n",
      "Loss:0.14592529833316803\n",
      "Epoch:1 \n",
      "Iteration:196 \n",
      "Loss:0.1323719173669815\n",
      "Epoch:1 \n",
      "Iteration:197 \n",
      "Loss:0.23956510424613953\n",
      "Epoch:1 \n",
      "Iteration:198 \n",
      "Loss:0.1127374917268753\n",
      "Epoch:1 \n",
      "Iteration:199 \n",
      "Loss:0.1258353739976883\n",
      "Epoch:1 \n",
      "Iteration:200 \n",
      "Loss:0.13518422842025757\n",
      "Epoch:1 \n",
      "Iteration:201 \n",
      "Loss:0.19253356754779816\n",
      "Epoch:1 \n",
      "Iteration:202 \n",
      "Loss:0.2624629735946655\n",
      "Epoch:1 \n",
      "Iteration:203 \n",
      "Loss:0.19795076549053192\n",
      "Epoch:1 \n",
      "Iteration:204 \n",
      "Loss:0.22124363481998444\n",
      "Epoch:1 \n",
      "Iteration:205 \n",
      "Loss:0.13537326455116272\n",
      "Epoch:1 \n",
      "Iteration:206 \n",
      "Loss:0.07512052357196808\n",
      "Epoch:1 \n",
      "Iteration:207 \n",
      "Loss:0.3330065608024597\n",
      "Epoch:1 \n",
      "Iteration:208 \n",
      "Loss:0.30471986532211304\n",
      "Epoch:1 \n",
      "Iteration:209 \n",
      "Loss:0.16882148385047913\n",
      "Epoch:1 \n",
      "Iteration:210 \n",
      "Loss:0.27691519260406494\n",
      "Epoch:1 \n",
      "Iteration:211 \n",
      "Loss:0.1952899694442749\n",
      "Epoch:1 \n",
      "Iteration:212 \n",
      "Loss:0.19514437019824982\n",
      "Epoch:1 \n",
      "Iteration:213 \n",
      "Loss:0.2352014183998108\n",
      "Epoch:1 \n",
      "Iteration:214 \n",
      "Loss:0.12370365113019943\n",
      "Epoch:1 \n",
      "Iteration:215 \n",
      "Loss:0.274437814950943\n",
      "Epoch:1 \n",
      "Iteration:216 \n",
      "Loss:0.06159636005759239\n",
      "Epoch:1 \n",
      "Iteration:217 \n",
      "Loss:0.32039517164230347\n",
      "Epoch:1 \n",
      "Iteration:218 \n",
      "Loss:0.1663617342710495\n",
      "Epoch:1 \n",
      "Iteration:219 \n",
      "Loss:0.1241391971707344\n",
      "Epoch:1 \n",
      "Iteration:220 \n",
      "Loss:0.2916605472564697\n",
      "Epoch:1 \n",
      "Iteration:221 \n",
      "Loss:0.26142963767051697\n",
      "Epoch:1 \n",
      "Iteration:222 \n",
      "Loss:0.1519048511981964\n",
      "Epoch:1 \n",
      "Iteration:223 \n",
      "Loss:0.16335809230804443\n",
      "Epoch:1 \n",
      "Iteration:224 \n",
      "Loss:0.07499317079782486\n",
      "Epoch:1 \n",
      "Iteration:225 \n",
      "Loss:0.11926721781492233\n",
      "Epoch:1 \n",
      "Iteration:226 \n",
      "Loss:0.1341986507177353\n",
      "Epoch:1 \n",
      "Iteration:227 \n",
      "Loss:0.21553756296634674\n",
      "Epoch:1 \n",
      "Iteration:228 \n",
      "Loss:0.2085375338792801\n",
      "Epoch:1 \n",
      "Iteration:229 \n",
      "Loss:0.13790622353553772\n",
      "Epoch:1 \n",
      "Iteration:230 \n",
      "Loss:0.14417821168899536\n",
      "Epoch:1 \n",
      "Iteration:231 \n",
      "Loss:0.13521558046340942\n",
      "Epoch:1 \n",
      "Iteration:232 \n",
      "Loss:0.25427722930908203\n",
      "Epoch:1 \n",
      "Iteration:233 \n",
      "Loss:0.061873696744441986\n",
      "Epoch:1 \n",
      "Iteration:234 \n",
      "Loss:0.12129952758550644\n",
      "Epoch:1 \n",
      "Iteration:235 \n",
      "Loss:0.08585207164287567\n",
      "Epoch:1 \n",
      "Iteration:236 \n",
      "Loss:0.09578448534011841\n",
      "Epoch:1 \n",
      "Iteration:237 \n",
      "Loss:0.24739769101142883\n",
      "Epoch:1 \n",
      "Iteration:238 \n",
      "Loss:0.20082828402519226\n",
      "Epoch:1 \n",
      "Iteration:239 \n",
      "Loss:0.16660286486148834\n",
      "Epoch:1 \n",
      "Iteration:240 \n",
      "Loss:0.12448081374168396\n",
      "Epoch:1 \n",
      "Iteration:241 \n",
      "Loss:0.1126178577542305\n",
      "Epoch:1 \n",
      "Iteration:242 \n",
      "Loss:0.1494017094373703\n",
      "Epoch:1 \n",
      "Iteration:243 \n",
      "Loss:0.061474427580833435\n",
      "Epoch:1 \n",
      "Iteration:244 \n",
      "Loss:0.08451125025749207\n",
      "Epoch:1 \n",
      "Iteration:245 \n",
      "Loss:0.17013268172740936\n",
      "Epoch:1 \n",
      "Iteration:246 \n",
      "Loss:0.18701007962226868\n",
      "Epoch:1 \n",
      "Iteration:247 \n",
      "Loss:0.16942645609378815\n",
      "Epoch:1 \n",
      "Iteration:248 \n",
      "Loss:0.13958017528057098\n",
      "Epoch:1 \n",
      "Iteration:249 \n",
      "Loss:0.084477499127388\n",
      "Epoch:1 \n",
      "Iteration:250 \n",
      "Loss:0.1355639100074768\n",
      "Epoch:1 \n",
      "Iteration:251 \n",
      "Loss:0.22790712118148804\n",
      "Epoch:1 \n",
      "Iteration:252 \n",
      "Loss:0.16315774619579315\n",
      "Epoch:1 \n",
      "Iteration:253 \n",
      "Loss:0.16369369626045227\n",
      "Epoch:1 \n",
      "Iteration:254 \n",
      "Loss:0.1281852126121521\n",
      "Epoch:1 \n",
      "Iteration:255 \n",
      "Loss:0.34571897983551025\n",
      "Epoch:1 \n",
      "Iteration:256 \n",
      "Loss:0.21460860967636108\n",
      "Epoch:1 \n",
      "Iteration:257 \n",
      "Loss:0.3529081344604492\n",
      "Epoch:1 \n",
      "Iteration:258 \n",
      "Loss:0.14152777194976807\n",
      "Epoch:1 \n",
      "Iteration:259 \n",
      "Loss:0.20369009673595428\n",
      "Epoch:1 \n",
      "Iteration:260 \n",
      "Loss:0.11168790608644485\n",
      "Epoch:1 \n",
      "Iteration:261 \n",
      "Loss:0.20936529338359833\n",
      "Epoch:1 \n",
      "Iteration:262 \n",
      "Loss:0.1992996633052826\n",
      "Epoch:1 \n",
      "Iteration:263 \n",
      "Loss:0.2548293173313141\n",
      "Epoch:1 \n",
      "Iteration:264 \n",
      "Loss:0.22339364886283875\n",
      "Epoch:1 \n",
      "Iteration:265 \n",
      "Loss:0.2226775586605072\n",
      "Epoch:1 \n",
      "Iteration:266 \n",
      "Loss:0.15242652595043182\n",
      "Epoch:1 \n",
      "Iteration:267 \n",
      "Loss:0.11700218170881271\n",
      "Epoch:1 \n",
      "Iteration:268 \n",
      "Loss:0.07112956047058105\n",
      "Epoch:1 \n",
      "Iteration:269 \n",
      "Loss:0.16115371882915497\n",
      "Epoch:1 \n",
      "Iteration:270 \n",
      "Loss:0.09328000247478485\n",
      "Epoch:1 \n",
      "Iteration:271 \n",
      "Loss:0.11228461563587189\n",
      "Epoch:1 \n",
      "Iteration:272 \n",
      "Loss:0.09753935039043427\n",
      "Epoch:1 \n",
      "Iteration:273 \n",
      "Loss:0.08504918962717056\n",
      "Epoch:1 \n",
      "Iteration:274 \n",
      "Loss:0.18780617415905\n",
      "Epoch:1 \n",
      "Iteration:275 \n",
      "Loss:0.12763231992721558\n",
      "Epoch:1 \n",
      "Iteration:276 \n",
      "Loss:0.15267400443553925\n",
      "Epoch:1 \n",
      "Iteration:277 \n",
      "Loss:0.1715623140335083\n",
      "Epoch:1 \n",
      "Iteration:278 \n",
      "Loss:0.18402642011642456\n",
      "Epoch:1 \n",
      "Iteration:279 \n",
      "Loss:0.21424001455307007\n",
      "Epoch:1 \n",
      "Iteration:280 \n",
      "Loss:0.08497539162635803\n",
      "Epoch:1 \n",
      "Iteration:281 \n",
      "Loss:0.1401849389076233\n",
      "Epoch:1 \n",
      "Iteration:282 \n",
      "Loss:0.13807371258735657\n",
      "Epoch:1 \n",
      "Iteration:283 \n",
      "Loss:0.15613220632076263\n",
      "Epoch:1 \n",
      "Iteration:284 \n",
      "Loss:0.17785942554473877\n",
      "Epoch:1 \n",
      "Iteration:285 \n",
      "Loss:0.29615554213523865\n",
      "Epoch:1 \n",
      "Iteration:286 \n",
      "Loss:0.2421244978904724\n",
      "Epoch:1 \n",
      "Iteration:287 \n",
      "Loss:0.17601080238819122\n",
      "Epoch:1 \n",
      "Iteration:288 \n",
      "Loss:0.16622912883758545\n",
      "Epoch:1 \n",
      "Iteration:289 \n",
      "Loss:0.32603874802589417\n",
      "Epoch:1 \n",
      "Iteration:290 \n",
      "Loss:0.1167239248752594\n",
      "Epoch:1 \n",
      "Iteration:291 \n",
      "Loss:0.1310199797153473\n",
      "Epoch:1 \n",
      "Iteration:292 \n",
      "Loss:0.14816546440124512\n",
      "Epoch:1 \n",
      "Iteration:293 \n",
      "Loss:0.1895904541015625\n",
      "Epoch:1 \n",
      "Iteration:294 \n",
      "Loss:0.17430013418197632\n",
      "Epoch:1 \n",
      "Iteration:295 \n",
      "Loss:0.30275866389274597\n",
      "Epoch:1 \n",
      "Iteration:296 \n",
      "Loss:0.1549168825149536\n",
      "Epoch:1 \n",
      "Iteration:297 \n",
      "Loss:0.2380707859992981\n",
      "Epoch:1 \n",
      "Iteration:298 \n",
      "Loss:0.11344513297080994\n",
      "Epoch:1 \n",
      "Iteration:299 \n",
      "Loss:0.2677982449531555\n",
      "Epoch:1 \n",
      "Iteration:300 \n",
      "Loss:0.17318986356258392\n",
      "Epoch:1 \n",
      "Iteration:301 \n",
      "Loss:0.16743168234825134\n",
      "Epoch:1 \n",
      "Iteration:302 \n",
      "Loss:0.05891239270567894\n",
      "Epoch:1 \n",
      "Iteration:303 \n",
      "Loss:0.1507492959499359\n",
      "Epoch:1 \n",
      "Iteration:304 \n",
      "Loss:0.19562767446041107\n",
      "Epoch:1 \n",
      "Iteration:305 \n",
      "Loss:0.04008667171001434\n",
      "Epoch:1 \n",
      "Iteration:306 \n",
      "Loss:0.1803441196680069\n",
      "Epoch:1 \n",
      "Iteration:307 \n",
      "Loss:0.19751280546188354\n",
      "Epoch:1 \n",
      "Iteration:308 \n",
      "Loss:0.18159769475460052\n",
      "Epoch:1 \n",
      "Iteration:309 \n",
      "Loss:0.09003745764493942\n",
      "Epoch:1 \n",
      "Iteration:310 \n",
      "Loss:0.17185933887958527\n",
      "Epoch:1 \n",
      "Iteration:311 \n",
      "Loss:0.10974127799272537\n",
      "Epoch:1 \n",
      "Iteration:312 \n",
      "Loss:0.3070503771305084\n",
      "Epoch:1 \n",
      "Iteration:313 \n",
      "Loss:0.1234973594546318\n",
      "Epoch:1 \n",
      "Iteration:314 \n",
      "Loss:0.13988421857357025\n",
      "Epoch:1 \n",
      "Iteration:315 \n",
      "Loss:0.1775360107421875\n",
      "Epoch:1 \n",
      "Iteration:316 \n",
      "Loss:0.14908771216869354\n",
      "Epoch:1 \n",
      "Iteration:317 \n",
      "Loss:0.08358968794345856\n",
      "Epoch:1 \n",
      "Iteration:318 \n",
      "Loss:0.16719035804271698\n",
      "Epoch:1 \n",
      "Iteration:319 \n",
      "Loss:0.13801692426204681\n",
      "Epoch:1 \n",
      "Iteration:320 \n",
      "Loss:0.15283483266830444\n",
      "Epoch:1 \n",
      "Iteration:321 \n",
      "Loss:0.09023714065551758\n",
      "Epoch:1 \n",
      "Iteration:322 \n",
      "Loss:0.12085916846990585\n",
      "Epoch:1 \n",
      "Iteration:323 \n",
      "Loss:0.19208984076976776\n",
      "Epoch:1 \n",
      "Iteration:324 \n",
      "Loss:0.10534988343715668\n",
      "Epoch:1 \n",
      "Iteration:325 \n",
      "Loss:0.09955122321844101\n",
      "Epoch:1 \n",
      "Iteration:326 \n",
      "Loss:0.10314197838306427\n",
      "Epoch:1 \n",
      "Iteration:327 \n",
      "Loss:0.19099943339824677\n",
      "Epoch:1 \n",
      "Iteration:328 \n",
      "Loss:0.15097354352474213\n",
      "Epoch:1 \n",
      "Iteration:329 \n",
      "Loss:0.05294886603951454\n",
      "Epoch:1 \n",
      "Iteration:330 \n",
      "Loss:0.1789417862892151\n",
      "Epoch:1 \n",
      "Iteration:331 \n",
      "Loss:0.13349762558937073\n",
      "Epoch:1 \n",
      "Iteration:332 \n",
      "Loss:0.08608779311180115\n",
      "Epoch:1 \n",
      "Iteration:333 \n",
      "Loss:0.2064773440361023\n",
      "Epoch:1 \n",
      "Iteration:334 \n",
      "Loss:0.17000742256641388\n",
      "Epoch:1 \n",
      "Iteration:335 \n",
      "Loss:0.3031696379184723\n",
      "Epoch:1 \n",
      "Iteration:336 \n",
      "Loss:0.1303916722536087\n",
      "Epoch:1 \n",
      "Iteration:337 \n",
      "Loss:0.15825255215168\n",
      "Epoch:1 \n",
      "Iteration:338 \n",
      "Loss:0.24353379011154175\n",
      "Epoch:1 \n",
      "Iteration:339 \n",
      "Loss:0.23819094896316528\n",
      "Epoch:1 \n",
      "Iteration:340 \n",
      "Loss:0.23188480734825134\n",
      "Epoch:1 \n",
      "Iteration:341 \n",
      "Loss:0.23926052451133728\n",
      "Epoch:1 \n",
      "Iteration:342 \n",
      "Loss:0.1374325305223465\n",
      "Epoch:1 \n",
      "Iteration:343 \n",
      "Loss:0.12234268337488174\n",
      "Epoch:1 \n",
      "Iteration:344 \n",
      "Loss:0.08939297497272491\n",
      "Epoch:1 \n",
      "Iteration:345 \n",
      "Loss:0.1573348492383957\n",
      "Epoch:1 \n",
      "Iteration:346 \n",
      "Loss:0.04774055629968643\n",
      "Epoch:1 \n",
      "Iteration:347 \n",
      "Loss:0.31348198652267456\n",
      "Epoch:1 \n",
      "Iteration:348 \n",
      "Loss:0.12617792189121246\n",
      "Epoch:1 \n",
      "Iteration:349 \n",
      "Loss:0.12596957385540009\n",
      "Epoch:1 \n",
      "Iteration:350 \n",
      "Loss:0.23239940404891968\n",
      "Epoch:1 \n",
      "Iteration:351 \n",
      "Loss:0.22754289209842682\n",
      "Epoch:1 \n",
      "Iteration:352 \n",
      "Loss:0.10897237807512283\n",
      "Epoch:1 \n",
      "Iteration:353 \n",
      "Loss:0.12211742252111435\n",
      "Epoch:1 \n",
      "Iteration:354 \n",
      "Loss:0.17609861493110657\n",
      "Epoch:1 \n",
      "Iteration:355 \n",
      "Loss:0.16741515696048737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \n",
      "Iteration:356 \n",
      "Loss:0.1579798460006714\n",
      "Epoch:1 \n",
      "Iteration:357 \n",
      "Loss:0.1322966068983078\n",
      "Epoch:1 \n",
      "Iteration:358 \n",
      "Loss:0.17488032579421997\n",
      "Epoch:1 \n",
      "Iteration:359 \n",
      "Loss:0.16777494549751282\n",
      "Epoch:1 \n",
      "Iteration:360 \n",
      "Loss:0.14855535328388214\n",
      "Epoch:1 \n",
      "Iteration:361 \n",
      "Loss:0.22715777158737183\n",
      "Epoch:1 \n",
      "Iteration:362 \n",
      "Loss:0.15137185156345367\n",
      "Epoch:1 \n",
      "Iteration:363 \n",
      "Loss:0.3162267804145813\n",
      "Epoch:1 \n",
      "Iteration:364 \n",
      "Loss:0.1435621976852417\n",
      "Epoch:1 \n",
      "Iteration:365 \n",
      "Loss:0.11869576573371887\n",
      "Epoch:1 \n",
      "Iteration:366 \n",
      "Loss:0.06551741808652878\n",
      "Epoch:1 \n",
      "Iteration:367 \n",
      "Loss:0.12392859160900116\n",
      "Epoch:1 \n",
      "Iteration:368 \n",
      "Loss:0.1441977471113205\n",
      "Epoch:1 \n",
      "Iteration:369 \n",
      "Loss:0.17136967182159424\n",
      "Epoch:1 \n",
      "Iteration:370 \n",
      "Loss:0.3830975592136383\n",
      "Epoch:1 \n",
      "Iteration:371 \n",
      "Loss:0.12549568712711334\n",
      "Epoch:1 \n",
      "Iteration:372 \n",
      "Loss:0.09441132843494415\n",
      "Epoch:1 \n",
      "Iteration:373 \n",
      "Loss:0.11308066546916962\n",
      "Epoch:1 \n",
      "Iteration:374 \n",
      "Loss:0.2946067750453949\n",
      "Epoch:1 \n",
      "Iteration:375 \n",
      "Loss:0.30554139614105225\n",
      "Epoch:1 \n",
      "Iteration:376 \n",
      "Loss:0.29851454496383667\n",
      "Epoch:1 \n",
      "Iteration:377 \n",
      "Loss:0.18765129148960114\n",
      "Epoch:1 \n",
      "Iteration:378 \n",
      "Loss:0.14548975229263306\n",
      "Epoch:1 \n",
      "Iteration:379 \n",
      "Loss:0.23775292932987213\n",
      "Epoch:1 \n",
      "Iteration:380 \n",
      "Loss:0.16772422194480896\n",
      "Epoch:1 \n",
      "Iteration:381 \n",
      "Loss:0.16343961656093597\n",
      "Epoch:1 \n",
      "Iteration:382 \n",
      "Loss:0.0664227083325386\n",
      "Epoch:1 \n",
      "Iteration:383 \n",
      "Loss:0.251888632774353\n",
      "Epoch:1 \n",
      "Iteration:384 \n",
      "Loss:0.17794324457645416\n",
      "Epoch:1 \n",
      "Iteration:385 \n",
      "Loss:0.14521580934524536\n",
      "Epoch:1 \n",
      "Iteration:386 \n",
      "Loss:0.12497042864561081\n",
      "Epoch:1 \n",
      "Iteration:387 \n",
      "Loss:0.2176038920879364\n",
      "Epoch:1 \n",
      "Iteration:388 \n",
      "Loss:0.12465760856866837\n",
      "Epoch:1 \n",
      "Iteration:389 \n",
      "Loss:0.08923342823982239\n",
      "Epoch:1 \n",
      "Iteration:390 \n",
      "Loss:0.23708225786685944\n",
      "Epoch:1 \n",
      "Iteration:391 \n",
      "Loss:0.18922735750675201\n",
      "Epoch:1 \n",
      "Iteration:392 \n",
      "Loss:0.07804815471172333\n",
      "Epoch:1 \n",
      "Iteration:393 \n",
      "Loss:0.18437808752059937\n",
      "Epoch:1 \n",
      "Iteration:394 \n",
      "Loss:0.13805130124092102\n",
      "Epoch:1 \n",
      "Iteration:395 \n",
      "Loss:0.16958487033843994\n",
      "Epoch:1 \n",
      "Iteration:396 \n",
      "Loss:0.16531363129615784\n",
      "Epoch:1 \n",
      "Iteration:397 \n",
      "Loss:0.12704482674598694\n",
      "Epoch:1 \n",
      "Iteration:398 \n",
      "Loss:0.1614457368850708\n",
      "Epoch:1 \n",
      "Iteration:399 \n",
      "Loss:0.22057953476905823\n",
      "Epoch:1 \n",
      "Iteration:400 \n",
      "Loss:0.14540232717990875\n",
      "Epoch:1 \n",
      "Iteration:401 \n",
      "Loss:0.0864008367061615\n",
      "Epoch:1 \n",
      "Iteration:402 \n",
      "Loss:0.14857891201972961\n",
      "Epoch:1 \n",
      "Iteration:403 \n",
      "Loss:0.07551080733537674\n",
      "Epoch:1 \n",
      "Iteration:404 \n",
      "Loss:0.14636777341365814\n",
      "Epoch:1 \n",
      "Iteration:405 \n",
      "Loss:0.22663001716136932\n",
      "Epoch:1 \n",
      "Iteration:406 \n",
      "Loss:0.1538436859846115\n",
      "Epoch:1 \n",
      "Iteration:407 \n",
      "Loss:0.09959450364112854\n",
      "Epoch:1 \n",
      "Iteration:408 \n",
      "Loss:0.24725183844566345\n",
      "Epoch:1 \n",
      "Iteration:409 \n",
      "Loss:0.27957770228385925\n",
      "Epoch:1 \n",
      "Iteration:410 \n",
      "Loss:0.07834064960479736\n",
      "Epoch:1 \n",
      "Iteration:411 \n",
      "Loss:0.14370116591453552\n",
      "Epoch:1 \n",
      "Iteration:412 \n",
      "Loss:0.20632970333099365\n",
      "Epoch:1 \n",
      "Iteration:413 \n",
      "Loss:0.18289625644683838\n",
      "Epoch:1 \n",
      "Iteration:414 \n",
      "Loss:0.22489890456199646\n",
      "Epoch:1 \n",
      "Iteration:415 \n",
      "Loss:0.16003383696079254\n",
      "Epoch:1 \n",
      "Iteration:416 \n",
      "Loss:0.2523242235183716\n",
      "Epoch:1 \n",
      "Iteration:417 \n",
      "Loss:0.28450965881347656\n",
      "Epoch:1 \n",
      "Iteration:418 \n",
      "Loss:0.24086427688598633\n",
      "Epoch:1 \n",
      "Iteration:419 \n",
      "Loss:0.12649831175804138\n",
      "Epoch:1 \n",
      "Iteration:420 \n",
      "Loss:0.18169528245925903\n",
      "Epoch:1 \n",
      "Iteration:421 \n",
      "Loss:0.18053504824638367\n",
      "Epoch:1 \n",
      "Iteration:422 \n",
      "Loss:0.1623576581478119\n",
      "Epoch:1 \n",
      "Iteration:423 \n",
      "Loss:0.09776506572961807\n",
      "Epoch:1 \n",
      "Iteration:424 \n",
      "Loss:0.23213250935077667\n",
      "Epoch:1 \n",
      "Iteration:425 \n",
      "Loss:0.16054318845272064\n",
      "Epoch:1 \n",
      "Iteration:426 \n",
      "Loss:0.15931454300880432\n",
      "Epoch:1 \n",
      "Iteration:427 \n",
      "Loss:0.06718993932008743\n",
      "Epoch:1 \n",
      "Iteration:428 \n",
      "Loss:0.13157907128334045\n",
      "Epoch:1 \n",
      "Iteration:429 \n",
      "Loss:0.15056023001670837\n",
      "Epoch:1 \n",
      "Iteration:430 \n",
      "Loss:0.13571284711360931\n",
      "Epoch:1 \n",
      "Iteration:431 \n",
      "Loss:0.1203511655330658\n",
      "Epoch:1 \n",
      "Iteration:432 \n",
      "Loss:0.1570037603378296\n",
      "Epoch:1 \n",
      "Iteration:433 \n",
      "Loss:0.11855635046958923\n",
      "Epoch:1 \n",
      "Iteration:434 \n",
      "Loss:0.09084481000900269\n",
      "Epoch:1 \n",
      "Iteration:435 \n",
      "Loss:0.19788722693920135\n",
      "Epoch:1 \n",
      "Iteration:436 \n",
      "Loss:0.09556737542152405\n",
      "Epoch:1 \n",
      "Iteration:437 \n",
      "Loss:0.2238909751176834\n",
      "Epoch:1 \n",
      "Iteration:438 \n",
      "Loss:0.11157716065645218\n",
      "Epoch:1 \n",
      "Iteration:439 \n",
      "Loss:0.08179385960102081\n",
      "Epoch:1 \n",
      "Iteration:440 \n",
      "Loss:0.0864664614200592\n",
      "Epoch:1 \n",
      "Iteration:441 \n",
      "Loss:0.11941032111644745\n",
      "Epoch:1 \n",
      "Iteration:442 \n",
      "Loss:0.09747246652841568\n",
      "Epoch:1 \n",
      "Iteration:443 \n",
      "Loss:0.14056435227394104\n",
      "Epoch:1 \n",
      "Iteration:444 \n",
      "Loss:0.12694060802459717\n",
      "Epoch:1 \n",
      "Iteration:445 \n",
      "Loss:0.17020730674266815\n",
      "Epoch:1 \n",
      "Iteration:446 \n",
      "Loss:0.15442584455013275\n",
      "Epoch:1 \n",
      "Iteration:447 \n",
      "Loss:0.1218116283416748\n",
      "Epoch:1 \n",
      "Iteration:448 \n",
      "Loss:0.1488742083311081\n",
      "Epoch:1 \n",
      "Iteration:449 \n",
      "Loss:0.141063392162323\n",
      "Epoch:1 \n",
      "Iteration:450 \n",
      "Loss:0.05923749506473541\n",
      "Epoch:1 \n",
      "Iteration:451 \n",
      "Loss:0.1407221555709839\n",
      "Epoch:1 \n",
      "Iteration:452 \n",
      "Loss:0.08396798372268677\n",
      "Epoch:1 \n",
      "Iteration:453 \n",
      "Loss:0.12817895412445068\n",
      "Epoch:1 \n",
      "Iteration:454 \n",
      "Loss:0.14802919328212738\n",
      "Epoch:1 \n",
      "Iteration:455 \n",
      "Loss:0.06963115930557251\n",
      "Epoch:1 \n",
      "Iteration:456 \n",
      "Loss:0.07898536324501038\n",
      "Epoch:1 \n",
      "Iteration:457 \n",
      "Loss:0.018804341554641724\n",
      "Epoch:1 \n",
      "Iteration:458 \n",
      "Loss:0.18707171082496643\n",
      "Epoch:1 \n",
      "Iteration:459 \n",
      "Loss:0.20028729736804962\n",
      "Epoch:1 \n",
      "Iteration:460 \n",
      "Loss:0.26463189721107483\n",
      "Epoch:1 \n",
      "Iteration:461 \n",
      "Loss:0.06801345944404602\n",
      "Epoch:1 \n",
      "Iteration:462 \n",
      "Loss:0.23379573225975037\n",
      "Epoch:1 \n",
      "Iteration:463 \n",
      "Loss:0.09285418689250946\n",
      "Epoch:1 \n",
      "Iteration:464 \n",
      "Loss:0.1288536787033081\n",
      "Epoch:1 \n",
      "Iteration:465 \n",
      "Loss:0.19858354330062866\n",
      "Epoch:1 \n",
      "Iteration:466 \n",
      "Loss:0.10527375340461731\n",
      "Epoch:1 \n",
      "Iteration:467 \n",
      "Loss:0.21339210867881775\n",
      "Epoch:1 \n",
      "Iteration:468 \n",
      "Loss:0.10691437125205994\n",
      "Epoch:1 \n",
      "Iteration:469 \n",
      "Loss:0.17181193828582764\n",
      "Epoch:1 \n",
      "Iteration:470 \n",
      "Loss:0.12533560395240784\n",
      "Epoch:1 \n",
      "Iteration:471 \n",
      "Loss:0.05817952752113342\n",
      "Epoch:1 \n",
      "Iteration:472 \n",
      "Loss:0.16680331528186798\n",
      "Epoch:1 \n",
      "Iteration:473 \n",
      "Loss:0.09557946026325226\n",
      "Epoch:1 \n",
      "Iteration:474 \n",
      "Loss:0.15664370357990265\n",
      "Epoch:1 \n",
      "Iteration:475 \n",
      "Loss:0.12586389482021332\n",
      "Epoch:1 \n",
      "Iteration:476 \n",
      "Loss:0.1069115623831749\n",
      "Epoch:1 \n",
      "Iteration:477 \n",
      "Loss:0.04154718294739723\n",
      "Epoch:1 \n",
      "Iteration:478 \n",
      "Loss:0.18392904102802277\n",
      "Epoch:1 \n",
      "Iteration:479 \n",
      "Loss:0.13265304267406464\n",
      "Epoch:1 \n",
      "Iteration:480 \n",
      "Loss:0.10919620841741562\n",
      "Epoch:1 \n",
      "Iteration:481 \n",
      "Loss:0.11138859391212463\n",
      "Epoch:1 \n",
      "Iteration:482 \n",
      "Loss:0.07535132765769958\n",
      "Epoch:1 \n",
      "Iteration:483 \n",
      "Loss:0.0642189085483551\n",
      "Epoch:1 \n",
      "Iteration:484 \n",
      "Loss:0.16047172248363495\n",
      "Epoch:1 \n",
      "Iteration:485 \n",
      "Loss:0.1741226613521576\n",
      "Epoch:1 \n",
      "Iteration:486 \n",
      "Loss:0.06477146595716476\n",
      "Epoch:1 \n",
      "Iteration:487 \n",
      "Loss:0.17340566217899323\n",
      "Epoch:1 \n",
      "Iteration:488 \n",
      "Loss:0.10121779143810272\n",
      "Epoch:1 \n",
      "Iteration:489 \n",
      "Loss:0.17965959012508392\n",
      "Epoch:1 \n",
      "Iteration:490 \n",
      "Loss:0.058013156056404114\n",
      "Epoch:1 \n",
      "Iteration:491 \n",
      "Loss:0.12729431688785553\n",
      "Epoch:1 \n",
      "Iteration:492 \n",
      "Loss:0.15683141350746155\n",
      "Epoch:1 \n",
      "Iteration:493 \n",
      "Loss:0.2584000825881958\n",
      "Epoch:1 \n",
      "Iteration:494 \n",
      "Loss:0.053398095071315765\n",
      "Epoch:1 \n",
      "Iteration:495 \n",
      "Loss:0.06947608292102814\n",
      "Epoch:1 \n",
      "Iteration:496 \n",
      "Loss:0.08760039508342743\n",
      "Epoch:1 \n",
      "Iteration:497 \n",
      "Loss:0.16803324222564697\n",
      "Epoch:1 \n",
      "Iteration:498 \n",
      "Loss:0.14669391512870789\n",
      "Epoch:1 \n",
      "Iteration:499 \n",
      "Loss:0.1604669690132141\n",
      "Epoch:1 \n",
      "Iteration:500 \n",
      "Loss:0.03770060837268829\n",
      "Epoch:1 \n",
      "Iteration:501 \n",
      "Loss:0.18763208389282227\n",
      "Epoch:1 \n",
      "Iteration:502 \n",
      "Loss:0.057373832911252975\n",
      "Epoch:1 \n",
      "Iteration:503 \n",
      "Loss:0.11099676787853241\n",
      "Epoch:1 \n",
      "Iteration:504 \n",
      "Loss:0.09875714033842087\n",
      "Epoch:1 \n",
      "Iteration:505 \n",
      "Loss:0.052983954548835754\n",
      "Epoch:1 \n",
      "Iteration:506 \n",
      "Loss:0.05324944481253624\n",
      "Epoch:1 \n",
      "Iteration:507 \n",
      "Loss:0.09516720473766327\n",
      "Epoch:1 \n",
      "Iteration:508 \n",
      "Loss:0.2105506956577301\n",
      "Epoch:1 \n",
      "Iteration:509 \n",
      "Loss:0.16112357378005981\n",
      "Epoch:1 \n",
      "Iteration:510 \n",
      "Loss:0.1232694536447525\n",
      "Epoch:1 \n",
      "Iteration:511 \n",
      "Loss:0.05190546065568924\n",
      "Epoch:1 \n",
      "Iteration:512 \n",
      "Loss:0.12582232058048248\n",
      "Epoch:1 \n",
      "Iteration:513 \n",
      "Loss:0.2602381706237793\n",
      "Epoch:1 \n",
      "Iteration:514 \n",
      "Loss:0.20105312764644623\n",
      "Epoch:1 \n",
      "Iteration:515 \n",
      "Loss:0.11387726664543152\n",
      "Epoch:1 \n",
      "Iteration:516 \n",
      "Loss:0.1497097909450531\n",
      "Epoch:1 \n",
      "Iteration:517 \n",
      "Loss:0.2781929075717926\n",
      "Epoch:1 \n",
      "Iteration:518 \n",
      "Loss:0.14638608694076538\n",
      "Epoch:1 \n",
      "Iteration:519 \n",
      "Loss:0.24004524946212769\n",
      "Epoch:1 \n",
      "Iteration:520 \n",
      "Loss:0.19831907749176025\n",
      "Epoch:1 \n",
      "Iteration:521 \n",
      "Loss:0.05353476479649544\n",
      "Epoch:1 \n",
      "Iteration:522 \n",
      "Loss:0.23325391113758087\n",
      "Epoch:1 \n",
      "Iteration:523 \n",
      "Loss:0.18582992255687714\n",
      "Epoch:1 \n",
      "Iteration:524 \n",
      "Loss:0.09449417889118195\n",
      "Epoch:1 \n",
      "Iteration:525 \n",
      "Loss:0.09688922017812729\n",
      "Epoch:1 \n",
      "Iteration:526 \n",
      "Loss:0.027983758598566055\n",
      "Epoch:1 \n",
      "Iteration:527 \n",
      "Loss:0.11870747804641724\n",
      "Epoch:1 \n",
      "Iteration:528 \n",
      "Loss:0.1011987179517746\n",
      "Epoch:1 \n",
      "Iteration:529 \n",
      "Loss:0.1776442676782608\n",
      "Epoch:1 \n",
      "Iteration:530 \n",
      "Loss:0.14917439222335815\n",
      "Epoch:1 \n",
      "Iteration:531 \n",
      "Loss:0.14289936423301697\n",
      "Epoch:1 \n",
      "Iteration:532 \n",
      "Loss:0.05317782983183861\n",
      "Epoch:1 \n",
      "Iteration:533 \n",
      "Loss:0.14169421792030334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \n",
      "Iteration:534 \n",
      "Loss:0.19296100735664368\n",
      "Epoch:1 \n",
      "Iteration:535 \n",
      "Loss:0.0548328198492527\n",
      "Epoch:1 \n",
      "Iteration:536 \n",
      "Loss:0.11793999373912811\n",
      "Epoch:1 \n",
      "Iteration:537 \n",
      "Loss:0.1974993497133255\n",
      "Epoch:1 \n",
      "Iteration:538 \n",
      "Loss:0.14726309478282928\n",
      "Epoch:1 \n",
      "Iteration:539 \n",
      "Loss:0.05452985689043999\n",
      "Epoch:1 \n",
      "Iteration:540 \n",
      "Loss:0.08887743204832077\n",
      "Epoch:1 \n",
      "Iteration:541 \n",
      "Loss:0.05349624156951904\n",
      "Epoch:1 \n",
      "Iteration:542 \n",
      "Loss:0.09971152245998383\n",
      "Epoch:1 \n",
      "Iteration:543 \n",
      "Loss:0.12198226898908615\n",
      "Epoch:1 \n",
      "Iteration:544 \n",
      "Loss:0.19008606672286987\n",
      "Epoch:1 \n",
      "Iteration:545 \n",
      "Loss:0.1730809360742569\n",
      "Epoch:1 \n",
      "Iteration:546 \n",
      "Loss:0.12990449368953705\n",
      "Epoch:1 \n",
      "Iteration:547 \n",
      "Loss:0.3251248896121979\n",
      "Epoch:1 \n",
      "Iteration:548 \n",
      "Loss:0.04347158968448639\n",
      "Epoch:1 \n",
      "Iteration:549 \n",
      "Loss:0.05970887839794159\n",
      "Epoch:1 \n",
      "Iteration:550 \n",
      "Loss:0.10945221036672592\n",
      "Epoch:1 \n",
      "Iteration:551 \n",
      "Loss:0.15638773143291473\n",
      "Epoch:1 \n",
      "Iteration:552 \n",
      "Loss:0.12025328725576401\n",
      "Epoch:1 \n",
      "Iteration:553 \n",
      "Loss:0.18447792530059814\n",
      "Epoch:1 \n",
      "Iteration:554 \n",
      "Loss:0.09545507282018661\n",
      "Epoch:1 \n",
      "Iteration:555 \n",
      "Loss:0.18379849195480347\n",
      "Epoch:1 \n",
      "Iteration:556 \n",
      "Loss:0.05731698498129845\n",
      "Epoch:1 \n",
      "Iteration:557 \n",
      "Loss:0.17120161652565002\n",
      "Epoch:1 \n",
      "Iteration:558 \n",
      "Loss:0.21065430343151093\n",
      "Epoch:1 \n",
      "Iteration:559 \n",
      "Loss:0.1444886028766632\n",
      "Epoch:1 \n",
      "Iteration:560 \n",
      "Loss:0.08166111260652542\n",
      "Epoch:1 \n",
      "Iteration:561 \n",
      "Loss:0.1046004667878151\n",
      "Epoch:1 \n",
      "Iteration:562 \n",
      "Loss:0.09464655071496964\n",
      "Epoch:1 \n",
      "Iteration:563 \n",
      "Loss:0.12497057020664215\n",
      "Epoch:1 \n",
      "Iteration:564 \n",
      "Loss:0.04581575095653534\n",
      "Epoch:1 \n",
      "Iteration:565 \n",
      "Loss:0.22922943532466888\n",
      "Epoch:1 \n",
      "Iteration:566 \n",
      "Loss:0.28213006258010864\n",
      "Epoch:1 \n",
      "Iteration:567 \n",
      "Loss:0.1906711757183075\n",
      "Epoch:1 \n",
      "Iteration:568 \n",
      "Loss:0.05716053023934364\n",
      "Epoch:1 \n",
      "Iteration:569 \n",
      "Loss:0.08213618397712708\n",
      "Epoch:1 \n",
      "Iteration:570 \n",
      "Loss:0.14924253523349762\n",
      "Epoch:1 \n",
      "Iteration:571 \n",
      "Loss:0.06512930244207382\n",
      "Epoch:1 \n",
      "Iteration:572 \n",
      "Loss:0.05922549217939377\n",
      "Epoch:1 \n",
      "Iteration:573 \n",
      "Loss:0.11661934107542038\n",
      "Epoch:1 \n",
      "Iteration:574 \n",
      "Loss:0.20099715888500214\n",
      "Epoch:1 \n",
      "Iteration:575 \n",
      "Loss:0.1725538671016693\n",
      "Epoch:1 \n",
      "Iteration:576 \n",
      "Loss:0.2321486622095108\n",
      "Epoch:1 \n",
      "Iteration:577 \n",
      "Loss:0.14236022531986237\n",
      "Epoch:1 \n",
      "Iteration:578 \n",
      "Loss:0.10517439246177673\n",
      "Epoch:1 \n",
      "Iteration:579 \n",
      "Loss:0.07205498218536377\n",
      "Epoch:1 \n",
      "Iteration:580 \n",
      "Loss:0.10681230574846268\n",
      "Epoch:1 \n",
      "Iteration:581 \n",
      "Loss:0.1372218132019043\n",
      "Epoch:1 \n",
      "Iteration:582 \n",
      "Loss:0.10903928428888321\n",
      "Epoch:1 \n",
      "Iteration:583 \n",
      "Loss:0.11332202702760696\n",
      "Epoch:1 \n",
      "Iteration:584 \n",
      "Loss:0.08661611378192902\n",
      "Epoch:1 \n",
      "Iteration:585 \n",
      "Loss:0.04033790901303291\n",
      "Epoch:1 \n",
      "Iteration:586 \n",
      "Loss:0.05375756695866585\n",
      "Epoch:1 \n",
      "Iteration:587 \n",
      "Loss:0.07908552139997482\n",
      "Epoch:1 \n",
      "Iteration:588 \n",
      "Loss:0.2648504674434662\n",
      "Epoch:1 \n",
      "Iteration:589 \n",
      "Loss:0.16532693803310394\n",
      "Epoch:1 \n",
      "Iteration:590 \n",
      "Loss:0.14812788367271423\n",
      "Epoch:1 \n",
      "Iteration:591 \n",
      "Loss:0.054632458835840225\n",
      "Epoch:1 \n",
      "Iteration:592 \n",
      "Loss:0.0649971216917038\n",
      "Epoch:1 \n",
      "Iteration:593 \n",
      "Loss:0.09802078455686569\n",
      "Epoch:1 \n",
      "Iteration:594 \n",
      "Loss:0.06874732673168182\n",
      "Epoch:1 \n",
      "Iteration:595 \n",
      "Loss:0.10750783234834671\n",
      "Epoch:1 \n",
      "Iteration:596 \n",
      "Loss:0.065363809466362\n",
      "Epoch:1 \n",
      "Iteration:597 \n",
      "Loss:0.06026590242981911\n",
      "Epoch:1 \n",
      "Iteration:598 \n",
      "Loss:0.07939674705266953\n",
      "Epoch:1 \n",
      "Iteration:599 \n",
      "Loss:0.14536221325397491\n",
      "Epoch:1 \n",
      "Iteration:600 \n",
      "Loss:0.18349143862724304\n",
      "\n",
      "Accuracy of network in epoch 1: 93.03166666666667\n",
      "Epoch:2 \n",
      "Iteration:1 \n",
      "Loss:0.03768860921263695\n",
      "Epoch:2 \n",
      "Iteration:2 \n",
      "Loss:0.1256306916475296\n",
      "Epoch:2 \n",
      "Iteration:3 \n",
      "Loss:0.13695181906223297\n",
      "Epoch:2 \n",
      "Iteration:4 \n",
      "Loss:0.10828784853219986\n",
      "Epoch:2 \n",
      "Iteration:5 \n",
      "Loss:0.09610897302627563\n",
      "Epoch:2 \n",
      "Iteration:6 \n",
      "Loss:0.10789445787668228\n",
      "Epoch:2 \n",
      "Iteration:7 \n",
      "Loss:0.17946983873844147\n",
      "Epoch:2 \n",
      "Iteration:8 \n",
      "Loss:0.11347386986017227\n",
      "Epoch:2 \n",
      "Iteration:9 \n",
      "Loss:0.05046037212014198\n",
      "Epoch:2 \n",
      "Iteration:10 \n",
      "Loss:0.05603288114070892\n",
      "Epoch:2 \n",
      "Iteration:11 \n",
      "Loss:0.049103789031505585\n",
      "Epoch:2 \n",
      "Iteration:12 \n",
      "Loss:0.14062586426734924\n",
      "Epoch:2 \n",
      "Iteration:13 \n",
      "Loss:0.016742099076509476\n",
      "Epoch:2 \n",
      "Iteration:14 \n",
      "Loss:0.14218811690807343\n",
      "Epoch:2 \n",
      "Iteration:15 \n",
      "Loss:0.0857653021812439\n",
      "Epoch:2 \n",
      "Iteration:16 \n",
      "Loss:0.10725483298301697\n",
      "Epoch:2 \n",
      "Iteration:17 \n",
      "Loss:0.06952115893363953\n",
      "Epoch:2 \n",
      "Iteration:18 \n",
      "Loss:0.08461152017116547\n",
      "Epoch:2 \n",
      "Iteration:19 \n",
      "Loss:0.0807526558637619\n",
      "Epoch:2 \n",
      "Iteration:20 \n",
      "Loss:0.04271145164966583\n",
      "Epoch:2 \n",
      "Iteration:21 \n",
      "Loss:0.10021254420280457\n",
      "Epoch:2 \n",
      "Iteration:22 \n",
      "Loss:0.2145097851753235\n",
      "Epoch:2 \n",
      "Iteration:23 \n",
      "Loss:0.1163840964436531\n",
      "Epoch:2 \n",
      "Iteration:24 \n",
      "Loss:0.15726019442081451\n",
      "Epoch:2 \n",
      "Iteration:25 \n",
      "Loss:0.02367512881755829\n",
      "Epoch:2 \n",
      "Iteration:26 \n",
      "Loss:0.1830364167690277\n",
      "Epoch:2 \n",
      "Iteration:27 \n",
      "Loss:0.1608065664768219\n",
      "Epoch:2 \n",
      "Iteration:28 \n",
      "Loss:0.13235792517662048\n",
      "Epoch:2 \n",
      "Iteration:29 \n",
      "Loss:0.08429094403982162\n",
      "Epoch:2 \n",
      "Iteration:30 \n",
      "Loss:0.082579106092453\n",
      "Epoch:2 \n",
      "Iteration:31 \n",
      "Loss:0.08736960589885712\n",
      "Epoch:2 \n",
      "Iteration:32 \n",
      "Loss:0.09068331867456436\n",
      "Epoch:2 \n",
      "Iteration:33 \n",
      "Loss:0.055903684347867966\n",
      "Epoch:2 \n",
      "Iteration:34 \n",
      "Loss:0.19918718934059143\n",
      "Epoch:2 \n",
      "Iteration:35 \n",
      "Loss:0.13910189270973206\n",
      "Epoch:2 \n",
      "Iteration:36 \n",
      "Loss:0.17552827298641205\n",
      "Epoch:2 \n",
      "Iteration:37 \n",
      "Loss:0.1186172366142273\n",
      "Epoch:2 \n",
      "Iteration:38 \n",
      "Loss:0.08279170840978622\n",
      "Epoch:2 \n",
      "Iteration:39 \n",
      "Loss:0.10063126683235168\n",
      "Epoch:2 \n",
      "Iteration:40 \n",
      "Loss:0.03503945469856262\n",
      "Epoch:2 \n",
      "Iteration:41 \n",
      "Loss:0.07224889099597931\n",
      "Epoch:2 \n",
      "Iteration:42 \n",
      "Loss:0.05653027445077896\n",
      "Epoch:2 \n",
      "Iteration:43 \n",
      "Loss:0.16159400343894958\n",
      "Epoch:2 \n",
      "Iteration:44 \n",
      "Loss:0.1234220489859581\n",
      "Epoch:2 \n",
      "Iteration:45 \n",
      "Loss:0.0317281112074852\n",
      "Epoch:2 \n",
      "Iteration:46 \n",
      "Loss:0.06791935116052628\n",
      "Epoch:2 \n",
      "Iteration:47 \n",
      "Loss:0.047314103692770004\n",
      "Epoch:2 \n",
      "Iteration:48 \n",
      "Loss:0.1521657258272171\n",
      "Epoch:2 \n",
      "Iteration:49 \n",
      "Loss:0.08422497659921646\n",
      "Epoch:2 \n",
      "Iteration:50 \n",
      "Loss:0.05944348871707916\n",
      "Epoch:2 \n",
      "Iteration:51 \n",
      "Loss:0.05755031108856201\n",
      "Epoch:2 \n",
      "Iteration:52 \n",
      "Loss:0.09010080248117447\n",
      "Epoch:2 \n",
      "Iteration:53 \n",
      "Loss:0.10535210371017456\n",
      "Epoch:2 \n",
      "Iteration:54 \n",
      "Loss:0.13041329383850098\n",
      "Epoch:2 \n",
      "Iteration:55 \n",
      "Loss:0.09273028373718262\n",
      "Epoch:2 \n",
      "Iteration:56 \n",
      "Loss:0.15835565328598022\n",
      "Epoch:2 \n",
      "Iteration:57 \n",
      "Loss:0.15424878895282745\n",
      "Epoch:2 \n",
      "Iteration:58 \n",
      "Loss:0.19810603559017181\n",
      "Epoch:2 \n",
      "Iteration:59 \n",
      "Loss:0.19962118566036224\n",
      "Epoch:2 \n",
      "Iteration:60 \n",
      "Loss:0.03862475976347923\n",
      "Epoch:2 \n",
      "Iteration:61 \n",
      "Loss:0.0819513201713562\n",
      "Epoch:2 \n",
      "Iteration:62 \n",
      "Loss:0.11862997710704803\n",
      "Epoch:2 \n",
      "Iteration:63 \n",
      "Loss:0.17596612870693207\n",
      "Epoch:2 \n",
      "Iteration:64 \n",
      "Loss:0.16683471202850342\n",
      "Epoch:2 \n",
      "Iteration:65 \n",
      "Loss:0.03789473697543144\n",
      "Epoch:2 \n",
      "Iteration:66 \n",
      "Loss:0.028709804639220238\n",
      "Epoch:2 \n",
      "Iteration:67 \n",
      "Loss:0.15830965340137482\n",
      "Epoch:2 \n",
      "Iteration:68 \n",
      "Loss:0.04561738669872284\n",
      "Epoch:2 \n",
      "Iteration:69 \n",
      "Loss:0.08934885263442993\n",
      "Epoch:2 \n",
      "Iteration:70 \n",
      "Loss:0.054945334792137146\n",
      "Epoch:2 \n",
      "Iteration:71 \n",
      "Loss:0.10217737406492233\n",
      "Epoch:2 \n",
      "Iteration:72 \n",
      "Loss:0.024448256939649582\n",
      "Epoch:2 \n",
      "Iteration:73 \n",
      "Loss:0.10416826605796814\n",
      "Epoch:2 \n",
      "Iteration:74 \n",
      "Loss:0.07856085151433945\n",
      "Epoch:2 \n",
      "Iteration:75 \n",
      "Loss:0.08827532082796097\n",
      "Epoch:2 \n",
      "Iteration:76 \n",
      "Loss:0.0962040051817894\n",
      "Epoch:2 \n",
      "Iteration:77 \n",
      "Loss:0.08688598871231079\n",
      "Epoch:2 \n",
      "Iteration:78 \n",
      "Loss:0.010532799176871777\n",
      "Epoch:2 \n",
      "Iteration:79 \n",
      "Loss:0.06756215542554855\n",
      "Epoch:2 \n",
      "Iteration:80 \n",
      "Loss:0.0935513898730278\n",
      "Epoch:2 \n",
      "Iteration:81 \n",
      "Loss:0.025664575397968292\n",
      "Epoch:2 \n",
      "Iteration:82 \n",
      "Loss:0.22062167525291443\n",
      "Epoch:2 \n",
      "Iteration:83 \n",
      "Loss:0.06896625459194183\n",
      "Epoch:2 \n",
      "Iteration:84 \n",
      "Loss:0.12421489506959915\n",
      "Epoch:2 \n",
      "Iteration:85 \n",
      "Loss:0.08726920187473297\n",
      "Epoch:2 \n",
      "Iteration:86 \n",
      "Loss:0.06861772388219833\n",
      "Epoch:2 \n",
      "Iteration:87 \n",
      "Loss:0.04847275838255882\n",
      "Epoch:2 \n",
      "Iteration:88 \n",
      "Loss:0.13171255588531494\n",
      "Epoch:2 \n",
      "Iteration:89 \n",
      "Loss:0.020263519138097763\n",
      "Epoch:2 \n",
      "Iteration:90 \n",
      "Loss:0.08402882516384125\n",
      "Epoch:2 \n",
      "Iteration:91 \n",
      "Loss:0.051180947571992874\n",
      "Epoch:2 \n",
      "Iteration:92 \n",
      "Loss:0.049127716571092606\n",
      "Epoch:2 \n",
      "Iteration:93 \n",
      "Loss:0.08888084441423416\n",
      "Epoch:2 \n",
      "Iteration:94 \n",
      "Loss:0.17704403400421143\n",
      "Epoch:2 \n",
      "Iteration:95 \n",
      "Loss:0.023156234994530678\n",
      "Epoch:2 \n",
      "Iteration:96 \n",
      "Loss:0.050671275705099106\n",
      "Epoch:2 \n",
      "Iteration:97 \n",
      "Loss:0.11057719588279724\n",
      "Epoch:2 \n",
      "Iteration:98 \n",
      "Loss:0.06792283803224564\n",
      "Epoch:2 \n",
      "Iteration:99 \n",
      "Loss:0.1934918463230133\n",
      "Epoch:2 \n",
      "Iteration:100 \n",
      "Loss:0.09845835715532303\n",
      "Epoch:2 \n",
      "Iteration:101 \n",
      "Loss:0.1792984902858734\n",
      "Epoch:2 \n",
      "Iteration:102 \n",
      "Loss:0.13294540345668793\n",
      "Epoch:2 \n",
      "Iteration:103 \n",
      "Loss:0.10784327238798141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2 \n",
      "Iteration:104 \n",
      "Loss:0.07323049753904343\n",
      "Epoch:2 \n",
      "Iteration:105 \n",
      "Loss:0.04443055018782616\n",
      "Epoch:2 \n",
      "Iteration:106 \n",
      "Loss:0.07564422488212585\n",
      "Epoch:2 \n",
      "Iteration:107 \n",
      "Loss:0.16559630632400513\n",
      "Epoch:2 \n",
      "Iteration:108 \n",
      "Loss:0.11632976680994034\n",
      "Epoch:2 \n",
      "Iteration:109 \n",
      "Loss:0.0820557102560997\n",
      "Epoch:2 \n",
      "Iteration:110 \n",
      "Loss:0.058863043785095215\n",
      "Epoch:2 \n",
      "Iteration:111 \n",
      "Loss:0.0629238709807396\n",
      "Epoch:2 \n",
      "Iteration:112 \n",
      "Loss:0.036725856363773346\n",
      "Epoch:2 \n",
      "Iteration:113 \n",
      "Loss:0.08275307714939117\n",
      "Epoch:2 \n",
      "Iteration:114 \n",
      "Loss:0.12446576356887817\n",
      "Epoch:2 \n",
      "Iteration:115 \n",
      "Loss:0.0786079689860344\n",
      "Epoch:2 \n",
      "Iteration:116 \n",
      "Loss:0.09837708622217178\n",
      "Epoch:2 \n",
      "Iteration:117 \n",
      "Loss:0.11518042534589767\n",
      "Epoch:2 \n",
      "Iteration:118 \n",
      "Loss:0.03974189609289169\n",
      "Epoch:2 \n",
      "Iteration:119 \n",
      "Loss:0.07056482881307602\n",
      "Epoch:2 \n",
      "Iteration:120 \n",
      "Loss:0.10706470161676407\n",
      "Epoch:2 \n",
      "Iteration:121 \n",
      "Loss:0.10666216909885406\n",
      "Epoch:2 \n",
      "Iteration:122 \n",
      "Loss:0.049494221806526184\n",
      "Epoch:2 \n",
      "Iteration:123 \n",
      "Loss:0.11394444108009338\n",
      "Epoch:2 \n",
      "Iteration:124 \n",
      "Loss:0.07226788997650146\n",
      "Epoch:2 \n",
      "Iteration:125 \n",
      "Loss:0.09296377003192902\n",
      "Epoch:2 \n",
      "Iteration:126 \n",
      "Loss:0.10244791209697723\n",
      "Epoch:2 \n",
      "Iteration:127 \n",
      "Loss:0.06408727914094925\n",
      "Epoch:2 \n",
      "Iteration:128 \n",
      "Loss:0.07959353178739548\n",
      "Epoch:2 \n",
      "Iteration:129 \n",
      "Loss:0.11107835918664932\n",
      "Epoch:2 \n",
      "Iteration:130 \n",
      "Loss:0.023910660296678543\n",
      "Epoch:2 \n",
      "Iteration:131 \n",
      "Loss:0.1283545196056366\n",
      "Epoch:2 \n",
      "Iteration:132 \n",
      "Loss:0.10096649825572968\n",
      "Epoch:2 \n",
      "Iteration:133 \n",
      "Loss:0.0758940801024437\n",
      "Epoch:2 \n",
      "Iteration:134 \n",
      "Loss:0.07442004233598709\n",
      "Epoch:2 \n",
      "Iteration:135 \n",
      "Loss:0.10350458323955536\n",
      "Epoch:2 \n",
      "Iteration:136 \n",
      "Loss:0.08413533121347427\n",
      "Epoch:2 \n",
      "Iteration:137 \n",
      "Loss:0.09812232851982117\n",
      "Epoch:2 \n",
      "Iteration:138 \n",
      "Loss:0.056328702718019485\n",
      "Epoch:2 \n",
      "Iteration:139 \n",
      "Loss:0.20106303691864014\n",
      "Epoch:2 \n",
      "Iteration:140 \n",
      "Loss:0.0989891067147255\n",
      "Epoch:2 \n",
      "Iteration:141 \n",
      "Loss:0.1570352017879486\n",
      "Epoch:2 \n",
      "Iteration:142 \n",
      "Loss:0.040356818586587906\n",
      "Epoch:2 \n",
      "Iteration:143 \n",
      "Loss:0.03203118219971657\n",
      "Epoch:2 \n",
      "Iteration:144 \n",
      "Loss:0.15831315517425537\n",
      "Epoch:2 \n",
      "Iteration:145 \n",
      "Loss:0.027550697326660156\n",
      "Epoch:2 \n",
      "Iteration:146 \n",
      "Loss:0.08113782107830048\n",
      "Epoch:2 \n",
      "Iteration:147 \n",
      "Loss:0.13548453152179718\n",
      "Epoch:2 \n",
      "Iteration:148 \n",
      "Loss:0.051072634756565094\n",
      "Epoch:2 \n",
      "Iteration:149 \n",
      "Loss:0.07170893996953964\n",
      "Epoch:2 \n",
      "Iteration:150 \n",
      "Loss:0.08512383699417114\n",
      "Epoch:2 \n",
      "Iteration:151 \n",
      "Loss:0.08447585254907608\n",
      "Epoch:2 \n",
      "Iteration:152 \n",
      "Loss:0.10489287972450256\n",
      "Epoch:2 \n",
      "Iteration:153 \n",
      "Loss:0.12810854613780975\n",
      "Epoch:2 \n",
      "Iteration:154 \n",
      "Loss:0.06750764697790146\n",
      "Epoch:2 \n",
      "Iteration:155 \n",
      "Loss:0.1143108382821083\n",
      "Epoch:2 \n",
      "Iteration:156 \n",
      "Loss:0.07451017200946808\n",
      "Epoch:2 \n",
      "Iteration:157 \n",
      "Loss:0.04295981302857399\n",
      "Epoch:2 \n",
      "Iteration:158 \n",
      "Loss:0.04076758399605751\n",
      "Epoch:2 \n",
      "Iteration:159 \n",
      "Loss:0.05125143378973007\n",
      "Epoch:2 \n",
      "Iteration:160 \n",
      "Loss:0.08065800368785858\n",
      "Epoch:2 \n",
      "Iteration:161 \n",
      "Loss:0.032230257987976074\n",
      "Epoch:2 \n",
      "Iteration:162 \n",
      "Loss:0.04377559572458267\n",
      "Epoch:2 \n",
      "Iteration:163 \n",
      "Loss:0.020634200423955917\n",
      "Epoch:2 \n",
      "Iteration:164 \n",
      "Loss:0.1335270255804062\n",
      "Epoch:2 \n",
      "Iteration:165 \n",
      "Loss:0.034214556217193604\n",
      "Epoch:2 \n",
      "Iteration:166 \n",
      "Loss:0.061434391885995865\n",
      "Epoch:2 \n",
      "Iteration:167 \n",
      "Loss:0.03717609867453575\n",
      "Epoch:2 \n",
      "Iteration:168 \n",
      "Loss:0.23791009187698364\n",
      "Epoch:2 \n",
      "Iteration:169 \n",
      "Loss:0.054906487464904785\n",
      "Epoch:2 \n",
      "Iteration:170 \n",
      "Loss:0.08690745383501053\n",
      "Epoch:2 \n",
      "Iteration:171 \n",
      "Loss:0.20042583346366882\n",
      "Epoch:2 \n",
      "Iteration:172 \n",
      "Loss:0.05465885251760483\n",
      "Epoch:2 \n",
      "Iteration:173 \n",
      "Loss:0.017153052613139153\n",
      "Epoch:2 \n",
      "Iteration:174 \n",
      "Loss:0.12338985502719879\n",
      "Epoch:2 \n",
      "Iteration:175 \n",
      "Loss:0.10872585326433182\n",
      "Epoch:2 \n",
      "Iteration:176 \n",
      "Loss:0.12953558564186096\n",
      "Epoch:2 \n",
      "Iteration:177 \n",
      "Loss:0.1391502320766449\n",
      "Epoch:2 \n",
      "Iteration:178 \n",
      "Loss:0.09906962513923645\n",
      "Epoch:2 \n",
      "Iteration:179 \n",
      "Loss:0.0098823681473732\n",
      "Epoch:2 \n",
      "Iteration:180 \n",
      "Loss:0.07000062614679337\n",
      "Epoch:2 \n",
      "Iteration:181 \n",
      "Loss:0.08142376691102982\n",
      "Epoch:2 \n",
      "Iteration:182 \n",
      "Loss:0.1667347401380539\n",
      "Epoch:2 \n",
      "Iteration:183 \n",
      "Loss:0.030405569821596146\n",
      "Epoch:2 \n",
      "Iteration:184 \n",
      "Loss:0.0505489744246006\n",
      "Epoch:2 \n",
      "Iteration:185 \n",
      "Loss:0.08576603978872299\n",
      "Epoch:2 \n",
      "Iteration:186 \n",
      "Loss:0.060864511877298355\n",
      "Epoch:2 \n",
      "Iteration:187 \n",
      "Loss:0.09758847951889038\n",
      "Epoch:2 \n",
      "Iteration:188 \n",
      "Loss:0.03645620495080948\n",
      "Epoch:2 \n",
      "Iteration:189 \n",
      "Loss:0.17544923722743988\n",
      "Epoch:2 \n",
      "Iteration:190 \n",
      "Loss:0.03714743256568909\n",
      "Epoch:2 \n",
      "Iteration:191 \n",
      "Loss:0.08691917359828949\n",
      "Epoch:2 \n",
      "Iteration:192 \n",
      "Loss:0.06506047397851944\n",
      "Epoch:2 \n",
      "Iteration:193 \n",
      "Loss:0.21158522367477417\n",
      "Epoch:2 \n",
      "Iteration:194 \n",
      "Loss:0.019708724692463875\n",
      "Epoch:2 \n",
      "Iteration:195 \n",
      "Loss:0.13108254969120026\n",
      "Epoch:2 \n",
      "Iteration:196 \n",
      "Loss:0.08538669347763062\n",
      "Epoch:2 \n",
      "Iteration:197 \n",
      "Loss:0.09504768252372742\n",
      "Epoch:2 \n",
      "Iteration:198 \n",
      "Loss:0.042060066014528275\n",
      "Epoch:2 \n",
      "Iteration:199 \n",
      "Loss:0.03852638974785805\n",
      "Epoch:2 \n",
      "Iteration:200 \n",
      "Loss:0.12602072954177856\n",
      "Epoch:2 \n",
      "Iteration:201 \n",
      "Loss:0.08068415522575378\n",
      "Epoch:2 \n",
      "Iteration:202 \n",
      "Loss:0.06731382012367249\n",
      "Epoch:2 \n",
      "Iteration:203 \n",
      "Loss:0.04185939207673073\n",
      "Epoch:2 \n",
      "Iteration:204 \n",
      "Loss:0.06309980899095535\n",
      "Epoch:2 \n",
      "Iteration:205 \n",
      "Loss:0.25756776332855225\n",
      "Epoch:2 \n",
      "Iteration:206 \n",
      "Loss:0.121894970536232\n",
      "Epoch:2 \n",
      "Iteration:207 \n",
      "Loss:0.16582271456718445\n",
      "Epoch:2 \n",
      "Iteration:208 \n",
      "Loss:0.05141178518533707\n",
      "Epoch:2 \n",
      "Iteration:209 \n",
      "Loss:0.1772456020116806\n",
      "Epoch:2 \n",
      "Iteration:210 \n",
      "Loss:0.2845035195350647\n",
      "Epoch:2 \n",
      "Iteration:211 \n",
      "Loss:0.07357021421194077\n",
      "Epoch:2 \n",
      "Iteration:212 \n",
      "Loss:0.06464225053787231\n",
      "Epoch:2 \n",
      "Iteration:213 \n",
      "Loss:0.04828151687979698\n",
      "Epoch:2 \n",
      "Iteration:214 \n",
      "Loss:0.053133681416511536\n",
      "Epoch:2 \n",
      "Iteration:215 \n",
      "Loss:0.13257725536823273\n",
      "Epoch:2 \n",
      "Iteration:216 \n",
      "Loss:0.06481591612100601\n",
      "Epoch:2 \n",
      "Iteration:217 \n",
      "Loss:0.1376100778579712\n",
      "Epoch:2 \n",
      "Iteration:218 \n",
      "Loss:0.14807011187076569\n",
      "Epoch:2 \n",
      "Iteration:219 \n",
      "Loss:0.0719819962978363\n",
      "Epoch:2 \n",
      "Iteration:220 \n",
      "Loss:0.039959631860256195\n",
      "Epoch:2 \n",
      "Iteration:221 \n",
      "Loss:0.15952752530574799\n",
      "Epoch:2 \n",
      "Iteration:222 \n",
      "Loss:0.1926977038383484\n",
      "Epoch:2 \n",
      "Iteration:223 \n",
      "Loss:0.06430050730705261\n",
      "Epoch:2 \n",
      "Iteration:224 \n",
      "Loss:0.024328796193003654\n",
      "Epoch:2 \n",
      "Iteration:225 \n",
      "Loss:0.057211656123399734\n",
      "Epoch:2 \n",
      "Iteration:226 \n",
      "Loss:0.10065016150474548\n",
      "Epoch:2 \n",
      "Iteration:227 \n",
      "Loss:0.06397677212953568\n",
      "Epoch:2 \n",
      "Iteration:228 \n",
      "Loss:0.018466521054506302\n",
      "Epoch:2 \n",
      "Iteration:229 \n",
      "Loss:0.07530060410499573\n",
      "Epoch:2 \n",
      "Iteration:230 \n",
      "Loss:0.10649986565113068\n",
      "Epoch:2 \n",
      "Iteration:231 \n",
      "Loss:0.048574239015579224\n",
      "Epoch:2 \n",
      "Iteration:232 \n",
      "Loss:0.0857330858707428\n",
      "Epoch:2 \n",
      "Iteration:233 \n",
      "Loss:0.12259946018457413\n",
      "Epoch:2 \n",
      "Iteration:234 \n",
      "Loss:0.033085696399211884\n",
      "Epoch:2 \n",
      "Iteration:235 \n",
      "Loss:0.1099027469754219\n",
      "Epoch:2 \n",
      "Iteration:236 \n",
      "Loss:0.10122503340244293\n",
      "Epoch:2 \n",
      "Iteration:237 \n",
      "Loss:0.0727650597691536\n",
      "Epoch:2 \n",
      "Iteration:238 \n",
      "Loss:0.2126000076532364\n",
      "Epoch:2 \n",
      "Iteration:239 \n",
      "Loss:0.08605486899614334\n",
      "Epoch:2 \n",
      "Iteration:240 \n",
      "Loss:0.08131510019302368\n",
      "Epoch:2 \n",
      "Iteration:241 \n",
      "Loss:0.08340586721897125\n",
      "Epoch:2 \n",
      "Iteration:242 \n",
      "Loss:0.048440221697092056\n",
      "Epoch:2 \n",
      "Iteration:243 \n",
      "Loss:0.10631762444972992\n",
      "Epoch:2 \n",
      "Iteration:244 \n",
      "Loss:0.05353979021310806\n",
      "Epoch:2 \n",
      "Iteration:245 \n",
      "Loss:0.09776134788990021\n",
      "Epoch:2 \n",
      "Iteration:246 \n",
      "Loss:0.06274985522031784\n",
      "Epoch:2 \n",
      "Iteration:247 \n",
      "Loss:0.15248923003673553\n",
      "Epoch:2 \n",
      "Iteration:248 \n",
      "Loss:0.036425407975912094\n",
      "Epoch:2 \n",
      "Iteration:249 \n",
      "Loss:0.14450109004974365\n",
      "Epoch:2 \n",
      "Iteration:250 \n",
      "Loss:0.03845297545194626\n",
      "Epoch:2 \n",
      "Iteration:251 \n",
      "Loss:0.16880793869495392\n",
      "Epoch:2 \n",
      "Iteration:252 \n",
      "Loss:0.09434308856725693\n",
      "Epoch:2 \n",
      "Iteration:253 \n",
      "Loss:0.17590011656284332\n",
      "Epoch:2 \n",
      "Iteration:254 \n",
      "Loss:0.0871458426117897\n",
      "Epoch:2 \n",
      "Iteration:255 \n",
      "Loss:0.0561591237783432\n",
      "Epoch:2 \n",
      "Iteration:256 \n",
      "Loss:0.018637211993336678\n",
      "Epoch:2 \n",
      "Iteration:257 \n",
      "Loss:0.06569203734397888\n",
      "Epoch:2 \n",
      "Iteration:258 \n",
      "Loss:0.10015752166509628\n",
      "Epoch:2 \n",
      "Iteration:259 \n",
      "Loss:0.04225564002990723\n",
      "Epoch:2 \n",
      "Iteration:260 \n",
      "Loss:0.10267823189496994\n",
      "Epoch:2 \n",
      "Iteration:261 \n",
      "Loss:0.08296890556812286\n",
      "Epoch:2 \n",
      "Iteration:262 \n",
      "Loss:0.03244716301560402\n",
      "Epoch:2 \n",
      "Iteration:263 \n",
      "Loss:0.18013425171375275\n",
      "Epoch:2 \n",
      "Iteration:264 \n",
      "Loss:0.04046235233545303\n",
      "Epoch:2 \n",
      "Iteration:265 \n",
      "Loss:0.024055728688836098\n",
      "Epoch:2 \n",
      "Iteration:266 \n",
      "Loss:0.11884070187807083\n",
      "Epoch:2 \n",
      "Iteration:267 \n",
      "Loss:0.02998245321214199\n",
      "Epoch:2 \n",
      "Iteration:268 \n",
      "Loss:0.052086878567934036\n",
      "Epoch:2 \n",
      "Iteration:269 \n",
      "Loss:0.09080106765031815\n",
      "Epoch:2 \n",
      "Iteration:270 \n",
      "Loss:0.0629081279039383\n",
      "Epoch:2 \n",
      "Iteration:271 \n",
      "Loss:0.0771857425570488\n",
      "Epoch:2 \n",
      "Iteration:272 \n",
      "Loss:0.018934419378638268\n",
      "Epoch:2 \n",
      "Iteration:273 \n",
      "Loss:0.11793094873428345\n",
      "Epoch:2 \n",
      "Iteration:274 \n",
      "Loss:0.04348869249224663\n",
      "Epoch:2 \n",
      "Iteration:275 \n",
      "Loss:0.09788016974925995\n",
      "Epoch:2 \n",
      "Iteration:276 \n",
      "Loss:0.01922612264752388\n",
      "Epoch:2 \n",
      "Iteration:277 \n",
      "Loss:0.13758008182048798\n",
      "Epoch:2 \n",
      "Iteration:278 \n",
      "Loss:0.022951893508434296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2 \n",
      "Iteration:279 \n",
      "Loss:0.04002572223544121\n",
      "Epoch:2 \n",
      "Iteration:280 \n",
      "Loss:0.058139171451330185\n",
      "Epoch:2 \n",
      "Iteration:281 \n",
      "Loss:0.05044596642255783\n",
      "Epoch:2 \n",
      "Iteration:282 \n",
      "Loss:0.1323990821838379\n",
      "Epoch:2 \n",
      "Iteration:283 \n",
      "Loss:0.12433477491140366\n",
      "Epoch:2 \n",
      "Iteration:284 \n",
      "Loss:0.07417905330657959\n",
      "Epoch:2 \n",
      "Iteration:285 \n",
      "Loss:0.0989016517996788\n",
      "Epoch:2 \n",
      "Iteration:286 \n",
      "Loss:0.029146023094654083\n",
      "Epoch:2 \n",
      "Iteration:287 \n",
      "Loss:0.07612933963537216\n",
      "Epoch:2 \n",
      "Iteration:288 \n",
      "Loss:0.060041218996047974\n",
      "Epoch:2 \n",
      "Iteration:289 \n",
      "Loss:0.13334660232067108\n",
      "Epoch:2 \n",
      "Iteration:290 \n",
      "Loss:0.019690461456775665\n",
      "Epoch:2 \n",
      "Iteration:291 \n",
      "Loss:0.017331814393401146\n",
      "Epoch:2 \n",
      "Iteration:292 \n",
      "Loss:0.07126867771148682\n",
      "Epoch:2 \n",
      "Iteration:293 \n",
      "Loss:0.025734635069966316\n",
      "Epoch:2 \n",
      "Iteration:294 \n",
      "Loss:0.04423552379012108\n",
      "Epoch:2 \n",
      "Iteration:295 \n",
      "Loss:0.03229411318898201\n",
      "Epoch:2 \n",
      "Iteration:296 \n",
      "Loss:0.05551575496792793\n",
      "Epoch:2 \n",
      "Iteration:297 \n",
      "Loss:0.02387768030166626\n",
      "Epoch:2 \n",
      "Iteration:298 \n",
      "Loss:0.12588995695114136\n",
      "Epoch:2 \n",
      "Iteration:299 \n",
      "Loss:0.10413670539855957\n",
      "Epoch:2 \n",
      "Iteration:300 \n",
      "Loss:0.14976704120635986\n",
      "Epoch:2 \n",
      "Iteration:301 \n",
      "Loss:0.14102274179458618\n",
      "Epoch:2 \n",
      "Iteration:302 \n",
      "Loss:0.10587026923894882\n",
      "Epoch:2 \n",
      "Iteration:303 \n",
      "Loss:0.02973896451294422\n",
      "Epoch:2 \n",
      "Iteration:304 \n",
      "Loss:0.02672480046749115\n",
      "Epoch:2 \n",
      "Iteration:305 \n",
      "Loss:0.08879692852497101\n",
      "Epoch:2 \n",
      "Iteration:306 \n",
      "Loss:0.07866974174976349\n",
      "Epoch:2 \n",
      "Iteration:307 \n",
      "Loss:0.2881695628166199\n",
      "Epoch:2 \n",
      "Iteration:308 \n",
      "Loss:0.2988487482070923\n",
      "Epoch:2 \n",
      "Iteration:309 \n",
      "Loss:0.10340525954961777\n",
      "Epoch:2 \n",
      "Iteration:310 \n",
      "Loss:0.1028824970126152\n",
      "Epoch:2 \n",
      "Iteration:311 \n",
      "Loss:0.15129925310611725\n",
      "Epoch:2 \n",
      "Iteration:312 \n",
      "Loss:0.08706580847501755\n",
      "Epoch:2 \n",
      "Iteration:313 \n",
      "Loss:0.16873355209827423\n",
      "Epoch:2 \n",
      "Iteration:314 \n",
      "Loss:0.061414770781993866\n",
      "Epoch:2 \n",
      "Iteration:315 \n",
      "Loss:0.09770325571298599\n",
      "Epoch:2 \n",
      "Iteration:316 \n",
      "Loss:0.13110196590423584\n",
      "Epoch:2 \n",
      "Iteration:317 \n",
      "Loss:0.1932714283466339\n",
      "Epoch:2 \n",
      "Iteration:318 \n",
      "Loss:0.0851825550198555\n",
      "Epoch:2 \n",
      "Iteration:319 \n",
      "Loss:0.08326251804828644\n",
      "Epoch:2 \n",
      "Iteration:320 \n",
      "Loss:0.07439133524894714\n",
      "Epoch:2 \n",
      "Iteration:321 \n",
      "Loss:0.08978495746850967\n",
      "Epoch:2 \n",
      "Iteration:322 \n",
      "Loss:0.19639675319194794\n",
      "Epoch:2 \n",
      "Iteration:323 \n",
      "Loss:0.04275340959429741\n",
      "Epoch:2 \n",
      "Iteration:324 \n",
      "Loss:0.09018409997224808\n",
      "Epoch:2 \n",
      "Iteration:325 \n",
      "Loss:0.05520032346248627\n",
      "Epoch:2 \n",
      "Iteration:326 \n",
      "Loss:0.09071093797683716\n",
      "Epoch:2 \n",
      "Iteration:327 \n",
      "Loss:0.047196563333272934\n",
      "Epoch:2 \n",
      "Iteration:328 \n",
      "Loss:0.06127800792455673\n",
      "Epoch:2 \n",
      "Iteration:329 \n",
      "Loss:0.07389719784259796\n",
      "Epoch:2 \n",
      "Iteration:330 \n",
      "Loss:0.04119421914219856\n",
      "Epoch:2 \n",
      "Iteration:331 \n",
      "Loss:0.08584859222173691\n",
      "Epoch:2 \n",
      "Iteration:332 \n",
      "Loss:0.05179165303707123\n",
      "Epoch:2 \n",
      "Iteration:333 \n",
      "Loss:0.17456145584583282\n",
      "Epoch:2 \n",
      "Iteration:334 \n",
      "Loss:0.07832734286785126\n",
      "Epoch:2 \n",
      "Iteration:335 \n",
      "Loss:0.05842089653015137\n",
      "Epoch:2 \n",
      "Iteration:336 \n",
      "Loss:0.06287868320941925\n",
      "Epoch:2 \n",
      "Iteration:337 \n",
      "Loss:0.06237107887864113\n",
      "Epoch:2 \n",
      "Iteration:338 \n",
      "Loss:0.02793659269809723\n",
      "Epoch:2 \n",
      "Iteration:339 \n",
      "Loss:0.16835179924964905\n",
      "Epoch:2 \n",
      "Iteration:340 \n",
      "Loss:0.13800548017024994\n",
      "Epoch:2 \n",
      "Iteration:341 \n",
      "Loss:0.18857212364673615\n",
      "Epoch:2 \n",
      "Iteration:342 \n",
      "Loss:0.06029169633984566\n",
      "Epoch:2 \n",
      "Iteration:343 \n",
      "Loss:0.04826518893241882\n",
      "Epoch:2 \n",
      "Iteration:344 \n",
      "Loss:0.14393354952335358\n",
      "Epoch:2 \n",
      "Iteration:345 \n",
      "Loss:0.06934497505426407\n",
      "Epoch:2 \n",
      "Iteration:346 \n",
      "Loss:0.11738409847021103\n",
      "Epoch:2 \n",
      "Iteration:347 \n",
      "Loss:0.06733574718236923\n",
      "Epoch:2 \n",
      "Iteration:348 \n",
      "Loss:0.016522157937288284\n",
      "Epoch:2 \n",
      "Iteration:349 \n",
      "Loss:0.08302673697471619\n",
      "Epoch:2 \n",
      "Iteration:350 \n",
      "Loss:0.04427957162261009\n",
      "Epoch:2 \n",
      "Iteration:351 \n",
      "Loss:0.06360725313425064\n",
      "Epoch:2 \n",
      "Iteration:352 \n",
      "Loss:0.13549792766571045\n",
      "Epoch:2 \n",
      "Iteration:353 \n",
      "Loss:0.028411608189344406\n",
      "Epoch:2 \n",
      "Iteration:354 \n",
      "Loss:0.12854833900928497\n",
      "Epoch:2 \n",
      "Iteration:355 \n",
      "Loss:0.14960280060768127\n",
      "Epoch:2 \n",
      "Iteration:356 \n",
      "Loss:0.03722245618700981\n",
      "Epoch:2 \n",
      "Iteration:357 \n",
      "Loss:0.09251126646995544\n",
      "Epoch:2 \n",
      "Iteration:358 \n",
      "Loss:0.06516646593809128\n",
      "Epoch:2 \n",
      "Iteration:359 \n",
      "Loss:0.06562315672636032\n",
      "Epoch:2 \n",
      "Iteration:360 \n",
      "Loss:0.042375531047582626\n",
      "Epoch:2 \n",
      "Iteration:361 \n",
      "Loss:0.06699983775615692\n",
      "Epoch:2 \n",
      "Iteration:362 \n",
      "Loss:0.13150759041309357\n",
      "Epoch:2 \n",
      "Iteration:363 \n",
      "Loss:0.04030489921569824\n",
      "Epoch:2 \n",
      "Iteration:364 \n",
      "Loss:0.08023533970117569\n",
      "Epoch:2 \n",
      "Iteration:365 \n",
      "Loss:0.06807806342840195\n",
      "Epoch:2 \n",
      "Iteration:366 \n",
      "Loss:0.07604335993528366\n",
      "Epoch:2 \n",
      "Iteration:367 \n",
      "Loss:0.06862395256757736\n",
      "Epoch:2 \n",
      "Iteration:368 \n",
      "Loss:0.14980755746364594\n",
      "Epoch:2 \n",
      "Iteration:369 \n",
      "Loss:0.034443341195583344\n",
      "Epoch:2 \n",
      "Iteration:370 \n",
      "Loss:0.04800425469875336\n",
      "Epoch:2 \n",
      "Iteration:371 \n",
      "Loss:0.051439229398965836\n",
      "Epoch:2 \n",
      "Iteration:372 \n",
      "Loss:0.09709055721759796\n",
      "Epoch:2 \n",
      "Iteration:373 \n",
      "Loss:0.08504204452037811\n",
      "Epoch:2 \n",
      "Iteration:374 \n",
      "Loss:0.1123960092663765\n",
      "Epoch:2 \n",
      "Iteration:375 \n",
      "Loss:0.1340862363576889\n",
      "Epoch:2 \n",
      "Iteration:376 \n",
      "Loss:0.07038675248622894\n",
      "Epoch:2 \n",
      "Iteration:377 \n",
      "Loss:0.18167418241500854\n",
      "Epoch:2 \n",
      "Iteration:378 \n",
      "Loss:0.06205783411860466\n",
      "Epoch:2 \n",
      "Iteration:379 \n",
      "Loss:0.0854194164276123\n",
      "Epoch:2 \n",
      "Iteration:380 \n",
      "Loss:0.02837475575506687\n",
      "Epoch:2 \n",
      "Iteration:381 \n",
      "Loss:0.06367925554513931\n",
      "Epoch:2 \n",
      "Iteration:382 \n",
      "Loss:0.03235067427158356\n",
      "Epoch:2 \n",
      "Iteration:383 \n",
      "Loss:0.06539309769868851\n",
      "Epoch:2 \n",
      "Iteration:384 \n",
      "Loss:0.00424795551225543\n",
      "Epoch:2 \n",
      "Iteration:385 \n",
      "Loss:0.03472180664539337\n",
      "Epoch:2 \n",
      "Iteration:386 \n",
      "Loss:0.09082392603158951\n",
      "Epoch:2 \n",
      "Iteration:387 \n",
      "Loss:0.03933430835604668\n",
      "Epoch:2 \n",
      "Iteration:388 \n",
      "Loss:0.17413005232810974\n",
      "Epoch:2 \n",
      "Iteration:389 \n",
      "Loss:0.03767053782939911\n",
      "Epoch:2 \n",
      "Iteration:390 \n",
      "Loss:0.12306584417819977\n",
      "Epoch:2 \n",
      "Iteration:391 \n",
      "Loss:0.03919290006160736\n",
      "Epoch:2 \n",
      "Iteration:392 \n",
      "Loss:0.015977347269654274\n",
      "Epoch:2 \n",
      "Iteration:393 \n",
      "Loss:0.11206331104040146\n",
      "Epoch:2 \n",
      "Iteration:394 \n",
      "Loss:0.09667129814624786\n",
      "Epoch:2 \n",
      "Iteration:395 \n",
      "Loss:0.07877352833747864\n",
      "Epoch:2 \n",
      "Iteration:396 \n",
      "Loss:0.10411285609006882\n",
      "Epoch:2 \n",
      "Iteration:397 \n",
      "Loss:0.06105979532003403\n",
      "Epoch:2 \n",
      "Iteration:398 \n",
      "Loss:0.12042148411273956\n",
      "Epoch:2 \n",
      "Iteration:399 \n",
      "Loss:0.01514794584363699\n",
      "Epoch:2 \n",
      "Iteration:400 \n",
      "Loss:0.0318511500954628\n",
      "Epoch:2 \n",
      "Iteration:401 \n",
      "Loss:0.2070971429347992\n",
      "Epoch:2 \n",
      "Iteration:402 \n",
      "Loss:0.1391669511795044\n",
      "Epoch:2 \n",
      "Iteration:403 \n",
      "Loss:0.03206668049097061\n",
      "Epoch:2 \n",
      "Iteration:404 \n",
      "Loss:0.21106724441051483\n",
      "Epoch:2 \n",
      "Iteration:405 \n",
      "Loss:0.10386157780885696\n",
      "Epoch:2 \n",
      "Iteration:406 \n",
      "Loss:0.01880810223519802\n",
      "Epoch:2 \n",
      "Iteration:407 \n",
      "Loss:0.08602999895811081\n",
      "Epoch:2 \n",
      "Iteration:408 \n",
      "Loss:0.19997011125087738\n",
      "Epoch:2 \n",
      "Iteration:409 \n",
      "Loss:0.055298689752817154\n",
      "Epoch:2 \n",
      "Iteration:410 \n",
      "Loss:0.0473397858440876\n",
      "Epoch:2 \n",
      "Iteration:411 \n",
      "Loss:0.20255659520626068\n",
      "Epoch:2 \n",
      "Iteration:412 \n",
      "Loss:0.045019883662462234\n",
      "Epoch:2 \n",
      "Iteration:413 \n",
      "Loss:0.1812419891357422\n",
      "Epoch:2 \n",
      "Iteration:414 \n",
      "Loss:0.08829294145107269\n",
      "Epoch:2 \n",
      "Iteration:415 \n",
      "Loss:0.10385873913764954\n",
      "Epoch:2 \n",
      "Iteration:416 \n",
      "Loss:0.041494231671094894\n",
      "Epoch:2 \n",
      "Iteration:417 \n",
      "Loss:0.030125033110380173\n",
      "Epoch:2 \n",
      "Iteration:418 \n",
      "Loss:0.07113087177276611\n",
      "Epoch:2 \n",
      "Iteration:419 \n",
      "Loss:0.1513689011335373\n",
      "Epoch:2 \n",
      "Iteration:420 \n",
      "Loss:0.08001048117876053\n",
      "Epoch:2 \n",
      "Iteration:421 \n",
      "Loss:0.08398983627557755\n",
      "Epoch:2 \n",
      "Iteration:422 \n",
      "Loss:0.06247904151678085\n",
      "Epoch:2 \n",
      "Iteration:423 \n",
      "Loss:0.08114616572856903\n",
      "Epoch:2 \n",
      "Iteration:424 \n",
      "Loss:0.04297105222940445\n",
      "Epoch:2 \n",
      "Iteration:425 \n",
      "Loss:0.09259044378995895\n",
      "Epoch:2 \n",
      "Iteration:426 \n",
      "Loss:0.1321702003479004\n",
      "Epoch:2 \n",
      "Iteration:427 \n",
      "Loss:0.17949098348617554\n",
      "Epoch:2 \n",
      "Iteration:428 \n",
      "Loss:0.06876982003450394\n",
      "Epoch:2 \n",
      "Iteration:429 \n",
      "Loss:0.09539525955915451\n",
      "Epoch:2 \n",
      "Iteration:430 \n",
      "Loss:0.10135524719953537\n",
      "Epoch:2 \n",
      "Iteration:431 \n",
      "Loss:0.035346776247024536\n",
      "Epoch:2 \n",
      "Iteration:432 \n",
      "Loss:0.12643378973007202\n",
      "Epoch:2 \n",
      "Iteration:433 \n",
      "Loss:0.02688165195286274\n",
      "Epoch:2 \n",
      "Iteration:434 \n",
      "Loss:0.052182093262672424\n",
      "Epoch:2 \n",
      "Iteration:435 \n",
      "Loss:0.23109851777553558\n",
      "Epoch:2 \n",
      "Iteration:436 \n",
      "Loss:0.1420464664697647\n",
      "Epoch:2 \n",
      "Iteration:437 \n",
      "Loss:0.09991009533405304\n",
      "Epoch:2 \n",
      "Iteration:438 \n",
      "Loss:0.03685600310564041\n",
      "Epoch:2 \n",
      "Iteration:439 \n",
      "Loss:0.10966259986162186\n",
      "Epoch:2 \n",
      "Iteration:440 \n",
      "Loss:0.14692892134189606\n",
      "Epoch:2 \n",
      "Iteration:441 \n",
      "Loss:0.07716568559408188\n",
      "Epoch:2 \n",
      "Iteration:442 \n",
      "Loss:0.10208268463611603\n",
      "Epoch:2 \n",
      "Iteration:443 \n",
      "Loss:0.1128358393907547\n",
      "Epoch:2 \n",
      "Iteration:444 \n",
      "Loss:0.062188971787691116\n",
      "Epoch:2 \n",
      "Iteration:445 \n",
      "Loss:0.08713003993034363\n",
      "Epoch:2 \n",
      "Iteration:446 \n",
      "Loss:0.09090324491262436\n",
      "Epoch:2 \n",
      "Iteration:447 \n",
      "Loss:0.11777570843696594\n",
      "Epoch:2 \n",
      "Iteration:448 \n",
      "Loss:0.061372045427560806\n",
      "Epoch:2 \n",
      "Iteration:449 \n",
      "Loss:0.07631354033946991\n",
      "Epoch:2 \n",
      "Iteration:450 \n",
      "Loss:0.05208823084831238\n",
      "Epoch:2 \n",
      "Iteration:451 \n",
      "Loss:0.08311394602060318\n",
      "Epoch:2 \n",
      "Iteration:452 \n",
      "Loss:0.0697915107011795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2 \n",
      "Iteration:453 \n",
      "Loss:0.04550196975469589\n",
      "Epoch:2 \n",
      "Iteration:454 \n",
      "Loss:0.01881393976509571\n",
      "Epoch:2 \n",
      "Iteration:455 \n",
      "Loss:0.09792184829711914\n",
      "Epoch:2 \n",
      "Iteration:456 \n",
      "Loss:0.0425495021045208\n",
      "Epoch:2 \n",
      "Iteration:457 \n",
      "Loss:0.048559416085481644\n",
      "Epoch:2 \n",
      "Iteration:458 \n",
      "Loss:0.20353494584560394\n",
      "Epoch:2 \n",
      "Iteration:459 \n",
      "Loss:0.0651637613773346\n",
      "Epoch:2 \n",
      "Iteration:460 \n",
      "Loss:0.068580262362957\n",
      "Epoch:2 \n",
      "Iteration:461 \n",
      "Loss:0.1157805547118187\n",
      "Epoch:2 \n",
      "Iteration:462 \n",
      "Loss:0.09785594791173935\n",
      "Epoch:2 \n",
      "Iteration:463 \n",
      "Loss:0.03745230287313461\n",
      "Epoch:2 \n",
      "Iteration:464 \n",
      "Loss:0.05954829230904579\n",
      "Epoch:2 \n",
      "Iteration:465 \n",
      "Loss:0.08531931787729263\n",
      "Epoch:2 \n",
      "Iteration:466 \n",
      "Loss:0.027128970250487328\n",
      "Epoch:2 \n",
      "Iteration:467 \n",
      "Loss:0.12149880081415176\n",
      "Epoch:2 \n",
      "Iteration:468 \n",
      "Loss:0.16803333163261414\n",
      "Epoch:2 \n",
      "Iteration:469 \n",
      "Loss:0.05600021407008171\n",
      "Epoch:2 \n",
      "Iteration:470 \n",
      "Loss:0.11090677231550217\n",
      "Epoch:2 \n",
      "Iteration:471 \n",
      "Loss:0.06198808550834656\n",
      "Epoch:2 \n",
      "Iteration:472 \n",
      "Loss:0.10487203299999237\n",
      "Epoch:2 \n",
      "Iteration:473 \n",
      "Loss:0.1134944036602974\n",
      "Epoch:2 \n",
      "Iteration:474 \n",
      "Loss:0.027583608403801918\n",
      "Epoch:2 \n",
      "Iteration:475 \n",
      "Loss:0.03191682696342468\n",
      "Epoch:2 \n",
      "Iteration:476 \n",
      "Loss:0.06409953534603119\n",
      "Epoch:2 \n",
      "Iteration:477 \n",
      "Loss:0.10737903416156769\n",
      "Epoch:2 \n",
      "Iteration:478 \n",
      "Loss:0.16667672991752625\n",
      "Epoch:2 \n",
      "Iteration:479 \n",
      "Loss:0.10453347861766815\n",
      "Epoch:2 \n",
      "Iteration:480 \n",
      "Loss:0.06638959050178528\n",
      "Epoch:2 \n",
      "Iteration:481 \n",
      "Loss:0.16401077806949615\n",
      "Epoch:2 \n",
      "Iteration:482 \n",
      "Loss:0.12214735150337219\n",
      "Epoch:2 \n",
      "Iteration:483 \n",
      "Loss:0.043374136090278625\n",
      "Epoch:2 \n",
      "Iteration:484 \n",
      "Loss:0.06023109331727028\n",
      "Epoch:2 \n",
      "Iteration:485 \n",
      "Loss:0.09209244698286057\n",
      "Epoch:2 \n",
      "Iteration:486 \n",
      "Loss:0.17898482084274292\n",
      "Epoch:2 \n",
      "Iteration:487 \n",
      "Loss:0.10224682837724686\n",
      "Epoch:2 \n",
      "Iteration:488 \n",
      "Loss:0.11113987863063812\n",
      "Epoch:2 \n",
      "Iteration:489 \n",
      "Loss:0.05184522643685341\n",
      "Epoch:2 \n",
      "Iteration:490 \n",
      "Loss:0.07663967460393906\n",
      "Epoch:2 \n",
      "Iteration:491 \n",
      "Loss:0.10011367499828339\n",
      "Epoch:2 \n",
      "Iteration:492 \n",
      "Loss:0.04430067539215088\n",
      "Epoch:2 \n",
      "Iteration:493 \n",
      "Loss:0.10282577574253082\n",
      "Epoch:2 \n",
      "Iteration:494 \n",
      "Loss:0.09932348132133484\n",
      "Epoch:2 \n",
      "Iteration:495 \n",
      "Loss:0.05465708300471306\n",
      "Epoch:2 \n",
      "Iteration:496 \n",
      "Loss:0.15924575924873352\n",
      "Epoch:2 \n",
      "Iteration:497 \n",
      "Loss:0.07463816553354263\n",
      "Epoch:2 \n",
      "Iteration:498 \n",
      "Loss:0.04614052176475525\n",
      "Epoch:2 \n",
      "Iteration:499 \n",
      "Loss:0.10149058699607849\n",
      "Epoch:2 \n",
      "Iteration:500 \n",
      "Loss:0.041597623378038406\n",
      "Epoch:2 \n",
      "Iteration:501 \n",
      "Loss:0.10619933903217316\n",
      "Epoch:2 \n",
      "Iteration:502 \n",
      "Loss:0.11472491919994354\n",
      "Epoch:2 \n",
      "Iteration:503 \n",
      "Loss:0.047853611409664154\n",
      "Epoch:2 \n",
      "Iteration:504 \n",
      "Loss:0.09121329337358475\n",
      "Epoch:2 \n",
      "Iteration:505 \n",
      "Loss:0.07136335968971252\n",
      "Epoch:2 \n",
      "Iteration:506 \n",
      "Loss:0.11915867030620575\n",
      "Epoch:2 \n",
      "Iteration:507 \n",
      "Loss:0.07286311686038971\n",
      "Epoch:2 \n",
      "Iteration:508 \n",
      "Loss:0.013039316982030869\n",
      "Epoch:2 \n",
      "Iteration:509 \n",
      "Loss:0.1344774216413498\n",
      "Epoch:2 \n",
      "Iteration:510 \n",
      "Loss:0.06905459612607956\n",
      "Epoch:2 \n",
      "Iteration:511 \n",
      "Loss:0.16367079317569733\n",
      "Epoch:2 \n",
      "Iteration:512 \n",
      "Loss:0.14452095329761505\n",
      "Epoch:2 \n",
      "Iteration:513 \n",
      "Loss:0.09186919033527374\n",
      "Epoch:2 \n",
      "Iteration:514 \n",
      "Loss:0.18742643296718597\n",
      "Epoch:2 \n",
      "Iteration:515 \n",
      "Loss:0.14501602947711945\n",
      "Epoch:2 \n",
      "Iteration:516 \n",
      "Loss:0.056415166705846786\n",
      "Epoch:2 \n",
      "Iteration:517 \n",
      "Loss:0.15252907574176788\n",
      "Epoch:2 \n",
      "Iteration:518 \n",
      "Loss:0.08370443433523178\n",
      "Epoch:2 \n",
      "Iteration:519 \n",
      "Loss:0.05489034205675125\n",
      "Epoch:2 \n",
      "Iteration:520 \n",
      "Loss:0.12789282202720642\n",
      "Epoch:2 \n",
      "Iteration:521 \n",
      "Loss:0.01347032655030489\n",
      "Epoch:2 \n",
      "Iteration:522 \n",
      "Loss:0.16926449537277222\n",
      "Epoch:2 \n",
      "Iteration:523 \n",
      "Loss:0.15164607763290405\n",
      "Epoch:2 \n",
      "Iteration:524 \n",
      "Loss:0.1280922144651413\n",
      "Epoch:2 \n",
      "Iteration:525 \n",
      "Loss:0.042470403015613556\n",
      "Epoch:2 \n",
      "Iteration:526 \n",
      "Loss:0.04008248448371887\n",
      "Epoch:2 \n",
      "Iteration:527 \n",
      "Loss:0.2344820201396942\n",
      "Epoch:2 \n",
      "Iteration:528 \n",
      "Loss:0.15870089828968048\n",
      "Epoch:2 \n",
      "Iteration:529 \n",
      "Loss:0.02038516290485859\n",
      "Epoch:2 \n",
      "Iteration:530 \n",
      "Loss:0.07264091074466705\n",
      "Epoch:2 \n",
      "Iteration:531 \n",
      "Loss:0.13728106021881104\n",
      "Epoch:2 \n",
      "Iteration:532 \n",
      "Loss:0.10198420286178589\n",
      "Epoch:2 \n",
      "Iteration:533 \n",
      "Loss:0.0652262344956398\n",
      "Epoch:2 \n",
      "Iteration:534 \n",
      "Loss:0.028083300217986107\n",
      "Epoch:2 \n",
      "Iteration:535 \n",
      "Loss:0.08931101113557816\n",
      "Epoch:2 \n",
      "Iteration:536 \n",
      "Loss:0.078989677131176\n",
      "Epoch:2 \n",
      "Iteration:537 \n",
      "Loss:0.05935883894562721\n",
      "Epoch:2 \n",
      "Iteration:538 \n",
      "Loss:0.0985092744231224\n",
      "Epoch:2 \n",
      "Iteration:539 \n",
      "Loss:0.09143944084644318\n",
      "Epoch:2 \n",
      "Iteration:540 \n",
      "Loss:0.11642756313085556\n",
      "Epoch:2 \n",
      "Iteration:541 \n",
      "Loss:0.06259001046419144\n",
      "Epoch:2 \n",
      "Iteration:542 \n",
      "Loss:0.03879936784505844\n",
      "Epoch:2 \n",
      "Iteration:543 \n",
      "Loss:0.10426793992519379\n",
      "Epoch:2 \n",
      "Iteration:544 \n",
      "Loss:0.014819770120084286\n",
      "Epoch:2 \n",
      "Iteration:545 \n",
      "Loss:0.036482661962509155\n",
      "Epoch:2 \n",
      "Iteration:546 \n",
      "Loss:0.13940773904323578\n",
      "Epoch:2 \n",
      "Iteration:547 \n",
      "Loss:0.14229685068130493\n",
      "Epoch:2 \n",
      "Iteration:548 \n",
      "Loss:0.20095893740653992\n",
      "Epoch:2 \n",
      "Iteration:549 \n",
      "Loss:0.11181967705488205\n",
      "Epoch:2 \n",
      "Iteration:550 \n",
      "Loss:0.05977347865700722\n",
      "Epoch:2 \n",
      "Iteration:551 \n",
      "Loss:0.08531860262155533\n",
      "Epoch:2 \n",
      "Iteration:552 \n",
      "Loss:0.06092125549912453\n",
      "Epoch:2 \n",
      "Iteration:553 \n",
      "Loss:0.013145566917955875\n",
      "Epoch:2 \n",
      "Iteration:554 \n",
      "Loss:0.17936508357524872\n",
      "Epoch:2 \n",
      "Iteration:555 \n",
      "Loss:0.11644499748945236\n",
      "Epoch:2 \n",
      "Iteration:556 \n",
      "Loss:0.13941636681556702\n",
      "Epoch:2 \n",
      "Iteration:557 \n",
      "Loss:0.05345474183559418\n",
      "Epoch:2 \n",
      "Iteration:558 \n",
      "Loss:0.08788249641656876\n",
      "Epoch:2 \n",
      "Iteration:559 \n",
      "Loss:0.13953299820423126\n",
      "Epoch:2 \n",
      "Iteration:560 \n",
      "Loss:0.1786438524723053\n",
      "Epoch:2 \n",
      "Iteration:561 \n",
      "Loss:0.15164700150489807\n",
      "Epoch:2 \n",
      "Iteration:562 \n",
      "Loss:0.08470664173364639\n",
      "Epoch:2 \n",
      "Iteration:563 \n",
      "Loss:0.13302446901798248\n",
      "Epoch:2 \n",
      "Iteration:564 \n",
      "Loss:0.10721060633659363\n",
      "Epoch:2 \n",
      "Iteration:565 \n",
      "Loss:0.074092335999012\n",
      "Epoch:2 \n",
      "Iteration:566 \n",
      "Loss:0.13409669697284698\n",
      "Epoch:2 \n",
      "Iteration:567 \n",
      "Loss:0.042138777673244476\n",
      "Epoch:2 \n",
      "Iteration:568 \n",
      "Loss:0.12802094221115112\n",
      "Epoch:2 \n",
      "Iteration:569 \n",
      "Loss:0.1266195923089981\n",
      "Epoch:2 \n",
      "Iteration:570 \n",
      "Loss:0.0965368002653122\n",
      "Epoch:2 \n",
      "Iteration:571 \n",
      "Loss:0.06095511093735695\n",
      "Epoch:2 \n",
      "Iteration:572 \n",
      "Loss:0.10491208732128143\n",
      "Epoch:2 \n",
      "Iteration:573 \n",
      "Loss:0.049069810658693314\n",
      "Epoch:2 \n",
      "Iteration:574 \n",
      "Loss:0.06768140941858292\n",
      "Epoch:2 \n",
      "Iteration:575 \n",
      "Loss:0.03246474266052246\n",
      "Epoch:2 \n",
      "Iteration:576 \n",
      "Loss:0.11883799731731415\n",
      "Epoch:2 \n",
      "Iteration:577 \n",
      "Loss:0.06053781881928444\n",
      "Epoch:2 \n",
      "Iteration:578 \n",
      "Loss:0.019635546952486038\n",
      "Epoch:2 \n",
      "Iteration:579 \n",
      "Loss:0.09838902205228806\n",
      "Epoch:2 \n",
      "Iteration:580 \n",
      "Loss:0.16635048389434814\n",
      "Epoch:2 \n",
      "Iteration:581 \n",
      "Loss:0.08783382177352905\n",
      "Epoch:2 \n",
      "Iteration:582 \n",
      "Loss:0.046604935079813004\n",
      "Epoch:2 \n",
      "Iteration:583 \n",
      "Loss:0.06369833648204803\n",
      "Epoch:2 \n",
      "Iteration:584 \n",
      "Loss:0.018946215510368347\n",
      "Epoch:2 \n",
      "Iteration:585 \n",
      "Loss:0.09667161852121353\n",
      "Epoch:2 \n",
      "Iteration:586 \n",
      "Loss:0.12447510659694672\n",
      "Epoch:2 \n",
      "Iteration:587 \n",
      "Loss:0.1134222224354744\n",
      "Epoch:2 \n",
      "Iteration:588 \n",
      "Loss:0.0619090236723423\n",
      "Epoch:2 \n",
      "Iteration:589 \n",
      "Loss:0.03641628101468086\n",
      "Epoch:2 \n",
      "Iteration:590 \n",
      "Loss:0.08511552959680557\n",
      "Epoch:2 \n",
      "Iteration:591 \n",
      "Loss:0.13629785180091858\n",
      "Epoch:2 \n",
      "Iteration:592 \n",
      "Loss:0.037782732397317886\n",
      "Epoch:2 \n",
      "Iteration:593 \n",
      "Loss:0.12178778648376465\n",
      "Epoch:2 \n",
      "Iteration:594 \n",
      "Loss:0.027897292748093605\n",
      "Epoch:2 \n",
      "Iteration:595 \n",
      "Loss:0.0259077288210392\n",
      "Epoch:2 \n",
      "Iteration:596 \n",
      "Loss:0.026701796799898148\n",
      "Epoch:2 \n",
      "Iteration:597 \n",
      "Loss:0.04176654666662216\n",
      "Epoch:2 \n",
      "Iteration:598 \n",
      "Loss:0.03215411677956581\n",
      "Epoch:2 \n",
      "Iteration:599 \n",
      "Loss:0.024355784058570862\n",
      "Epoch:2 \n",
      "Iteration:600 \n",
      "Loss:0.10602573305368423\n",
      "\n",
      "Accuracy of network in epoch 2: 97.215\n",
      "Epoch:3 \n",
      "Iteration:1 \n",
      "Loss:0.06507548689842224\n",
      "Epoch:3 \n",
      "Iteration:2 \n",
      "Loss:0.05685470625758171\n",
      "Epoch:3 \n",
      "Iteration:3 \n",
      "Loss:0.050836361944675446\n",
      "Epoch:3 \n",
      "Iteration:4 \n",
      "Loss:0.1531015932559967\n",
      "Epoch:3 \n",
      "Iteration:5 \n",
      "Loss:0.05276000499725342\n",
      "Epoch:3 \n",
      "Iteration:6 \n",
      "Loss:0.040957726538181305\n",
      "Epoch:3 \n",
      "Iteration:7 \n",
      "Loss:0.009984851814806461\n",
      "Epoch:3 \n",
      "Iteration:8 \n",
      "Loss:0.07752212882041931\n",
      "Epoch:3 \n",
      "Iteration:9 \n",
      "Loss:0.055770035833120346\n",
      "Epoch:3 \n",
      "Iteration:10 \n",
      "Loss:0.011395781300961971\n",
      "Epoch:3 \n",
      "Iteration:11 \n",
      "Loss:0.12790292501449585\n",
      "Epoch:3 \n",
      "Iteration:12 \n",
      "Loss:0.026682408526539803\n",
      "Epoch:3 \n",
      "Iteration:13 \n",
      "Loss:0.061499666422605515\n",
      "Epoch:3 \n",
      "Iteration:14 \n",
      "Loss:0.061734963208436966\n",
      "Epoch:3 \n",
      "Iteration:15 \n",
      "Loss:0.11611274629831314\n",
      "Epoch:3 \n",
      "Iteration:16 \n",
      "Loss:0.14121262729167938\n",
      "Epoch:3 \n",
      "Iteration:17 \n",
      "Loss:0.027529515326023102\n",
      "Epoch:3 \n",
      "Iteration:18 \n",
      "Loss:0.0688527449965477\n",
      "Epoch:3 \n",
      "Iteration:19 \n",
      "Loss:0.05044511705636978\n",
      "Epoch:3 \n",
      "Iteration:20 \n",
      "Loss:0.05832422897219658\n",
      "Epoch:3 \n",
      "Iteration:21 \n",
      "Loss:0.06288693100214005\n",
      "Epoch:3 \n",
      "Iteration:22 \n",
      "Loss:0.1361725628376007\n",
      "Epoch:3 \n",
      "Iteration:23 \n",
      "Loss:0.08357694000005722\n",
      "Epoch:3 \n",
      "Iteration:24 \n",
      "Loss:0.008071056567132473\n",
      "Epoch:3 \n",
      "Iteration:25 \n",
      "Loss:0.019149335101246834\n",
      "Epoch:3 \n",
      "Iteration:26 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.004059730097651482\n",
      "Epoch:3 \n",
      "Iteration:27 \n",
      "Loss:0.046423234045505524\n",
      "Epoch:3 \n",
      "Iteration:28 \n",
      "Loss:0.08173656463623047\n",
      "Epoch:3 \n",
      "Iteration:29 \n",
      "Loss:0.14501319825649261\n",
      "Epoch:3 \n",
      "Iteration:30 \n",
      "Loss:0.06762338429689407\n",
      "Epoch:3 \n",
      "Iteration:31 \n",
      "Loss:0.06413081288337708\n",
      "Epoch:3 \n",
      "Iteration:32 \n",
      "Loss:0.0186809953302145\n",
      "Epoch:3 \n",
      "Iteration:33 \n",
      "Loss:0.055727168917655945\n",
      "Epoch:3 \n",
      "Iteration:34 \n",
      "Loss:0.08849415183067322\n",
      "Epoch:3 \n",
      "Iteration:35 \n",
      "Loss:0.08858504891395569\n",
      "Epoch:3 \n",
      "Iteration:36 \n",
      "Loss:0.07735386490821838\n",
      "Epoch:3 \n",
      "Iteration:37 \n",
      "Loss:0.06992447376251221\n",
      "Epoch:3 \n",
      "Iteration:38 \n",
      "Loss:0.047465477138757706\n",
      "Epoch:3 \n",
      "Iteration:39 \n",
      "Loss:0.10266265273094177\n",
      "Epoch:3 \n",
      "Iteration:40 \n",
      "Loss:0.05181694030761719\n",
      "Epoch:3 \n",
      "Iteration:41 \n",
      "Loss:0.018204815685749054\n",
      "Epoch:3 \n",
      "Iteration:42 \n",
      "Loss:0.07821105420589447\n",
      "Epoch:3 \n",
      "Iteration:43 \n",
      "Loss:0.029496340081095695\n",
      "Epoch:3 \n",
      "Iteration:44 \n",
      "Loss:0.06150510907173157\n",
      "Epoch:3 \n",
      "Iteration:45 \n",
      "Loss:0.09743770956993103\n",
      "Epoch:3 \n",
      "Iteration:46 \n",
      "Loss:0.023362793028354645\n",
      "Epoch:3 \n",
      "Iteration:47 \n",
      "Loss:0.012773985043168068\n",
      "Epoch:3 \n",
      "Iteration:48 \n",
      "Loss:0.026919851079583168\n",
      "Epoch:3 \n",
      "Iteration:49 \n",
      "Loss:0.05501348897814751\n",
      "Epoch:3 \n",
      "Iteration:50 \n",
      "Loss:0.03430558741092682\n",
      "Epoch:3 \n",
      "Iteration:51 \n",
      "Loss:0.06731242686510086\n",
      "Epoch:3 \n",
      "Iteration:52 \n",
      "Loss:0.02945077419281006\n",
      "Epoch:3 \n",
      "Iteration:53 \n",
      "Loss:0.025796061381697655\n",
      "Epoch:3 \n",
      "Iteration:54 \n",
      "Loss:0.036317549645900726\n",
      "Epoch:3 \n",
      "Iteration:55 \n",
      "Loss:0.08765850216150284\n",
      "Epoch:3 \n",
      "Iteration:56 \n",
      "Loss:0.03827374801039696\n",
      "Epoch:3 \n",
      "Iteration:57 \n",
      "Loss:0.10664905607700348\n",
      "Epoch:3 \n",
      "Iteration:58 \n",
      "Loss:0.01799718104302883\n",
      "Epoch:3 \n",
      "Iteration:59 \n",
      "Loss:0.0073890988714993\n",
      "Epoch:3 \n",
      "Iteration:60 \n",
      "Loss:0.06816842406988144\n",
      "Epoch:3 \n",
      "Iteration:61 \n",
      "Loss:0.09402107447385788\n",
      "Epoch:3 \n",
      "Iteration:62 \n",
      "Loss:0.04320809990167618\n",
      "Epoch:3 \n",
      "Iteration:63 \n",
      "Loss:0.02976069040596485\n",
      "Epoch:3 \n",
      "Iteration:64 \n",
      "Loss:0.08735881745815277\n",
      "Epoch:3 \n",
      "Iteration:65 \n",
      "Loss:0.07688412070274353\n",
      "Epoch:3 \n",
      "Iteration:66 \n",
      "Loss:0.06768199056386948\n",
      "Epoch:3 \n",
      "Iteration:67 \n",
      "Loss:0.023938028141856194\n",
      "Epoch:3 \n",
      "Iteration:68 \n",
      "Loss:0.05929949879646301\n",
      "Epoch:3 \n",
      "Iteration:69 \n",
      "Loss:0.02841467782855034\n",
      "Epoch:3 \n",
      "Iteration:70 \n",
      "Loss:0.1679990440607071\n",
      "Epoch:3 \n",
      "Iteration:71 \n",
      "Loss:0.12365128844976425\n",
      "Epoch:3 \n",
      "Iteration:72 \n",
      "Loss:0.2302166223526001\n",
      "Epoch:3 \n",
      "Iteration:73 \n",
      "Loss:0.09471377730369568\n",
      "Epoch:3 \n",
      "Iteration:74 \n",
      "Loss:0.05234023556113243\n",
      "Epoch:3 \n",
      "Iteration:75 \n",
      "Loss:0.07776690274477005\n",
      "Epoch:3 \n",
      "Iteration:76 \n",
      "Loss:0.03717222809791565\n",
      "Epoch:3 \n",
      "Iteration:77 \n",
      "Loss:0.12611238658428192\n",
      "Epoch:3 \n",
      "Iteration:78 \n",
      "Loss:0.049026697874069214\n",
      "Epoch:3 \n",
      "Iteration:79 \n",
      "Loss:0.04879935085773468\n",
      "Epoch:3 \n",
      "Iteration:80 \n",
      "Loss:0.03330663591623306\n",
      "Epoch:3 \n",
      "Iteration:81 \n",
      "Loss:0.053591154515743256\n",
      "Epoch:3 \n",
      "Iteration:82 \n",
      "Loss:0.05310116708278656\n",
      "Epoch:3 \n",
      "Iteration:83 \n",
      "Loss:0.07160241901874542\n",
      "Epoch:3 \n",
      "Iteration:84 \n",
      "Loss:0.046649474650621414\n",
      "Epoch:3 \n",
      "Iteration:85 \n",
      "Loss:0.06963767111301422\n",
      "Epoch:3 \n",
      "Iteration:86 \n",
      "Loss:0.06221383064985275\n",
      "Epoch:3 \n",
      "Iteration:87 \n",
      "Loss:0.037910059094429016\n",
      "Epoch:3 \n",
      "Iteration:88 \n",
      "Loss:0.054913368076086044\n",
      "Epoch:3 \n",
      "Iteration:89 \n",
      "Loss:0.10284756869077682\n",
      "Epoch:3 \n",
      "Iteration:90 \n",
      "Loss:0.022256288677453995\n",
      "Epoch:3 \n",
      "Iteration:91 \n",
      "Loss:0.05771421268582344\n",
      "Epoch:3 \n",
      "Iteration:92 \n",
      "Loss:0.06307995319366455\n",
      "Epoch:3 \n",
      "Iteration:93 \n",
      "Loss:0.08350738883018494\n",
      "Epoch:3 \n",
      "Iteration:94 \n",
      "Loss:0.04036369174718857\n",
      "Epoch:3 \n",
      "Iteration:95 \n",
      "Loss:0.10758436471223831\n",
      "Epoch:3 \n",
      "Iteration:96 \n",
      "Loss:0.052400678396224976\n",
      "Epoch:3 \n",
      "Iteration:97 \n",
      "Loss:0.035749778151512146\n",
      "Epoch:3 \n",
      "Iteration:98 \n",
      "Loss:0.0746956467628479\n",
      "Epoch:3 \n",
      "Iteration:99 \n",
      "Loss:0.01734868809580803\n",
      "Epoch:3 \n",
      "Iteration:100 \n",
      "Loss:0.030712759122252464\n",
      "Epoch:3 \n",
      "Iteration:101 \n",
      "Loss:0.04040813073515892\n",
      "Epoch:3 \n",
      "Iteration:102 \n",
      "Loss:0.0280124731361866\n",
      "Epoch:3 \n",
      "Iteration:103 \n",
      "Loss:0.0047061508521437645\n",
      "Epoch:3 \n",
      "Iteration:104 \n",
      "Loss:0.032903045415878296\n",
      "Epoch:3 \n",
      "Iteration:105 \n",
      "Loss:0.0678003579378128\n",
      "Epoch:3 \n",
      "Iteration:106 \n",
      "Loss:0.07531868666410446\n",
      "Epoch:3 \n",
      "Iteration:107 \n",
      "Loss:0.011015328578650951\n",
      "Epoch:3 \n",
      "Iteration:108 \n",
      "Loss:0.08877000957727432\n",
      "Epoch:3 \n",
      "Iteration:109 \n",
      "Loss:0.039238158613443375\n",
      "Epoch:3 \n",
      "Iteration:110 \n",
      "Loss:0.06281284987926483\n",
      "Epoch:3 \n",
      "Iteration:111 \n",
      "Loss:0.009448271244764328\n",
      "Epoch:3 \n",
      "Iteration:112 \n",
      "Loss:0.011291783303022385\n",
      "Epoch:3 \n",
      "Iteration:113 \n",
      "Loss:0.14380235970020294\n",
      "Epoch:3 \n",
      "Iteration:114 \n",
      "Loss:0.04733504354953766\n",
      "Epoch:3 \n",
      "Iteration:115 \n",
      "Loss:0.0742424800992012\n",
      "Epoch:3 \n",
      "Iteration:116 \n",
      "Loss:0.03219160810112953\n",
      "Epoch:3 \n",
      "Iteration:117 \n",
      "Loss:0.08142344653606415\n",
      "Epoch:3 \n",
      "Iteration:118 \n",
      "Loss:0.0178996492177248\n",
      "Epoch:3 \n",
      "Iteration:119 \n",
      "Loss:0.023358721286058426\n",
      "Epoch:3 \n",
      "Iteration:120 \n",
      "Loss:0.043391358107328415\n",
      "Epoch:3 \n",
      "Iteration:121 \n",
      "Loss:0.006980248261243105\n",
      "Epoch:3 \n",
      "Iteration:122 \n",
      "Loss:0.03172539547085762\n",
      "Epoch:3 \n",
      "Iteration:123 \n",
      "Loss:0.025352146476507187\n",
      "Epoch:3 \n",
      "Iteration:124 \n",
      "Loss:0.10236527770757675\n",
      "Epoch:3 \n",
      "Iteration:125 \n",
      "Loss:0.03848658502101898\n",
      "Epoch:3 \n",
      "Iteration:126 \n",
      "Loss:0.118221215903759\n",
      "Epoch:3 \n",
      "Iteration:127 \n",
      "Loss:0.06300851702690125\n",
      "Epoch:3 \n",
      "Iteration:128 \n",
      "Loss:0.05326468497514725\n",
      "Epoch:3 \n",
      "Iteration:129 \n",
      "Loss:0.040979254990816116\n",
      "Epoch:3 \n",
      "Iteration:130 \n",
      "Loss:0.065855473279953\n",
      "Epoch:3 \n",
      "Iteration:131 \n",
      "Loss:0.1450963169336319\n",
      "Epoch:3 \n",
      "Iteration:132 \n",
      "Loss:0.06824100762605667\n",
      "Epoch:3 \n",
      "Iteration:133 \n",
      "Loss:0.01313074491918087\n",
      "Epoch:3 \n",
      "Iteration:134 \n",
      "Loss:0.02532646618783474\n",
      "Epoch:3 \n",
      "Iteration:135 \n",
      "Loss:0.011330613866448402\n",
      "Epoch:3 \n",
      "Iteration:136 \n",
      "Loss:0.12227289378643036\n",
      "Epoch:3 \n",
      "Iteration:137 \n",
      "Loss:0.042571473866701126\n",
      "Epoch:3 \n",
      "Iteration:138 \n",
      "Loss:0.05427588149905205\n",
      "Epoch:3 \n",
      "Iteration:139 \n",
      "Loss:0.08714954555034637\n",
      "Epoch:3 \n",
      "Iteration:140 \n",
      "Loss:0.19018897414207458\n",
      "Epoch:3 \n",
      "Iteration:141 \n",
      "Loss:0.033999938517808914\n",
      "Epoch:3 \n",
      "Iteration:142 \n",
      "Loss:0.07070242613554001\n",
      "Epoch:3 \n",
      "Iteration:143 \n",
      "Loss:0.05443559214472771\n",
      "Epoch:3 \n",
      "Iteration:144 \n",
      "Loss:0.011954161338508129\n",
      "Epoch:3 \n",
      "Iteration:145 \n",
      "Loss:0.043753813952207565\n",
      "Epoch:3 \n",
      "Iteration:146 \n",
      "Loss:0.036113590002059937\n",
      "Epoch:3 \n",
      "Iteration:147 \n",
      "Loss:0.03572002425789833\n",
      "Epoch:3 \n",
      "Iteration:148 \n",
      "Loss:0.020305495709180832\n",
      "Epoch:3 \n",
      "Iteration:149 \n",
      "Loss:0.13291138410568237\n",
      "Epoch:3 \n",
      "Iteration:150 \n",
      "Loss:0.05774228647351265\n",
      "Epoch:3 \n",
      "Iteration:151 \n",
      "Loss:0.09012045711278915\n",
      "Epoch:3 \n",
      "Iteration:152 \n",
      "Loss:0.07675786316394806\n",
      "Epoch:3 \n",
      "Iteration:153 \n",
      "Loss:0.027684856206178665\n",
      "Epoch:3 \n",
      "Iteration:154 \n",
      "Loss:0.047280844300985336\n",
      "Epoch:3 \n",
      "Iteration:155 \n",
      "Loss:0.03487660735845566\n",
      "Epoch:3 \n",
      "Iteration:156 \n",
      "Loss:0.03292856365442276\n",
      "Epoch:3 \n",
      "Iteration:157 \n",
      "Loss:0.05858949199318886\n",
      "Epoch:3 \n",
      "Iteration:158 \n",
      "Loss:0.08673455566167831\n",
      "Epoch:3 \n",
      "Iteration:159 \n",
      "Loss:0.03973165899515152\n",
      "Epoch:3 \n",
      "Iteration:160 \n",
      "Loss:0.1479998528957367\n",
      "Epoch:3 \n",
      "Iteration:161 \n",
      "Loss:0.04793471470475197\n",
      "Epoch:3 \n",
      "Iteration:162 \n",
      "Loss:0.025470953434705734\n",
      "Epoch:3 \n",
      "Iteration:163 \n",
      "Loss:0.06881735473871231\n",
      "Epoch:3 \n",
      "Iteration:164 \n",
      "Loss:0.017351467162370682\n",
      "Epoch:3 \n",
      "Iteration:165 \n",
      "Loss:0.09779451042413712\n",
      "Epoch:3 \n",
      "Iteration:166 \n",
      "Loss:0.040784209966659546\n",
      "Epoch:3 \n",
      "Iteration:167 \n",
      "Loss:0.07346498966217041\n",
      "Epoch:3 \n",
      "Iteration:168 \n",
      "Loss:0.021280502900481224\n",
      "Epoch:3 \n",
      "Iteration:169 \n",
      "Loss:0.07501194626092911\n",
      "Epoch:3 \n",
      "Iteration:170 \n",
      "Loss:0.0683286115527153\n",
      "Epoch:3 \n",
      "Iteration:171 \n",
      "Loss:0.046312157064676285\n",
      "Epoch:3 \n",
      "Iteration:172 \n",
      "Loss:0.013812290504574776\n",
      "Epoch:3 \n",
      "Iteration:173 \n",
      "Loss:0.08569474518299103\n",
      "Epoch:3 \n",
      "Iteration:174 \n",
      "Loss:0.08229739964008331\n",
      "Epoch:3 \n",
      "Iteration:175 \n",
      "Loss:0.09575134515762329\n",
      "Epoch:3 \n",
      "Iteration:176 \n",
      "Loss:0.05939517542719841\n",
      "Epoch:3 \n",
      "Iteration:177 \n",
      "Loss:0.04195753112435341\n",
      "Epoch:3 \n",
      "Iteration:178 \n",
      "Loss:0.07900586724281311\n",
      "Epoch:3 \n",
      "Iteration:179 \n",
      "Loss:0.11260567605495453\n",
      "Epoch:3 \n",
      "Iteration:180 \n",
      "Loss:0.04570407047867775\n",
      "Epoch:3 \n",
      "Iteration:181 \n",
      "Loss:0.11269282549619675\n",
      "Epoch:3 \n",
      "Iteration:182 \n",
      "Loss:0.046963516622781754\n",
      "Epoch:3 \n",
      "Iteration:183 \n",
      "Loss:0.053563155233860016\n",
      "Epoch:3 \n",
      "Iteration:184 \n",
      "Loss:0.14870327711105347\n",
      "Epoch:3 \n",
      "Iteration:185 \n",
      "Loss:0.022914163768291473\n",
      "Epoch:3 \n",
      "Iteration:186 \n",
      "Loss:0.049962759017944336\n",
      "Epoch:3 \n",
      "Iteration:187 \n",
      "Loss:0.0712253674864769\n",
      "Epoch:3 \n",
      "Iteration:188 \n",
      "Loss:0.1353616863489151\n",
      "Epoch:3 \n",
      "Iteration:189 \n",
      "Loss:0.043921440839767456\n",
      "Epoch:3 \n",
      "Iteration:190 \n",
      "Loss:0.09061047434806824\n",
      "Epoch:3 \n",
      "Iteration:191 \n",
      "Loss:0.22199466824531555\n",
      "Epoch:3 \n",
      "Iteration:192 \n",
      "Loss:0.07586213201284409\n",
      "Epoch:3 \n",
      "Iteration:193 \n",
      "Loss:0.0725313276052475\n",
      "Epoch:3 \n",
      "Iteration:194 \n",
      "Loss:0.047340258955955505\n",
      "Epoch:3 \n",
      "Iteration:195 \n",
      "Loss:0.10653276741504669\n",
      "Epoch:3 \n",
      "Iteration:196 \n",
      "Loss:0.09254342317581177\n",
      "Epoch:3 \n",
      "Iteration:197 \n",
      "Loss:0.0783831924200058\n",
      "Epoch:3 \n",
      "Iteration:198 \n",
      "Loss:0.0283550713211298\n",
      "Epoch:3 \n",
      "Iteration:199 \n",
      "Loss:0.029116563498973846\n",
      "Epoch:3 \n",
      "Iteration:200 \n",
      "Loss:0.0898512527346611\n",
      "Epoch:3 \n",
      "Iteration:201 \n",
      "Loss:0.07754413038492203\n",
      "Epoch:3 \n",
      "Iteration:202 \n",
      "Loss:0.27233198285102844\n",
      "Epoch:3 \n",
      "Iteration:203 \n",
      "Loss:0.018377184867858887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3 \n",
      "Iteration:204 \n",
      "Loss:0.04528307542204857\n",
      "Epoch:3 \n",
      "Iteration:205 \n",
      "Loss:0.02606377750635147\n",
      "Epoch:3 \n",
      "Iteration:206 \n",
      "Loss:0.0827031210064888\n",
      "Epoch:3 \n",
      "Iteration:207 \n",
      "Loss:0.08707594126462936\n",
      "Epoch:3 \n",
      "Iteration:208 \n",
      "Loss:0.10203990340232849\n",
      "Epoch:3 \n",
      "Iteration:209 \n",
      "Loss:0.05694695934653282\n",
      "Epoch:3 \n",
      "Iteration:210 \n",
      "Loss:0.0983322337269783\n",
      "Epoch:3 \n",
      "Iteration:211 \n",
      "Loss:0.0763617530465126\n",
      "Epoch:3 \n",
      "Iteration:212 \n",
      "Loss:0.04667603597044945\n",
      "Epoch:3 \n",
      "Iteration:213 \n",
      "Loss:0.12779290974140167\n",
      "Epoch:3 \n",
      "Iteration:214 \n",
      "Loss:0.09448330104351044\n",
      "Epoch:3 \n",
      "Iteration:215 \n",
      "Loss:0.013670090585947037\n",
      "Epoch:3 \n",
      "Iteration:216 \n",
      "Loss:0.021754853427410126\n",
      "Epoch:3 \n",
      "Iteration:217 \n",
      "Loss:0.18197712302207947\n",
      "Epoch:3 \n",
      "Iteration:218 \n",
      "Loss:0.10635692626237869\n",
      "Epoch:3 \n",
      "Iteration:219 \n",
      "Loss:0.06416255235671997\n",
      "Epoch:3 \n",
      "Iteration:220 \n",
      "Loss:0.06289878487586975\n",
      "Epoch:3 \n",
      "Iteration:221 \n",
      "Loss:0.046780314296483994\n",
      "Epoch:3 \n",
      "Iteration:222 \n",
      "Loss:0.17944437265396118\n",
      "Epoch:3 \n",
      "Iteration:223 \n",
      "Loss:0.04472976177930832\n",
      "Epoch:3 \n",
      "Iteration:224 \n",
      "Loss:0.024951009079813957\n",
      "Epoch:3 \n",
      "Iteration:225 \n",
      "Loss:0.05295839160680771\n",
      "Epoch:3 \n",
      "Iteration:226 \n",
      "Loss:0.07977861166000366\n",
      "Epoch:3 \n",
      "Iteration:227 \n",
      "Loss:0.051057200878858566\n",
      "Epoch:3 \n",
      "Iteration:228 \n",
      "Loss:0.06329610198736191\n",
      "Epoch:3 \n",
      "Iteration:229 \n",
      "Loss:0.032405558973550797\n",
      "Epoch:3 \n",
      "Iteration:230 \n",
      "Loss:0.03913480043411255\n",
      "Epoch:3 \n",
      "Iteration:231 \n",
      "Loss:0.05912960693240166\n",
      "Epoch:3 \n",
      "Iteration:232 \n",
      "Loss:0.04702155664563179\n",
      "Epoch:3 \n",
      "Iteration:233 \n",
      "Loss:0.028853679075837135\n",
      "Epoch:3 \n",
      "Iteration:234 \n",
      "Loss:0.0560724213719368\n",
      "Epoch:3 \n",
      "Iteration:235 \n",
      "Loss:0.08401910960674286\n",
      "Epoch:3 \n",
      "Iteration:236 \n",
      "Loss:0.07216251641511917\n",
      "Epoch:3 \n",
      "Iteration:237 \n",
      "Loss:0.06714190542697906\n",
      "Epoch:3 \n",
      "Iteration:238 \n",
      "Loss:0.018573889508843422\n",
      "Epoch:3 \n",
      "Iteration:239 \n",
      "Loss:0.027786126360297203\n",
      "Epoch:3 \n",
      "Iteration:240 \n",
      "Loss:0.06321242451667786\n",
      "Epoch:3 \n",
      "Iteration:241 \n",
      "Loss:0.07703101634979248\n",
      "Epoch:3 \n",
      "Iteration:242 \n",
      "Loss:0.02014216221868992\n",
      "Epoch:3 \n",
      "Iteration:243 \n",
      "Loss:0.12333828210830688\n",
      "Epoch:3 \n",
      "Iteration:244 \n",
      "Loss:0.08558963984251022\n",
      "Epoch:3 \n",
      "Iteration:245 \n",
      "Loss:0.06808220595121384\n",
      "Epoch:3 \n",
      "Iteration:246 \n",
      "Loss:0.07930892705917358\n",
      "Epoch:3 \n",
      "Iteration:247 \n",
      "Loss:0.04545634239912033\n",
      "Epoch:3 \n",
      "Iteration:248 \n",
      "Loss:0.027262378484010696\n",
      "Epoch:3 \n",
      "Iteration:249 \n",
      "Loss:0.005780881270766258\n",
      "Epoch:3 \n",
      "Iteration:250 \n",
      "Loss:0.0859537348151207\n",
      "Epoch:3 \n",
      "Iteration:251 \n",
      "Loss:0.1368052363395691\n",
      "Epoch:3 \n",
      "Iteration:252 \n",
      "Loss:0.05443878471851349\n",
      "Epoch:3 \n",
      "Iteration:253 \n",
      "Loss:0.04661558195948601\n",
      "Epoch:3 \n",
      "Iteration:254 \n",
      "Loss:0.012230098247528076\n",
      "Epoch:3 \n",
      "Iteration:255 \n",
      "Loss:0.05395623669028282\n",
      "Epoch:3 \n",
      "Iteration:256 \n",
      "Loss:0.04030292108654976\n",
      "Epoch:3 \n",
      "Iteration:257 \n",
      "Loss:0.13883548974990845\n",
      "Epoch:3 \n",
      "Iteration:258 \n",
      "Loss:0.041905637830495834\n",
      "Epoch:3 \n",
      "Iteration:259 \n",
      "Loss:0.04805561527609825\n",
      "Epoch:3 \n",
      "Iteration:260 \n",
      "Loss:0.005061304196715355\n",
      "Epoch:3 \n",
      "Iteration:261 \n",
      "Loss:0.01223828550428152\n",
      "Epoch:3 \n",
      "Iteration:262 \n",
      "Loss:0.06648411601781845\n",
      "Epoch:3 \n",
      "Iteration:263 \n",
      "Loss:0.06587139517068863\n",
      "Epoch:3 \n",
      "Iteration:264 \n",
      "Loss:0.04736965522170067\n",
      "Epoch:3 \n",
      "Iteration:265 \n",
      "Loss:0.18263986706733704\n",
      "Epoch:3 \n",
      "Iteration:266 \n",
      "Loss:0.08784440904855728\n",
      "Epoch:3 \n",
      "Iteration:267 \n",
      "Loss:0.10307471454143524\n",
      "Epoch:3 \n",
      "Iteration:268 \n",
      "Loss:0.023594481870532036\n",
      "Epoch:3 \n",
      "Iteration:269 \n",
      "Loss:0.07121305912733078\n",
      "Epoch:3 \n",
      "Iteration:270 \n",
      "Loss:0.11043193936347961\n",
      "Epoch:3 \n",
      "Iteration:271 \n",
      "Loss:0.0723830983042717\n",
      "Epoch:3 \n",
      "Iteration:272 \n",
      "Loss:0.035881590098142624\n",
      "Epoch:3 \n",
      "Iteration:273 \n",
      "Loss:0.05593635141849518\n",
      "Epoch:3 \n",
      "Iteration:274 \n",
      "Loss:0.14742034673690796\n",
      "Epoch:3 \n",
      "Iteration:275 \n",
      "Loss:0.107148677110672\n",
      "Epoch:3 \n",
      "Iteration:276 \n",
      "Loss:0.02263864129781723\n",
      "Epoch:3 \n",
      "Iteration:277 \n",
      "Loss:0.15307112038135529\n",
      "Epoch:3 \n",
      "Iteration:278 \n",
      "Loss:0.04787875711917877\n",
      "Epoch:3 \n",
      "Iteration:279 \n",
      "Loss:0.01562538929283619\n",
      "Epoch:3 \n",
      "Iteration:280 \n",
      "Loss:0.05024971812963486\n",
      "Epoch:3 \n",
      "Iteration:281 \n",
      "Loss:0.09709260612726212\n",
      "Epoch:3 \n",
      "Iteration:282 \n",
      "Loss:0.11517886817455292\n",
      "Epoch:3 \n",
      "Iteration:283 \n",
      "Loss:0.02666594460606575\n",
      "Epoch:3 \n",
      "Iteration:284 \n",
      "Loss:0.10835277289152145\n",
      "Epoch:3 \n",
      "Iteration:285 \n",
      "Loss:0.06984353065490723\n",
      "Epoch:3 \n",
      "Iteration:286 \n",
      "Loss:0.0071766371838748455\n",
      "Epoch:3 \n",
      "Iteration:287 \n",
      "Loss:0.175770103931427\n",
      "Epoch:3 \n",
      "Iteration:288 \n",
      "Loss:0.17256784439086914\n",
      "Epoch:3 \n",
      "Iteration:289 \n",
      "Loss:0.027325186878442764\n",
      "Epoch:3 \n",
      "Iteration:290 \n",
      "Loss:0.017134616151452065\n",
      "Epoch:3 \n",
      "Iteration:291 \n",
      "Loss:0.10804054141044617\n",
      "Epoch:3 \n",
      "Iteration:292 \n",
      "Loss:0.03294110670685768\n",
      "Epoch:3 \n",
      "Iteration:293 \n",
      "Loss:0.036379944533109665\n",
      "Epoch:3 \n",
      "Iteration:294 \n",
      "Loss:0.04411547631025314\n",
      "Epoch:3 \n",
      "Iteration:295 \n",
      "Loss:0.07565080374479294\n",
      "Epoch:3 \n",
      "Iteration:296 \n",
      "Loss:0.0573832169175148\n",
      "Epoch:3 \n",
      "Iteration:297 \n",
      "Loss:0.08438429981470108\n",
      "Epoch:3 \n",
      "Iteration:298 \n",
      "Loss:0.13010452687740326\n",
      "Epoch:3 \n",
      "Iteration:299 \n",
      "Loss:0.03196795657277107\n",
      "Epoch:3 \n",
      "Iteration:300 \n",
      "Loss:0.01535466592758894\n",
      "Epoch:3 \n",
      "Iteration:301 \n",
      "Loss:0.06136434152722359\n",
      "Epoch:3 \n",
      "Iteration:302 \n",
      "Loss:0.023291394114494324\n",
      "Epoch:3 \n",
      "Iteration:303 \n",
      "Loss:0.0185555312782526\n",
      "Epoch:3 \n",
      "Iteration:304 \n",
      "Loss:0.11194606125354767\n",
      "Epoch:3 \n",
      "Iteration:305 \n",
      "Loss:0.04936112463474274\n",
      "Epoch:3 \n",
      "Iteration:306 \n",
      "Loss:0.05691419914364815\n",
      "Epoch:3 \n",
      "Iteration:307 \n",
      "Loss:0.06733754277229309\n",
      "Epoch:3 \n",
      "Iteration:308 \n",
      "Loss:0.0600847564637661\n",
      "Epoch:3 \n",
      "Iteration:309 \n",
      "Loss:0.128073051571846\n",
      "Epoch:3 \n",
      "Iteration:310 \n",
      "Loss:0.08597646653652191\n",
      "Epoch:3 \n",
      "Iteration:311 \n",
      "Loss:0.09011654555797577\n",
      "Epoch:3 \n",
      "Iteration:312 \n",
      "Loss:0.03729492798447609\n",
      "Epoch:3 \n",
      "Iteration:313 \n",
      "Loss:0.1447417289018631\n",
      "Epoch:3 \n",
      "Iteration:314 \n",
      "Loss:0.026292163878679276\n",
      "Epoch:3 \n",
      "Iteration:315 \n",
      "Loss:0.04945821315050125\n",
      "Epoch:3 \n",
      "Iteration:316 \n",
      "Loss:0.023042850196361542\n",
      "Epoch:3 \n",
      "Iteration:317 \n",
      "Loss:0.043212905526161194\n",
      "Epoch:3 \n",
      "Iteration:318 \n",
      "Loss:0.042006924748420715\n",
      "Epoch:3 \n",
      "Iteration:319 \n",
      "Loss:0.047286033630371094\n",
      "Epoch:3 \n",
      "Iteration:320 \n",
      "Loss:0.05363646149635315\n",
      "Epoch:3 \n",
      "Iteration:321 \n",
      "Loss:0.05747861787676811\n",
      "Epoch:3 \n",
      "Iteration:322 \n",
      "Loss:0.07507288455963135\n",
      "Epoch:3 \n",
      "Iteration:323 \n",
      "Loss:0.049577079713344574\n",
      "Epoch:3 \n",
      "Iteration:324 \n",
      "Loss:0.052529629319906235\n",
      "Epoch:3 \n",
      "Iteration:325 \n",
      "Loss:0.0602988675236702\n",
      "Epoch:3 \n",
      "Iteration:326 \n",
      "Loss:0.08123020082712173\n",
      "Epoch:3 \n",
      "Iteration:327 \n",
      "Loss:0.01547863706946373\n",
      "Epoch:3 \n",
      "Iteration:328 \n",
      "Loss:0.07707434892654419\n",
      "Epoch:3 \n",
      "Iteration:329 \n",
      "Loss:0.04051278531551361\n",
      "Epoch:3 \n",
      "Iteration:330 \n",
      "Loss:0.009196980856359005\n",
      "Epoch:3 \n",
      "Iteration:331 \n",
      "Loss:0.0819530040025711\n",
      "Epoch:3 \n",
      "Iteration:332 \n",
      "Loss:0.20804451406002045\n",
      "Epoch:3 \n",
      "Iteration:333 \n",
      "Loss:0.015630604699254036\n",
      "Epoch:3 \n",
      "Iteration:334 \n",
      "Loss:0.047279275953769684\n",
      "Epoch:3 \n",
      "Iteration:335 \n",
      "Loss:0.015820754691958427\n",
      "Epoch:3 \n",
      "Iteration:336 \n",
      "Loss:0.030621837824583054\n",
      "Epoch:3 \n",
      "Iteration:337 \n",
      "Loss:0.04196449741721153\n",
      "Epoch:3 \n",
      "Iteration:338 \n",
      "Loss:0.26463621854782104\n",
      "Epoch:3 \n",
      "Iteration:339 \n",
      "Loss:0.027494564652442932\n",
      "Epoch:3 \n",
      "Iteration:340 \n",
      "Loss:0.026664409786462784\n",
      "Epoch:3 \n",
      "Iteration:341 \n",
      "Loss:0.021668260917067528\n",
      "Epoch:3 \n",
      "Iteration:342 \n",
      "Loss:0.036023177206516266\n",
      "Epoch:3 \n",
      "Iteration:343 \n",
      "Loss:0.08685459196567535\n",
      "Epoch:3 \n",
      "Iteration:344 \n",
      "Loss:0.03214238956570625\n",
      "Epoch:3 \n",
      "Iteration:345 \n",
      "Loss:0.15978045761585236\n",
      "Epoch:3 \n",
      "Iteration:346 \n",
      "Loss:0.1079966351389885\n",
      "Epoch:3 \n",
      "Iteration:347 \n",
      "Loss:0.016815856099128723\n",
      "Epoch:3 \n",
      "Iteration:348 \n",
      "Loss:0.02298133075237274\n",
      "Epoch:3 \n",
      "Iteration:349 \n",
      "Loss:0.04000164568424225\n",
      "Epoch:3 \n",
      "Iteration:350 \n",
      "Loss:0.028447626158595085\n",
      "Epoch:3 \n",
      "Iteration:351 \n",
      "Loss:0.021516937762498856\n",
      "Epoch:3 \n",
      "Iteration:352 \n",
      "Loss:0.07417622953653336\n",
      "Epoch:3 \n",
      "Iteration:353 \n",
      "Loss:0.06155497953295708\n",
      "Epoch:3 \n",
      "Iteration:354 \n",
      "Loss:0.023866722360253334\n",
      "Epoch:3 \n",
      "Iteration:355 \n",
      "Loss:0.1153397485613823\n",
      "Epoch:3 \n",
      "Iteration:356 \n",
      "Loss:0.06120302155613899\n",
      "Epoch:3 \n",
      "Iteration:357 \n",
      "Loss:0.02457522414624691\n",
      "Epoch:3 \n",
      "Iteration:358 \n",
      "Loss:0.05458023026585579\n",
      "Epoch:3 \n",
      "Iteration:359 \n",
      "Loss:0.08095678687095642\n",
      "Epoch:3 \n",
      "Iteration:360 \n",
      "Loss:0.045584581792354584\n",
      "Epoch:3 \n",
      "Iteration:361 \n",
      "Loss:0.026411592960357666\n",
      "Epoch:3 \n",
      "Iteration:362 \n",
      "Loss:0.026539266109466553\n",
      "Epoch:3 \n",
      "Iteration:363 \n",
      "Loss:0.050297483801841736\n",
      "Epoch:3 \n",
      "Iteration:364 \n",
      "Loss:0.016372786834836006\n",
      "Epoch:3 \n",
      "Iteration:365 \n",
      "Loss:0.027314424514770508\n",
      "Epoch:3 \n",
      "Iteration:366 \n",
      "Loss:0.037081822752952576\n",
      "Epoch:3 \n",
      "Iteration:367 \n",
      "Loss:0.15916821360588074\n",
      "Epoch:3 \n",
      "Iteration:368 \n",
      "Loss:0.060564830899238586\n",
      "Epoch:3 \n",
      "Iteration:369 \n",
      "Loss:0.1259833574295044\n",
      "Epoch:3 \n",
      "Iteration:370 \n",
      "Loss:0.10481490939855576\n",
      "Epoch:3 \n",
      "Iteration:371 \n",
      "Loss:0.058637332171201706\n",
      "Epoch:3 \n",
      "Iteration:372 \n",
      "Loss:0.07531488686800003\n",
      "Epoch:3 \n",
      "Iteration:373 \n",
      "Loss:0.19107292592525482\n",
      "Epoch:3 \n",
      "Iteration:374 \n",
      "Loss:0.1496812105178833\n",
      "Epoch:3 \n",
      "Iteration:375 \n",
      "Loss:0.213098406791687\n",
      "Epoch:3 \n",
      "Iteration:376 \n",
      "Loss:0.06078518554568291\n",
      "Epoch:3 \n",
      "Iteration:377 \n",
      "Loss:0.04792303591966629\n",
      "Epoch:3 \n",
      "Iteration:378 \n",
      "Loss:0.12813062965869904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3 \n",
      "Iteration:379 \n",
      "Loss:0.08556623756885529\n",
      "Epoch:3 \n",
      "Iteration:380 \n",
      "Loss:0.024678921326994896\n",
      "Epoch:3 \n",
      "Iteration:381 \n",
      "Loss:0.0942143127322197\n",
      "Epoch:3 \n",
      "Iteration:382 \n",
      "Loss:0.04351355507969856\n",
      "Epoch:3 \n",
      "Iteration:383 \n",
      "Loss:0.04660025238990784\n",
      "Epoch:3 \n",
      "Iteration:384 \n",
      "Loss:0.06373007595539093\n",
      "Epoch:3 \n",
      "Iteration:385 \n",
      "Loss:0.05564454570412636\n",
      "Epoch:3 \n",
      "Iteration:386 \n",
      "Loss:0.032816123217344284\n",
      "Epoch:3 \n",
      "Iteration:387 \n",
      "Loss:0.08133237808942795\n",
      "Epoch:3 \n",
      "Iteration:388 \n",
      "Loss:0.07473548501729965\n",
      "Epoch:3 \n",
      "Iteration:389 \n",
      "Loss:0.08674032241106033\n",
      "Epoch:3 \n",
      "Iteration:390 \n",
      "Loss:0.03935132920742035\n",
      "Epoch:3 \n",
      "Iteration:391 \n",
      "Loss:0.12914147973060608\n",
      "Epoch:3 \n",
      "Iteration:392 \n",
      "Loss:0.0740227997303009\n",
      "Epoch:3 \n",
      "Iteration:393 \n",
      "Loss:0.07169558852910995\n",
      "Epoch:3 \n",
      "Iteration:394 \n",
      "Loss:0.07901866734027863\n",
      "Epoch:3 \n",
      "Iteration:395 \n",
      "Loss:0.012888790108263493\n",
      "Epoch:3 \n",
      "Iteration:396 \n",
      "Loss:0.023101985454559326\n",
      "Epoch:3 \n",
      "Iteration:397 \n",
      "Loss:0.027346855029463768\n",
      "Epoch:3 \n",
      "Iteration:398 \n",
      "Loss:0.050713587552309036\n",
      "Epoch:3 \n",
      "Iteration:399 \n",
      "Loss:0.13524292409420013\n",
      "Epoch:3 \n",
      "Iteration:400 \n",
      "Loss:0.009454277344048023\n",
      "Epoch:3 \n",
      "Iteration:401 \n",
      "Loss:0.0868653953075409\n",
      "Epoch:3 \n",
      "Iteration:402 \n",
      "Loss:0.09657636284828186\n",
      "Epoch:3 \n",
      "Iteration:403 \n",
      "Loss:0.06118474155664444\n",
      "Epoch:3 \n",
      "Iteration:404 \n",
      "Loss:0.007587381638586521\n",
      "Epoch:3 \n",
      "Iteration:405 \n",
      "Loss:0.07533643394708633\n",
      "Epoch:3 \n",
      "Iteration:406 \n",
      "Loss:0.0638110414147377\n",
      "Epoch:3 \n",
      "Iteration:407 \n",
      "Loss:0.09557557851076126\n",
      "Epoch:3 \n",
      "Iteration:408 \n",
      "Loss:0.18209640681743622\n",
      "Epoch:3 \n",
      "Iteration:409 \n",
      "Loss:0.03930702060461044\n",
      "Epoch:3 \n",
      "Iteration:410 \n",
      "Loss:0.05902566760778427\n",
      "Epoch:3 \n",
      "Iteration:411 \n",
      "Loss:0.030768513679504395\n",
      "Epoch:3 \n",
      "Iteration:412 \n",
      "Loss:0.03843061625957489\n",
      "Epoch:3 \n",
      "Iteration:413 \n",
      "Loss:0.028910653665661812\n",
      "Epoch:3 \n",
      "Iteration:414 \n",
      "Loss:0.05996627360582352\n",
      "Epoch:3 \n",
      "Iteration:415 \n",
      "Loss:0.02921878732740879\n",
      "Epoch:3 \n",
      "Iteration:416 \n",
      "Loss:0.03671879693865776\n",
      "Epoch:3 \n",
      "Iteration:417 \n",
      "Loss:0.033204495906829834\n",
      "Epoch:3 \n",
      "Iteration:418 \n",
      "Loss:0.07315924763679504\n",
      "Epoch:3 \n",
      "Iteration:419 \n",
      "Loss:0.12551261484622955\n",
      "Epoch:3 \n",
      "Iteration:420 \n",
      "Loss:0.029948584735393524\n",
      "Epoch:3 \n",
      "Iteration:421 \n",
      "Loss:0.1145990863442421\n",
      "Epoch:3 \n",
      "Iteration:422 \n",
      "Loss:0.12129977345466614\n",
      "Epoch:3 \n",
      "Iteration:423 \n",
      "Loss:0.11679253727197647\n",
      "Epoch:3 \n",
      "Iteration:424 \n",
      "Loss:0.06504392623901367\n",
      "Epoch:3 \n",
      "Iteration:425 \n",
      "Loss:0.11825316399335861\n",
      "Epoch:3 \n",
      "Iteration:426 \n",
      "Loss:0.021926239132881165\n",
      "Epoch:3 \n",
      "Iteration:427 \n",
      "Loss:0.03096041828393936\n",
      "Epoch:3 \n",
      "Iteration:428 \n",
      "Loss:0.0646766647696495\n",
      "Epoch:3 \n",
      "Iteration:429 \n",
      "Loss:0.08733537048101425\n",
      "Epoch:3 \n",
      "Iteration:430 \n",
      "Loss:0.014815184287726879\n",
      "Epoch:3 \n",
      "Iteration:431 \n",
      "Loss:0.05173107236623764\n",
      "Epoch:3 \n",
      "Iteration:432 \n",
      "Loss:0.00603444641456008\n",
      "Epoch:3 \n",
      "Iteration:433 \n",
      "Loss:0.016772666946053505\n",
      "Epoch:3 \n",
      "Iteration:434 \n",
      "Loss:0.03908924758434296\n",
      "Epoch:3 \n",
      "Iteration:435 \n",
      "Loss:0.03855489566922188\n",
      "Epoch:3 \n",
      "Iteration:436 \n",
      "Loss:0.03819161280989647\n",
      "Epoch:3 \n",
      "Iteration:437 \n",
      "Loss:0.032628774642944336\n",
      "Epoch:3 \n",
      "Iteration:438 \n",
      "Loss:0.036820754408836365\n",
      "Epoch:3 \n",
      "Iteration:439 \n",
      "Loss:0.04800525680184364\n",
      "Epoch:3 \n",
      "Iteration:440 \n",
      "Loss:0.033686865121126175\n",
      "Epoch:3 \n",
      "Iteration:441 \n",
      "Loss:0.0831451416015625\n",
      "Epoch:3 \n",
      "Iteration:442 \n",
      "Loss:0.059462033212184906\n",
      "Epoch:3 \n",
      "Iteration:443 \n",
      "Loss:0.14983554184436798\n",
      "Epoch:3 \n",
      "Iteration:444 \n",
      "Loss:0.012346874922513962\n",
      "Epoch:3 \n",
      "Iteration:445 \n",
      "Loss:0.014506292529404163\n",
      "Epoch:3 \n",
      "Iteration:446 \n",
      "Loss:0.12367135286331177\n",
      "Epoch:3 \n",
      "Iteration:447 \n",
      "Loss:0.017795918509364128\n",
      "Epoch:3 \n",
      "Iteration:448 \n",
      "Loss:0.055968575179576874\n",
      "Epoch:3 \n",
      "Iteration:449 \n",
      "Loss:0.04598839208483696\n",
      "Epoch:3 \n",
      "Iteration:450 \n",
      "Loss:0.015113008208572865\n",
      "Epoch:3 \n",
      "Iteration:451 \n",
      "Loss:0.014321736991405487\n",
      "Epoch:3 \n",
      "Iteration:452 \n",
      "Loss:0.07545002549886703\n",
      "Epoch:3 \n",
      "Iteration:453 \n",
      "Loss:0.019356178119778633\n",
      "Epoch:3 \n",
      "Iteration:454 \n",
      "Loss:0.11621472239494324\n",
      "Epoch:3 \n",
      "Iteration:455 \n",
      "Loss:0.19944007694721222\n",
      "Epoch:3 \n",
      "Iteration:456 \n",
      "Loss:0.04061676561832428\n",
      "Epoch:3 \n",
      "Iteration:457 \n",
      "Loss:0.044181112200021744\n",
      "Epoch:3 \n",
      "Iteration:458 \n",
      "Loss:0.05118435248732567\n",
      "Epoch:3 \n",
      "Iteration:459 \n",
      "Loss:0.12524893879890442\n",
      "Epoch:3 \n",
      "Iteration:460 \n",
      "Loss:0.09160235524177551\n",
      "Epoch:3 \n",
      "Iteration:461 \n",
      "Loss:0.08854662626981735\n",
      "Epoch:3 \n",
      "Iteration:462 \n",
      "Loss:0.018272338435053825\n",
      "Epoch:3 \n",
      "Iteration:463 \n",
      "Loss:0.079646036028862\n",
      "Epoch:3 \n",
      "Iteration:464 \n",
      "Loss:0.017677420750260353\n",
      "Epoch:3 \n",
      "Iteration:465 \n",
      "Loss:0.10727082937955856\n",
      "Epoch:3 \n",
      "Iteration:466 \n",
      "Loss:0.15684157609939575\n",
      "Epoch:3 \n",
      "Iteration:467 \n",
      "Loss:0.07077421993017197\n",
      "Epoch:3 \n",
      "Iteration:468 \n",
      "Loss:0.02392643690109253\n",
      "Epoch:3 \n",
      "Iteration:469 \n",
      "Loss:0.028030812740325928\n",
      "Epoch:3 \n",
      "Iteration:470 \n",
      "Loss:0.023524584248661995\n",
      "Epoch:3 \n",
      "Iteration:471 \n",
      "Loss:0.05504795163869858\n",
      "Epoch:3 \n",
      "Iteration:472 \n",
      "Loss:0.041099730879068375\n",
      "Epoch:3 \n",
      "Iteration:473 \n",
      "Loss:0.05134458839893341\n",
      "Epoch:3 \n",
      "Iteration:474 \n",
      "Loss:0.11096357554197311\n",
      "Epoch:3 \n",
      "Iteration:475 \n",
      "Loss:0.04499058425426483\n",
      "Epoch:3 \n",
      "Iteration:476 \n",
      "Loss:0.036424990743398666\n",
      "Epoch:3 \n",
      "Iteration:477 \n",
      "Loss:0.04963929206132889\n",
      "Epoch:3 \n",
      "Iteration:478 \n",
      "Loss:0.07822341471910477\n",
      "Epoch:3 \n",
      "Iteration:479 \n",
      "Loss:0.038502320647239685\n",
      "Epoch:3 \n",
      "Iteration:480 \n",
      "Loss:0.05495280399918556\n",
      "Epoch:3 \n",
      "Iteration:481 \n",
      "Loss:0.05292540416121483\n",
      "Epoch:3 \n",
      "Iteration:482 \n",
      "Loss:0.08286896347999573\n",
      "Epoch:3 \n",
      "Iteration:483 \n",
      "Loss:0.06266821175813675\n",
      "Epoch:3 \n",
      "Iteration:484 \n",
      "Loss:0.1510794460773468\n",
      "Epoch:3 \n",
      "Iteration:485 \n",
      "Loss:0.02668604627251625\n",
      "Epoch:3 \n",
      "Iteration:486 \n",
      "Loss:0.01735815592110157\n",
      "Epoch:3 \n",
      "Iteration:487 \n",
      "Loss:0.012418835423886776\n",
      "Epoch:3 \n",
      "Iteration:488 \n",
      "Loss:0.026516320183873177\n",
      "Epoch:3 \n",
      "Iteration:489 \n",
      "Loss:0.04831799492239952\n",
      "Epoch:3 \n",
      "Iteration:490 \n",
      "Loss:0.024625005200505257\n",
      "Epoch:3 \n",
      "Iteration:491 \n",
      "Loss:0.031595949083566666\n",
      "Epoch:3 \n",
      "Iteration:492 \n",
      "Loss:0.062487564980983734\n",
      "Epoch:3 \n",
      "Iteration:493 \n",
      "Loss:0.0430024228990078\n",
      "Epoch:3 \n",
      "Iteration:494 \n",
      "Loss:0.09127211570739746\n",
      "Epoch:3 \n",
      "Iteration:495 \n",
      "Loss:0.09712136536836624\n",
      "Epoch:3 \n",
      "Iteration:496 \n",
      "Loss:0.07134747505187988\n",
      "Epoch:3 \n",
      "Iteration:497 \n",
      "Loss:0.121104896068573\n",
      "Epoch:3 \n",
      "Iteration:498 \n",
      "Loss:0.147923082113266\n",
      "Epoch:3 \n",
      "Iteration:499 \n",
      "Loss:0.023370489478111267\n",
      "Epoch:3 \n",
      "Iteration:500 \n",
      "Loss:0.1316874474287033\n",
      "Epoch:3 \n",
      "Iteration:501 \n",
      "Loss:0.031174367293715477\n",
      "Epoch:3 \n",
      "Iteration:502 \n",
      "Loss:0.054284367710351944\n",
      "Epoch:3 \n",
      "Iteration:503 \n",
      "Loss:0.09452662616968155\n",
      "Epoch:3 \n",
      "Iteration:504 \n",
      "Loss:0.006037610117346048\n",
      "Epoch:3 \n",
      "Iteration:505 \n",
      "Loss:0.07172221690416336\n",
      "Epoch:3 \n",
      "Iteration:506 \n",
      "Loss:0.1037885844707489\n",
      "Epoch:3 \n",
      "Iteration:507 \n",
      "Loss:0.03325257450342178\n",
      "Epoch:3 \n",
      "Iteration:508 \n",
      "Loss:0.03974100947380066\n",
      "Epoch:3 \n",
      "Iteration:509 \n",
      "Loss:0.055190183222293854\n",
      "Epoch:3 \n",
      "Iteration:510 \n",
      "Loss:0.12394395470619202\n",
      "Epoch:3 \n",
      "Iteration:511 \n",
      "Loss:0.017157621681690216\n",
      "Epoch:3 \n",
      "Iteration:512 \n",
      "Loss:0.08471744507551193\n",
      "Epoch:3 \n",
      "Iteration:513 \n",
      "Loss:0.19270697236061096\n",
      "Epoch:3 \n",
      "Iteration:514 \n",
      "Loss:0.00915892980992794\n",
      "Epoch:3 \n",
      "Iteration:515 \n",
      "Loss:0.11456885933876038\n",
      "Epoch:3 \n",
      "Iteration:516 \n",
      "Loss:0.05020492896437645\n",
      "Epoch:3 \n",
      "Iteration:517 \n",
      "Loss:0.12951892614364624\n",
      "Epoch:3 \n",
      "Iteration:518 \n",
      "Loss:0.09133592247962952\n",
      "Epoch:3 \n",
      "Iteration:519 \n",
      "Loss:0.06451079249382019\n",
      "Epoch:3 \n",
      "Iteration:520 \n",
      "Loss:0.10322973877191544\n",
      "Epoch:3 \n",
      "Iteration:521 \n",
      "Loss:0.09876592457294464\n",
      "Epoch:3 \n",
      "Iteration:522 \n",
      "Loss:0.071944959461689\n",
      "Epoch:3 \n",
      "Iteration:523 \n",
      "Loss:0.06504212319850922\n",
      "Epoch:3 \n",
      "Iteration:524 \n",
      "Loss:0.08795938640832901\n",
      "Epoch:3 \n",
      "Iteration:525 \n",
      "Loss:0.03814290463924408\n",
      "Epoch:3 \n",
      "Iteration:526 \n",
      "Loss:0.01877966709434986\n",
      "Epoch:3 \n",
      "Iteration:527 \n",
      "Loss:0.05336418002843857\n",
      "Epoch:3 \n",
      "Iteration:528 \n",
      "Loss:0.040779076516628265\n",
      "Epoch:3 \n",
      "Iteration:529 \n",
      "Loss:0.029231682419776917\n",
      "Epoch:3 \n",
      "Iteration:530 \n",
      "Loss:0.022919287905097008\n",
      "Epoch:3 \n",
      "Iteration:531 \n",
      "Loss:0.023215139284729958\n",
      "Epoch:3 \n",
      "Iteration:532 \n",
      "Loss:0.02789212204515934\n",
      "Epoch:3 \n",
      "Iteration:533 \n",
      "Loss:0.08830823749303818\n",
      "Epoch:3 \n",
      "Iteration:534 \n",
      "Loss:0.06086815893650055\n",
      "Epoch:3 \n",
      "Iteration:535 \n",
      "Loss:0.06807231158018112\n",
      "Epoch:3 \n",
      "Iteration:536 \n",
      "Loss:0.045101579278707504\n",
      "Epoch:3 \n",
      "Iteration:537 \n",
      "Loss:0.01850729063153267\n",
      "Epoch:3 \n",
      "Iteration:538 \n",
      "Loss:0.015373787842690945\n",
      "Epoch:3 \n",
      "Iteration:539 \n",
      "Loss:0.07212677597999573\n",
      "Epoch:3 \n",
      "Iteration:540 \n",
      "Loss:0.11486479640007019\n",
      "Epoch:3 \n",
      "Iteration:541 \n",
      "Loss:0.006568409036844969\n",
      "Epoch:3 \n",
      "Iteration:542 \n",
      "Loss:0.04779447987675667\n",
      "Epoch:3 \n",
      "Iteration:543 \n",
      "Loss:0.002478356007486582\n",
      "Epoch:3 \n",
      "Iteration:544 \n",
      "Loss:0.02181730978190899\n",
      "Epoch:3 \n",
      "Iteration:545 \n",
      "Loss:0.16042587161064148\n",
      "Epoch:3 \n",
      "Iteration:546 \n",
      "Loss:0.09131696075201035\n",
      "Epoch:3 \n",
      "Iteration:547 \n",
      "Loss:0.04490528255701065\n",
      "Epoch:3 \n",
      "Iteration:548 \n",
      "Loss:0.05089196562767029\n",
      "Epoch:3 \n",
      "Iteration:549 \n",
      "Loss:0.049557413905858994\n",
      "Epoch:3 \n",
      "Iteration:550 \n",
      "Loss:0.06745120882987976\n",
      "Epoch:3 \n",
      "Iteration:551 \n",
      "Loss:0.07445251196622849\n",
      "Epoch:3 \n",
      "Iteration:552 \n",
      "Loss:0.022643830627202988\n",
      "Epoch:3 \n",
      "Iteration:553 \n",
      "Loss:0.0636422410607338\n",
      "Epoch:3 \n",
      "Iteration:554 \n",
      "Loss:0.034051116555929184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3 \n",
      "Iteration:555 \n",
      "Loss:0.24185127019882202\n",
      "Epoch:3 \n",
      "Iteration:556 \n",
      "Loss:0.04561919346451759\n",
      "Epoch:3 \n",
      "Iteration:557 \n",
      "Loss:0.07687684148550034\n",
      "Epoch:3 \n",
      "Iteration:558 \n",
      "Loss:0.12885460257530212\n",
      "Epoch:3 \n",
      "Iteration:559 \n",
      "Loss:0.047977544367313385\n",
      "Epoch:3 \n",
      "Iteration:560 \n",
      "Loss:0.041476693004369736\n",
      "Epoch:3 \n",
      "Iteration:561 \n",
      "Loss:0.0528879277408123\n",
      "Epoch:3 \n",
      "Iteration:562 \n",
      "Loss:0.056906808167696\n",
      "Epoch:3 \n",
      "Iteration:563 \n",
      "Loss:0.04545257240533829\n",
      "Epoch:3 \n",
      "Iteration:564 \n",
      "Loss:0.14434482157230377\n",
      "Epoch:3 \n",
      "Iteration:565 \n",
      "Loss:0.09007858484983444\n",
      "Epoch:3 \n",
      "Iteration:566 \n",
      "Loss:0.059502530843019485\n",
      "Epoch:3 \n",
      "Iteration:567 \n",
      "Loss:0.04964425042271614\n",
      "Epoch:3 \n",
      "Iteration:568 \n",
      "Loss:0.1237768679857254\n",
      "Epoch:3 \n",
      "Iteration:569 \n",
      "Loss:0.08240116387605667\n",
      "Epoch:3 \n",
      "Iteration:570 \n",
      "Loss:0.06299039721488953\n",
      "Epoch:3 \n",
      "Iteration:571 \n",
      "Loss:0.07882066816091537\n",
      "Epoch:3 \n",
      "Iteration:572 \n",
      "Loss:0.09825509786605835\n",
      "Epoch:3 \n",
      "Iteration:573 \n",
      "Loss:0.08380954712629318\n",
      "Epoch:3 \n",
      "Iteration:574 \n",
      "Loss:0.10015127062797546\n",
      "Epoch:3 \n",
      "Iteration:575 \n",
      "Loss:0.051494285464286804\n",
      "Epoch:3 \n",
      "Iteration:576 \n",
      "Loss:0.10279891639947891\n",
      "Epoch:3 \n",
      "Iteration:577 \n",
      "Loss:0.09098298847675323\n",
      "Epoch:3 \n",
      "Iteration:578 \n",
      "Loss:0.0633343905210495\n",
      "Epoch:3 \n",
      "Iteration:579 \n",
      "Loss:0.08485736697912216\n",
      "Epoch:3 \n",
      "Iteration:580 \n",
      "Loss:0.08493006974458694\n",
      "Epoch:3 \n",
      "Iteration:581 \n",
      "Loss:0.03729868680238724\n",
      "Epoch:3 \n",
      "Iteration:582 \n",
      "Loss:0.1048179417848587\n",
      "Epoch:3 \n",
      "Iteration:583 \n",
      "Loss:0.10830719769001007\n",
      "Epoch:3 \n",
      "Iteration:584 \n",
      "Loss:0.01557110995054245\n",
      "Epoch:3 \n",
      "Iteration:585 \n",
      "Loss:0.018897496163845062\n",
      "Epoch:3 \n",
      "Iteration:586 \n",
      "Loss:0.10859076678752899\n",
      "Epoch:3 \n",
      "Iteration:587 \n",
      "Loss:0.061787303537130356\n",
      "Epoch:3 \n",
      "Iteration:588 \n",
      "Loss:0.0567367747426033\n",
      "Epoch:3 \n",
      "Iteration:589 \n",
      "Loss:0.02698596566915512\n",
      "Epoch:3 \n",
      "Iteration:590 \n",
      "Loss:0.05311235040426254\n",
      "Epoch:3 \n",
      "Iteration:591 \n",
      "Loss:0.05536394193768501\n",
      "Epoch:3 \n",
      "Iteration:592 \n",
      "Loss:0.09901095181703568\n",
      "Epoch:3 \n",
      "Iteration:593 \n",
      "Loss:0.045028939843177795\n",
      "Epoch:3 \n",
      "Iteration:594 \n",
      "Loss:0.15335804224014282\n",
      "Epoch:3 \n",
      "Iteration:595 \n",
      "Loss:0.04694725573062897\n",
      "Epoch:3 \n",
      "Iteration:596 \n",
      "Loss:0.04241202771663666\n",
      "Epoch:3 \n",
      "Iteration:597 \n",
      "Loss:0.078656405210495\n",
      "Epoch:3 \n",
      "Iteration:598 \n",
      "Loss:0.0703522264957428\n",
      "Epoch:3 \n",
      "Iteration:599 \n",
      "Loss:0.05391570180654526\n",
      "Epoch:3 \n",
      "Iteration:600 \n",
      "Loss:0.08491485565900803\n",
      "\n",
      "Accuracy of network in epoch 3: 97.97833333333334\n",
      "Epoch:4 \n",
      "Iteration:1 \n",
      "Loss:0.01789376325905323\n",
      "Epoch:4 \n",
      "Iteration:2 \n",
      "Loss:0.07889601588249207\n",
      "Epoch:4 \n",
      "Iteration:3 \n",
      "Loss:0.04703843966126442\n",
      "Epoch:4 \n",
      "Iteration:4 \n",
      "Loss:0.04022621735930443\n",
      "Epoch:4 \n",
      "Iteration:5 \n",
      "Loss:0.008627141825854778\n",
      "Epoch:4 \n",
      "Iteration:6 \n",
      "Loss:0.01156854722648859\n",
      "Epoch:4 \n",
      "Iteration:7 \n",
      "Loss:0.01550103910267353\n",
      "Epoch:4 \n",
      "Iteration:8 \n",
      "Loss:0.07879588752985\n",
      "Epoch:4 \n",
      "Iteration:9 \n",
      "Loss:0.008173178881406784\n",
      "Epoch:4 \n",
      "Iteration:10 \n",
      "Loss:0.059393320232629776\n",
      "Epoch:4 \n",
      "Iteration:11 \n",
      "Loss:0.022475985810160637\n",
      "Epoch:4 \n",
      "Iteration:12 \n",
      "Loss:0.03332153707742691\n",
      "Epoch:4 \n",
      "Iteration:13 \n",
      "Loss:0.015555769205093384\n",
      "Epoch:4 \n",
      "Iteration:14 \n",
      "Loss:0.0357709601521492\n",
      "Epoch:4 \n",
      "Iteration:15 \n",
      "Loss:0.06433311104774475\n",
      "Epoch:4 \n",
      "Iteration:16 \n",
      "Loss:0.016791533678770065\n",
      "Epoch:4 \n",
      "Iteration:17 \n",
      "Loss:0.04974867403507233\n",
      "Epoch:4 \n",
      "Iteration:18 \n",
      "Loss:0.005947496276348829\n",
      "Epoch:4 \n",
      "Iteration:19 \n",
      "Loss:0.05682890862226486\n",
      "Epoch:4 \n",
      "Iteration:20 \n",
      "Loss:0.06448543816804886\n",
      "Epoch:4 \n",
      "Iteration:21 \n",
      "Loss:0.04339738190174103\n",
      "Epoch:4 \n",
      "Iteration:22 \n",
      "Loss:0.07480448484420776\n",
      "Epoch:4 \n",
      "Iteration:23 \n",
      "Loss:0.023384351283311844\n",
      "Epoch:4 \n",
      "Iteration:24 \n",
      "Loss:0.10775664448738098\n",
      "Epoch:4 \n",
      "Iteration:25 \n",
      "Loss:0.02458544261753559\n",
      "Epoch:4 \n",
      "Iteration:26 \n",
      "Loss:0.07701321691274643\n",
      "Epoch:4 \n",
      "Iteration:27 \n",
      "Loss:0.12433914095163345\n",
      "Epoch:4 \n",
      "Iteration:28 \n",
      "Loss:0.031119676306843758\n",
      "Epoch:4 \n",
      "Iteration:29 \n",
      "Loss:0.016854573041200638\n",
      "Epoch:4 \n",
      "Iteration:30 \n",
      "Loss:0.003978261258453131\n",
      "Epoch:4 \n",
      "Iteration:31 \n",
      "Loss:0.0025878397282212973\n",
      "Epoch:4 \n",
      "Iteration:32 \n",
      "Loss:0.09982051700353622\n",
      "Epoch:4 \n",
      "Iteration:33 \n",
      "Loss:0.08204580098390579\n",
      "Epoch:4 \n",
      "Iteration:34 \n",
      "Loss:0.01873853988945484\n",
      "Epoch:4 \n",
      "Iteration:35 \n",
      "Loss:0.05223340913653374\n",
      "Epoch:4 \n",
      "Iteration:36 \n",
      "Loss:0.01321937795728445\n",
      "Epoch:4 \n",
      "Iteration:37 \n",
      "Loss:0.023532243445515633\n",
      "Epoch:4 \n",
      "Iteration:38 \n",
      "Loss:0.0490330308675766\n",
      "Epoch:4 \n",
      "Iteration:39 \n",
      "Loss:0.026632240042090416\n",
      "Epoch:4 \n",
      "Iteration:40 \n",
      "Loss:0.03256245702505112\n",
      "Epoch:4 \n",
      "Iteration:41 \n",
      "Loss:0.03067956492304802\n",
      "Epoch:4 \n",
      "Iteration:42 \n",
      "Loss:0.024496756494045258\n",
      "Epoch:4 \n",
      "Iteration:43 \n",
      "Loss:0.020979812368750572\n",
      "Epoch:4 \n",
      "Iteration:44 \n",
      "Loss:0.04607443884015083\n",
      "Epoch:4 \n",
      "Iteration:45 \n",
      "Loss:0.00590901542454958\n",
      "Epoch:4 \n",
      "Iteration:46 \n",
      "Loss:0.03034631535410881\n",
      "Epoch:4 \n",
      "Iteration:47 \n",
      "Loss:0.08082685619592667\n",
      "Epoch:4 \n",
      "Iteration:48 \n",
      "Loss:0.06856287270784378\n",
      "Epoch:4 \n",
      "Iteration:49 \n",
      "Loss:0.04518041014671326\n",
      "Epoch:4 \n",
      "Iteration:50 \n",
      "Loss:0.016538048163056374\n",
      "Epoch:4 \n",
      "Iteration:51 \n",
      "Loss:0.007430723402649164\n",
      "Epoch:4 \n",
      "Iteration:52 \n",
      "Loss:0.017678290605545044\n",
      "Epoch:4 \n",
      "Iteration:53 \n",
      "Loss:0.0033998119179159403\n",
      "Epoch:4 \n",
      "Iteration:54 \n",
      "Loss:0.014363058842718601\n",
      "Epoch:4 \n",
      "Iteration:55 \n",
      "Loss:0.04996103793382645\n",
      "Epoch:4 \n",
      "Iteration:56 \n",
      "Loss:0.016824737191200256\n",
      "Epoch:4 \n",
      "Iteration:57 \n",
      "Loss:0.018151648342609406\n",
      "Epoch:4 \n",
      "Iteration:58 \n",
      "Loss:0.06474575400352478\n",
      "Epoch:4 \n",
      "Iteration:59 \n",
      "Loss:0.06597334146499634\n",
      "Epoch:4 \n",
      "Iteration:60 \n",
      "Loss:0.0074410829693078995\n",
      "Epoch:4 \n",
      "Iteration:61 \n",
      "Loss:0.029403066262602806\n",
      "Epoch:4 \n",
      "Iteration:62 \n",
      "Loss:0.014681065455079079\n",
      "Epoch:4 \n",
      "Iteration:63 \n",
      "Loss:0.021315770223736763\n",
      "Epoch:4 \n",
      "Iteration:64 \n",
      "Loss:0.00859622098505497\n",
      "Epoch:4 \n",
      "Iteration:65 \n",
      "Loss:0.00688653951510787\n",
      "Epoch:4 \n",
      "Iteration:66 \n",
      "Loss:0.04730942100286484\n",
      "Epoch:4 \n",
      "Iteration:67 \n",
      "Loss:0.02298618294298649\n",
      "Epoch:4 \n",
      "Iteration:68 \n",
      "Loss:0.06538284569978714\n",
      "Epoch:4 \n",
      "Iteration:69 \n",
      "Loss:0.011603139340877533\n",
      "Epoch:4 \n",
      "Iteration:70 \n",
      "Loss:0.047224294394254684\n",
      "Epoch:4 \n",
      "Iteration:71 \n",
      "Loss:0.008952029049396515\n",
      "Epoch:4 \n",
      "Iteration:72 \n",
      "Loss:0.02915778197348118\n",
      "Epoch:4 \n",
      "Iteration:73 \n",
      "Loss:0.01331786997616291\n",
      "Epoch:4 \n",
      "Iteration:74 \n",
      "Loss:0.006045385729521513\n",
      "Epoch:4 \n",
      "Iteration:75 \n",
      "Loss:0.013076921924948692\n",
      "Epoch:4 \n",
      "Iteration:76 \n",
      "Loss:0.02799094282090664\n",
      "Epoch:4 \n",
      "Iteration:77 \n",
      "Loss:0.03331561014056206\n",
      "Epoch:4 \n",
      "Iteration:78 \n",
      "Loss:0.005030190572142601\n",
      "Epoch:4 \n",
      "Iteration:79 \n",
      "Loss:0.021165695041418076\n",
      "Epoch:4 \n",
      "Iteration:80 \n",
      "Loss:0.016653193160891533\n",
      "Epoch:4 \n",
      "Iteration:81 \n",
      "Loss:0.029071908444166183\n",
      "Epoch:4 \n",
      "Iteration:82 \n",
      "Loss:0.002818274311721325\n",
      "Epoch:4 \n",
      "Iteration:83 \n",
      "Loss:0.09432516992092133\n",
      "Epoch:4 \n",
      "Iteration:84 \n",
      "Loss:0.03774137422442436\n",
      "Epoch:4 \n",
      "Iteration:85 \n",
      "Loss:0.07432451099157333\n",
      "Epoch:4 \n",
      "Iteration:86 \n",
      "Loss:0.06524107605218887\n",
      "Epoch:4 \n",
      "Iteration:87 \n",
      "Loss:0.007082641124725342\n",
      "Epoch:4 \n",
      "Iteration:88 \n",
      "Loss:0.050045523792505264\n",
      "Epoch:4 \n",
      "Iteration:89 \n",
      "Loss:0.042525481432676315\n",
      "Epoch:4 \n",
      "Iteration:90 \n",
      "Loss:0.09866044670343399\n",
      "Epoch:4 \n",
      "Iteration:91 \n",
      "Loss:0.08991432934999466\n",
      "Epoch:4 \n",
      "Iteration:92 \n",
      "Loss:0.08320696651935577\n",
      "Epoch:4 \n",
      "Iteration:93 \n",
      "Loss:0.010825909674167633\n",
      "Epoch:4 \n",
      "Iteration:94 \n",
      "Loss:0.05765775218605995\n",
      "Epoch:4 \n",
      "Iteration:95 \n",
      "Loss:0.08058343082666397\n",
      "Epoch:4 \n",
      "Iteration:96 \n",
      "Loss:0.006746064405888319\n",
      "Epoch:4 \n",
      "Iteration:97 \n",
      "Loss:0.022668354213237762\n",
      "Epoch:4 \n",
      "Iteration:98 \n",
      "Loss:0.003434357000514865\n",
      "Epoch:4 \n",
      "Iteration:99 \n",
      "Loss:0.044832728803157806\n",
      "Epoch:4 \n",
      "Iteration:100 \n",
      "Loss:0.009273026138544083\n",
      "Epoch:4 \n",
      "Iteration:101 \n",
      "Loss:0.011010735295712948\n",
      "Epoch:4 \n",
      "Iteration:102 \n",
      "Loss:0.030676711350679398\n",
      "Epoch:4 \n",
      "Iteration:103 \n",
      "Loss:0.013816446997225285\n",
      "Epoch:4 \n",
      "Iteration:104 \n",
      "Loss:0.08443519473075867\n",
      "Epoch:4 \n",
      "Iteration:105 \n",
      "Loss:0.027454061433672905\n",
      "Epoch:4 \n",
      "Iteration:106 \n",
      "Loss:0.03269563615322113\n",
      "Epoch:4 \n",
      "Iteration:107 \n",
      "Loss:0.038587652146816254\n",
      "Epoch:4 \n",
      "Iteration:108 \n",
      "Loss:0.036297816783189774\n",
      "Epoch:4 \n",
      "Iteration:109 \n",
      "Loss:0.031072886660695076\n",
      "Epoch:4 \n",
      "Iteration:110 \n",
      "Loss:0.03862094134092331\n",
      "Epoch:4 \n",
      "Iteration:111 \n",
      "Loss:0.1002841368317604\n",
      "Epoch:4 \n",
      "Iteration:112 \n",
      "Loss:0.030631277710199356\n",
      "Epoch:4 \n",
      "Iteration:113 \n",
      "Loss:0.07150768488645554\n",
      "Epoch:4 \n",
      "Iteration:114 \n",
      "Loss:0.030221426859498024\n",
      "Epoch:4 \n",
      "Iteration:115 \n",
      "Loss:0.011252054944634438\n",
      "Epoch:4 \n",
      "Iteration:116 \n",
      "Loss:0.07272277772426605\n",
      "Epoch:4 \n",
      "Iteration:117 \n",
      "Loss:0.03852635994553566\n",
      "Epoch:4 \n",
      "Iteration:118 \n",
      "Loss:0.04058416187763214\n",
      "Epoch:4 \n",
      "Iteration:119 \n",
      "Loss:0.007790723815560341\n",
      "Epoch:4 \n",
      "Iteration:120 \n",
      "Loss:0.015223665162920952\n",
      "Epoch:4 \n",
      "Iteration:121 \n",
      "Loss:0.1175655946135521\n",
      "Epoch:4 \n",
      "Iteration:122 \n",
      "Loss:0.10863318294286728\n",
      "Epoch:4 \n",
      "Iteration:123 \n",
      "Loss:0.04736940562725067\n",
      "Epoch:4 \n",
      "Iteration:124 \n",
      "Loss:0.024785034358501434\n",
      "Epoch:4 \n",
      "Iteration:125 \n",
      "Loss:0.02570423111319542\n",
      "Epoch:4 \n",
      "Iteration:126 \n",
      "Loss:0.012892677448689938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4 \n",
      "Iteration:127 \n",
      "Loss:0.007912449538707733\n",
      "Epoch:4 \n",
      "Iteration:128 \n",
      "Loss:0.041013747453689575\n",
      "Epoch:4 \n",
      "Iteration:129 \n",
      "Loss:0.018509484827518463\n",
      "Epoch:4 \n",
      "Iteration:130 \n",
      "Loss:0.019345315173268318\n",
      "Epoch:4 \n",
      "Iteration:131 \n",
      "Loss:0.10311723500490189\n",
      "Epoch:4 \n",
      "Iteration:132 \n",
      "Loss:0.05665489658713341\n",
      "Epoch:4 \n",
      "Iteration:133 \n",
      "Loss:0.10963281989097595\n",
      "Epoch:4 \n",
      "Iteration:134 \n",
      "Loss:0.043743062764406204\n",
      "Epoch:4 \n",
      "Iteration:135 \n",
      "Loss:0.057814355939626694\n",
      "Epoch:4 \n",
      "Iteration:136 \n",
      "Loss:0.020585181191563606\n",
      "Epoch:4 \n",
      "Iteration:137 \n",
      "Loss:0.11075325310230255\n",
      "Epoch:4 \n",
      "Iteration:138 \n",
      "Loss:0.02633899636566639\n",
      "Epoch:4 \n",
      "Iteration:139 \n",
      "Loss:0.01688660867512226\n",
      "Epoch:4 \n",
      "Iteration:140 \n",
      "Loss:0.03505046293139458\n",
      "Epoch:4 \n",
      "Iteration:141 \n",
      "Loss:0.06518782675266266\n",
      "Epoch:4 \n",
      "Iteration:142 \n",
      "Loss:0.02159339375793934\n",
      "Epoch:4 \n",
      "Iteration:143 \n",
      "Loss:0.03421924263238907\n",
      "Epoch:4 \n",
      "Iteration:144 \n",
      "Loss:0.020732436329126358\n",
      "Epoch:4 \n",
      "Iteration:145 \n",
      "Loss:0.04402105510234833\n",
      "Epoch:4 \n",
      "Iteration:146 \n",
      "Loss:0.06428989768028259\n",
      "Epoch:4 \n",
      "Iteration:147 \n",
      "Loss:0.03373410552740097\n",
      "Epoch:4 \n",
      "Iteration:148 \n",
      "Loss:0.013051302172243595\n",
      "Epoch:4 \n",
      "Iteration:149 \n",
      "Loss:0.057291179895401\n",
      "Epoch:4 \n",
      "Iteration:150 \n",
      "Loss:0.02513781376183033\n",
      "Epoch:4 \n",
      "Iteration:151 \n",
      "Loss:0.02907457761466503\n",
      "Epoch:4 \n",
      "Iteration:152 \n",
      "Loss:0.045794662088155746\n",
      "Epoch:4 \n",
      "Iteration:153 \n",
      "Loss:0.07223228365182877\n",
      "Epoch:4 \n",
      "Iteration:154 \n",
      "Loss:0.09580782055854797\n",
      "Epoch:4 \n",
      "Iteration:155 \n",
      "Loss:0.024940773844718933\n",
      "Epoch:4 \n",
      "Iteration:156 \n",
      "Loss:0.03560597822070122\n",
      "Epoch:4 \n",
      "Iteration:157 \n",
      "Loss:0.02950041927397251\n",
      "Epoch:4 \n",
      "Iteration:158 \n",
      "Loss:0.22930078208446503\n",
      "Epoch:4 \n",
      "Iteration:159 \n",
      "Loss:0.021253779530525208\n",
      "Epoch:4 \n",
      "Iteration:160 \n",
      "Loss:0.0204850435256958\n",
      "Epoch:4 \n",
      "Iteration:161 \n",
      "Loss:0.025805898010730743\n",
      "Epoch:4 \n",
      "Iteration:162 \n",
      "Loss:0.012012061662971973\n",
      "Epoch:4 \n",
      "Iteration:163 \n",
      "Loss:0.04875238239765167\n",
      "Epoch:4 \n",
      "Iteration:164 \n",
      "Loss:0.03282483294606209\n",
      "Epoch:4 \n",
      "Iteration:165 \n",
      "Loss:0.06108585000038147\n",
      "Epoch:4 \n",
      "Iteration:166 \n",
      "Loss:0.09782817959785461\n",
      "Epoch:4 \n",
      "Iteration:167 \n",
      "Loss:0.0010686555178835988\n",
      "Epoch:4 \n",
      "Iteration:168 \n",
      "Loss:0.012737821787595749\n",
      "Epoch:4 \n",
      "Iteration:169 \n",
      "Loss:0.053565338253974915\n",
      "Epoch:4 \n",
      "Iteration:170 \n",
      "Loss:0.0070737628266215324\n",
      "Epoch:4 \n",
      "Iteration:171 \n",
      "Loss:0.07067570090293884\n",
      "Epoch:4 \n",
      "Iteration:172 \n",
      "Loss:0.002382984384894371\n",
      "Epoch:4 \n",
      "Iteration:173 \n",
      "Loss:0.0251668319106102\n",
      "Epoch:4 \n",
      "Iteration:174 \n",
      "Loss:0.06688268482685089\n",
      "Epoch:4 \n",
      "Iteration:175 \n",
      "Loss:0.05048993602395058\n",
      "Epoch:4 \n",
      "Iteration:176 \n",
      "Loss:0.05242740735411644\n",
      "Epoch:4 \n",
      "Iteration:177 \n",
      "Loss:0.0658479705452919\n",
      "Epoch:4 \n",
      "Iteration:178 \n",
      "Loss:0.012247789651155472\n",
      "Epoch:4 \n",
      "Iteration:179 \n",
      "Loss:0.03499563783407211\n",
      "Epoch:4 \n",
      "Iteration:180 \n",
      "Loss:0.03592647984623909\n",
      "Epoch:4 \n",
      "Iteration:181 \n",
      "Loss:0.06284985691308975\n",
      "Epoch:4 \n",
      "Iteration:182 \n",
      "Loss:0.008889662101864815\n",
      "Epoch:4 \n",
      "Iteration:183 \n",
      "Loss:0.03530336171388626\n",
      "Epoch:4 \n",
      "Iteration:184 \n",
      "Loss:0.09110040962696075\n",
      "Epoch:4 \n",
      "Iteration:185 \n",
      "Loss:0.06151720881462097\n",
      "Epoch:4 \n",
      "Iteration:186 \n",
      "Loss:0.03167538717389107\n",
      "Epoch:4 \n",
      "Iteration:187 \n",
      "Loss:0.020943166688084602\n",
      "Epoch:4 \n",
      "Iteration:188 \n",
      "Loss:0.046729739755392075\n",
      "Epoch:4 \n",
      "Iteration:189 \n",
      "Loss:0.010419253259897232\n",
      "Epoch:4 \n",
      "Iteration:190 \n",
      "Loss:0.09722647070884705\n",
      "Epoch:4 \n",
      "Iteration:191 \n",
      "Loss:0.02741861157119274\n",
      "Epoch:4 \n",
      "Iteration:192 \n",
      "Loss:0.011463813483715057\n",
      "Epoch:4 \n",
      "Iteration:193 \n",
      "Loss:0.06587100774049759\n",
      "Epoch:4 \n",
      "Iteration:194 \n",
      "Loss:0.013265818357467651\n",
      "Epoch:4 \n",
      "Iteration:195 \n",
      "Loss:0.007137294393032789\n",
      "Epoch:4 \n",
      "Iteration:196 \n",
      "Loss:0.027204925194382668\n",
      "Epoch:4 \n",
      "Iteration:197 \n",
      "Loss:0.06652376055717468\n",
      "Epoch:4 \n",
      "Iteration:198 \n",
      "Loss:0.018772903829813004\n",
      "Epoch:4 \n",
      "Iteration:199 \n",
      "Loss:0.043611861765384674\n",
      "Epoch:4 \n",
      "Iteration:200 \n",
      "Loss:0.0396699495613575\n",
      "Epoch:4 \n",
      "Iteration:201 \n",
      "Loss:0.015717720612883568\n",
      "Epoch:4 \n",
      "Iteration:202 \n",
      "Loss:0.02865874022245407\n",
      "Epoch:4 \n",
      "Iteration:203 \n",
      "Loss:0.08666635304689407\n",
      "Epoch:4 \n",
      "Iteration:204 \n",
      "Loss:0.13036249577999115\n",
      "Epoch:4 \n",
      "Iteration:205 \n",
      "Loss:0.06751979887485504\n",
      "Epoch:4 \n",
      "Iteration:206 \n",
      "Loss:0.10360526293516159\n",
      "Epoch:4 \n",
      "Iteration:207 \n",
      "Loss:0.03300411254167557\n",
      "Epoch:4 \n",
      "Iteration:208 \n",
      "Loss:0.04355408996343613\n",
      "Epoch:4 \n",
      "Iteration:209 \n",
      "Loss:0.046397749334573746\n",
      "Epoch:4 \n",
      "Iteration:210 \n",
      "Loss:0.099107526242733\n",
      "Epoch:4 \n",
      "Iteration:211 \n",
      "Loss:0.005363555159419775\n",
      "Epoch:4 \n",
      "Iteration:212 \n",
      "Loss:0.016780894249677658\n",
      "Epoch:4 \n",
      "Iteration:213 \n",
      "Loss:0.034937020391225815\n",
      "Epoch:4 \n",
      "Iteration:214 \n",
      "Loss:0.032664209604263306\n",
      "Epoch:4 \n",
      "Iteration:215 \n",
      "Loss:0.021255025640130043\n",
      "Epoch:4 \n",
      "Iteration:216 \n",
      "Loss:0.0952213853597641\n",
      "Epoch:4 \n",
      "Iteration:217 \n",
      "Loss:0.026839274913072586\n",
      "Epoch:4 \n",
      "Iteration:218 \n",
      "Loss:0.02510874904692173\n",
      "Epoch:4 \n",
      "Iteration:219 \n",
      "Loss:0.0050577991642057896\n",
      "Epoch:4 \n",
      "Iteration:220 \n",
      "Loss:0.0354459248483181\n",
      "Epoch:4 \n",
      "Iteration:221 \n",
      "Loss:0.013688416220247746\n",
      "Epoch:4 \n",
      "Iteration:222 \n",
      "Loss:0.010984021238982677\n",
      "Epoch:4 \n",
      "Iteration:223 \n",
      "Loss:0.01632738672196865\n",
      "Epoch:4 \n",
      "Iteration:224 \n",
      "Loss:0.008102284744381905\n",
      "Epoch:4 \n",
      "Iteration:225 \n",
      "Loss:0.019166121259331703\n",
      "Epoch:4 \n",
      "Iteration:226 \n",
      "Loss:0.008138826116919518\n",
      "Epoch:4 \n",
      "Iteration:227 \n",
      "Loss:0.02244025655090809\n",
      "Epoch:4 \n",
      "Iteration:228 \n",
      "Loss:0.10369366407394409\n",
      "Epoch:4 \n",
      "Iteration:229 \n",
      "Loss:0.04329647496342659\n",
      "Epoch:4 \n",
      "Iteration:230 \n",
      "Loss:0.021417617797851562\n",
      "Epoch:4 \n",
      "Iteration:231 \n",
      "Loss:0.012163382954895496\n",
      "Epoch:4 \n",
      "Iteration:232 \n",
      "Loss:0.07784514129161835\n",
      "Epoch:4 \n",
      "Iteration:233 \n",
      "Loss:0.05458538979291916\n",
      "Epoch:4 \n",
      "Iteration:234 \n",
      "Loss:0.08573481440544128\n",
      "Epoch:4 \n",
      "Iteration:235 \n",
      "Loss:0.018282504752278328\n",
      "Epoch:4 \n",
      "Iteration:236 \n",
      "Loss:0.03855879232287407\n",
      "Epoch:4 \n",
      "Iteration:237 \n",
      "Loss:0.0955142080783844\n",
      "Epoch:4 \n",
      "Iteration:238 \n",
      "Loss:0.15187755227088928\n",
      "Epoch:4 \n",
      "Iteration:239 \n",
      "Loss:0.07280685752630234\n",
      "Epoch:4 \n",
      "Iteration:240 \n",
      "Loss:0.15878306329250336\n",
      "Epoch:4 \n",
      "Iteration:241 \n",
      "Loss:0.06972454488277435\n",
      "Epoch:4 \n",
      "Iteration:242 \n",
      "Loss:0.012178049422800541\n",
      "Epoch:4 \n",
      "Iteration:243 \n",
      "Loss:0.0801473930478096\n",
      "Epoch:4 \n",
      "Iteration:244 \n",
      "Loss:0.1338726282119751\n",
      "Epoch:4 \n",
      "Iteration:245 \n",
      "Loss:0.0067077274434268475\n",
      "Epoch:4 \n",
      "Iteration:246 \n",
      "Loss:0.034120168536901474\n",
      "Epoch:4 \n",
      "Iteration:247 \n",
      "Loss:0.052360281348228455\n",
      "Epoch:4 \n",
      "Iteration:248 \n",
      "Loss:0.057258643209934235\n",
      "Epoch:4 \n",
      "Iteration:249 \n",
      "Loss:0.12406441569328308\n",
      "Epoch:4 \n",
      "Iteration:250 \n",
      "Loss:0.05287754908204079\n",
      "Epoch:4 \n",
      "Iteration:251 \n",
      "Loss:0.1877429336309433\n",
      "Epoch:4 \n",
      "Iteration:252 \n",
      "Loss:0.08802897483110428\n",
      "Epoch:4 \n",
      "Iteration:253 \n",
      "Loss:0.06309767812490463\n",
      "Epoch:4 \n",
      "Iteration:254 \n",
      "Loss:0.03678484633564949\n",
      "Epoch:4 \n",
      "Iteration:255 \n",
      "Loss:0.014342695474624634\n",
      "Epoch:4 \n",
      "Iteration:256 \n",
      "Loss:0.044328488409519196\n",
      "Epoch:4 \n",
      "Iteration:257 \n",
      "Loss:0.053095147013664246\n",
      "Epoch:4 \n",
      "Iteration:258 \n",
      "Loss:0.04106579348444939\n",
      "Epoch:4 \n",
      "Iteration:259 \n",
      "Loss:0.04265962541103363\n",
      "Epoch:4 \n",
      "Iteration:260 \n",
      "Loss:0.03779234364628792\n",
      "Epoch:4 \n",
      "Iteration:261 \n",
      "Loss:0.048096828162670135\n",
      "Epoch:4 \n",
      "Iteration:262 \n",
      "Loss:0.05857823044061661\n",
      "Epoch:4 \n",
      "Iteration:263 \n",
      "Loss:0.14568962156772614\n",
      "Epoch:4 \n",
      "Iteration:264 \n",
      "Loss:0.010872261598706245\n",
      "Epoch:4 \n",
      "Iteration:265 \n",
      "Loss:0.030020492151379585\n",
      "Epoch:4 \n",
      "Iteration:266 \n",
      "Loss:0.03108099475502968\n",
      "Epoch:4 \n",
      "Iteration:267 \n",
      "Loss:0.060746122151613235\n",
      "Epoch:4 \n",
      "Iteration:268 \n",
      "Loss:0.010861274786293507\n",
      "Epoch:4 \n",
      "Iteration:269 \n",
      "Loss:0.041624534875154495\n",
      "Epoch:4 \n",
      "Iteration:270 \n",
      "Loss:0.04964950680732727\n",
      "Epoch:4 \n",
      "Iteration:271 \n",
      "Loss:0.02079024724662304\n",
      "Epoch:4 \n",
      "Iteration:272 \n",
      "Loss:0.039441172033548355\n",
      "Epoch:4 \n",
      "Iteration:273 \n",
      "Loss:0.03411968797445297\n",
      "Epoch:4 \n",
      "Iteration:274 \n",
      "Loss:0.026641465723514557\n",
      "Epoch:4 \n",
      "Iteration:275 \n",
      "Loss:0.02539953961968422\n",
      "Epoch:4 \n",
      "Iteration:276 \n",
      "Loss:0.047852907329797745\n",
      "Epoch:4 \n",
      "Iteration:277 \n",
      "Loss:0.04497009143233299\n",
      "Epoch:4 \n",
      "Iteration:278 \n",
      "Loss:0.01229097880423069\n",
      "Epoch:4 \n",
      "Iteration:279 \n",
      "Loss:0.0556337833404541\n",
      "Epoch:4 \n",
      "Iteration:280 \n",
      "Loss:0.058529749512672424\n",
      "Epoch:4 \n",
      "Iteration:281 \n",
      "Loss:0.09918298572301865\n",
      "Epoch:4 \n",
      "Iteration:282 \n",
      "Loss:0.030847666785120964\n",
      "Epoch:4 \n",
      "Iteration:283 \n",
      "Loss:0.053102895617485046\n",
      "Epoch:4 \n",
      "Iteration:284 \n",
      "Loss:0.04342872276902199\n",
      "Epoch:4 \n",
      "Iteration:285 \n",
      "Loss:0.029163140803575516\n",
      "Epoch:4 \n",
      "Iteration:286 \n",
      "Loss:0.1216549426317215\n",
      "Epoch:4 \n",
      "Iteration:287 \n",
      "Loss:0.049458179622888565\n",
      "Epoch:4 \n",
      "Iteration:288 \n",
      "Loss:0.019466133788228035\n",
      "Epoch:4 \n",
      "Iteration:289 \n",
      "Loss:0.015868451446294785\n",
      "Epoch:4 \n",
      "Iteration:290 \n",
      "Loss:0.026472298428416252\n",
      "Epoch:4 \n",
      "Iteration:291 \n",
      "Loss:0.008392277173697948\n",
      "Epoch:4 \n",
      "Iteration:292 \n",
      "Loss:0.1151043251156807\n",
      "Epoch:4 \n",
      "Iteration:293 \n",
      "Loss:0.03363701328635216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4 \n",
      "Iteration:294 \n",
      "Loss:0.0700664296746254\n",
      "Epoch:4 \n",
      "Iteration:295 \n",
      "Loss:0.03385183960199356\n",
      "Epoch:4 \n",
      "Iteration:296 \n",
      "Loss:0.0621645413339138\n",
      "Epoch:4 \n",
      "Iteration:297 \n",
      "Loss:0.030149735510349274\n",
      "Epoch:4 \n",
      "Iteration:298 \n",
      "Loss:0.017900250852108\n",
      "Epoch:4 \n",
      "Iteration:299 \n",
      "Loss:0.018759354948997498\n",
      "Epoch:4 \n",
      "Iteration:300 \n",
      "Loss:0.01161237433552742\n",
      "Epoch:4 \n",
      "Iteration:301 \n",
      "Loss:0.05781356990337372\n",
      "Epoch:4 \n",
      "Iteration:302 \n",
      "Loss:0.13541348278522491\n",
      "Epoch:4 \n",
      "Iteration:303 \n",
      "Loss:0.03125126659870148\n",
      "Epoch:4 \n",
      "Iteration:304 \n",
      "Loss:0.04410817101597786\n",
      "Epoch:4 \n",
      "Iteration:305 \n",
      "Loss:0.01324605941772461\n",
      "Epoch:4 \n",
      "Iteration:306 \n",
      "Loss:0.046619124710559845\n",
      "Epoch:4 \n",
      "Iteration:307 \n",
      "Loss:0.01618296466767788\n",
      "Epoch:4 \n",
      "Iteration:308 \n",
      "Loss:0.05036887153983116\n",
      "Epoch:4 \n",
      "Iteration:309 \n",
      "Loss:0.046777963638305664\n",
      "Epoch:4 \n",
      "Iteration:310 \n",
      "Loss:0.05538192763924599\n",
      "Epoch:4 \n",
      "Iteration:311 \n",
      "Loss:0.009595093317329884\n",
      "Epoch:4 \n",
      "Iteration:312 \n",
      "Loss:0.021324045956134796\n",
      "Epoch:4 \n",
      "Iteration:313 \n",
      "Loss:0.057597726583480835\n",
      "Epoch:4 \n",
      "Iteration:314 \n",
      "Loss:0.007440105080604553\n",
      "Epoch:4 \n",
      "Iteration:315 \n",
      "Loss:0.032388605177402496\n",
      "Epoch:4 \n",
      "Iteration:316 \n",
      "Loss:0.029006514698266983\n",
      "Epoch:4 \n",
      "Iteration:317 \n",
      "Loss:0.059292688965797424\n",
      "Epoch:4 \n",
      "Iteration:318 \n",
      "Loss:0.0158858485519886\n",
      "Epoch:4 \n",
      "Iteration:319 \n",
      "Loss:0.009078847244381905\n",
      "Epoch:4 \n",
      "Iteration:320 \n",
      "Loss:0.12016525119543076\n",
      "Epoch:4 \n",
      "Iteration:321 \n",
      "Loss:0.09501208364963531\n",
      "Epoch:4 \n",
      "Iteration:322 \n",
      "Loss:0.16298708319664001\n",
      "Epoch:4 \n",
      "Iteration:323 \n",
      "Loss:0.013584684580564499\n",
      "Epoch:4 \n",
      "Iteration:324 \n",
      "Loss:0.034604012966156006\n",
      "Epoch:4 \n",
      "Iteration:325 \n",
      "Loss:0.026373988017439842\n",
      "Epoch:4 \n",
      "Iteration:326 \n",
      "Loss:0.025165721774101257\n",
      "Epoch:4 \n",
      "Iteration:327 \n",
      "Loss:0.014634179882705212\n",
      "Epoch:4 \n",
      "Iteration:328 \n",
      "Loss:0.05756602808833122\n",
      "Epoch:4 \n",
      "Iteration:329 \n",
      "Loss:0.031187834218144417\n",
      "Epoch:4 \n",
      "Iteration:330 \n",
      "Loss:0.11075112223625183\n",
      "Epoch:4 \n",
      "Iteration:331 \n",
      "Loss:0.1315167397260666\n",
      "Epoch:4 \n",
      "Iteration:332 \n",
      "Loss:0.008925540372729301\n",
      "Epoch:4 \n",
      "Iteration:333 \n",
      "Loss:0.034147828817367554\n",
      "Epoch:4 \n",
      "Iteration:334 \n",
      "Loss:0.038945067673921585\n",
      "Epoch:4 \n",
      "Iteration:335 \n",
      "Loss:0.08711092919111252\n",
      "Epoch:4 \n",
      "Iteration:336 \n",
      "Loss:0.05929270386695862\n",
      "Epoch:4 \n",
      "Iteration:337 \n",
      "Loss:0.05204319581389427\n",
      "Epoch:4 \n",
      "Iteration:338 \n",
      "Loss:0.03917538374662399\n",
      "Epoch:4 \n",
      "Iteration:339 \n",
      "Loss:0.039570700377225876\n",
      "Epoch:4 \n",
      "Iteration:340 \n",
      "Loss:0.06836003065109253\n",
      "Epoch:4 \n",
      "Iteration:341 \n",
      "Loss:0.08269675076007843\n",
      "Epoch:4 \n",
      "Iteration:342 \n",
      "Loss:0.010118076577782631\n",
      "Epoch:4 \n",
      "Iteration:343 \n",
      "Loss:0.07666605710983276\n",
      "Epoch:4 \n",
      "Iteration:344 \n",
      "Loss:0.008852044120430946\n",
      "Epoch:4 \n",
      "Iteration:345 \n",
      "Loss:0.07971706986427307\n",
      "Epoch:4 \n",
      "Iteration:346 \n",
      "Loss:0.07816839963197708\n",
      "Epoch:4 \n",
      "Iteration:347 \n",
      "Loss:0.054274026304483414\n",
      "Epoch:4 \n",
      "Iteration:348 \n",
      "Loss:0.034354422241449356\n",
      "Epoch:4 \n",
      "Iteration:349 \n",
      "Loss:0.049411311745643616\n",
      "Epoch:4 \n",
      "Iteration:350 \n",
      "Loss:0.09869148582220078\n",
      "Epoch:4 \n",
      "Iteration:351 \n",
      "Loss:0.05709552764892578\n",
      "Epoch:4 \n",
      "Iteration:352 \n",
      "Loss:0.08586981892585754\n",
      "Epoch:4 \n",
      "Iteration:353 \n",
      "Loss:0.028442520648241043\n",
      "Epoch:4 \n",
      "Iteration:354 \n",
      "Loss:0.019120456650853157\n",
      "Epoch:4 \n",
      "Iteration:355 \n",
      "Loss:0.07481022924184799\n",
      "Epoch:4 \n",
      "Iteration:356 \n",
      "Loss:0.10527177155017853\n",
      "Epoch:4 \n",
      "Iteration:357 \n",
      "Loss:0.0619005523622036\n",
      "Epoch:4 \n",
      "Iteration:358 \n",
      "Loss:0.01030681747943163\n",
      "Epoch:4 \n",
      "Iteration:359 \n",
      "Loss:0.03534584864974022\n",
      "Epoch:4 \n",
      "Iteration:360 \n",
      "Loss:0.028305325657129288\n",
      "Epoch:4 \n",
      "Iteration:361 \n",
      "Loss:0.021814441308379173\n",
      "Epoch:4 \n",
      "Iteration:362 \n",
      "Loss:0.0297683198004961\n",
      "Epoch:4 \n",
      "Iteration:363 \n",
      "Loss:0.08374860137701035\n",
      "Epoch:4 \n",
      "Iteration:364 \n",
      "Loss:0.048098813742399216\n",
      "Epoch:4 \n",
      "Iteration:365 \n",
      "Loss:0.023173758760094643\n",
      "Epoch:4 \n",
      "Iteration:366 \n",
      "Loss:0.12156688421964645\n",
      "Epoch:4 \n",
      "Iteration:367 \n",
      "Loss:0.04499993845820427\n",
      "Epoch:4 \n",
      "Iteration:368 \n",
      "Loss:0.030780615285038948\n",
      "Epoch:4 \n",
      "Iteration:369 \n",
      "Loss:0.07484473288059235\n",
      "Epoch:4 \n",
      "Iteration:370 \n",
      "Loss:0.10344385355710983\n",
      "Epoch:4 \n",
      "Iteration:371 \n",
      "Loss:0.0921032726764679\n",
      "Epoch:4 \n",
      "Iteration:372 \n",
      "Loss:0.0654354840517044\n",
      "Epoch:4 \n",
      "Iteration:373 \n",
      "Loss:0.05005211383104324\n",
      "Epoch:4 \n",
      "Iteration:374 \n",
      "Loss:0.02038717269897461\n",
      "Epoch:4 \n",
      "Iteration:375 \n",
      "Loss:0.04223956540226936\n",
      "Epoch:4 \n",
      "Iteration:376 \n",
      "Loss:0.08283750712871552\n",
      "Epoch:4 \n",
      "Iteration:377 \n",
      "Loss:0.0768808051943779\n",
      "Epoch:4 \n",
      "Iteration:378 \n",
      "Loss:0.0410267636179924\n",
      "Epoch:4 \n",
      "Iteration:379 \n",
      "Loss:0.04930896311998367\n",
      "Epoch:4 \n",
      "Iteration:380 \n",
      "Loss:0.02938346192240715\n",
      "Epoch:4 \n",
      "Iteration:381 \n",
      "Loss:0.07919315993785858\n",
      "Epoch:4 \n",
      "Iteration:382 \n",
      "Loss:0.04901115596294403\n",
      "Epoch:4 \n",
      "Iteration:383 \n",
      "Loss:0.06069306284189224\n",
      "Epoch:4 \n",
      "Iteration:384 \n",
      "Loss:0.0152069590985775\n",
      "Epoch:4 \n",
      "Iteration:385 \n",
      "Loss:0.027033641934394836\n",
      "Epoch:4 \n",
      "Iteration:386 \n",
      "Loss:0.014176848344504833\n",
      "Epoch:4 \n",
      "Iteration:387 \n",
      "Loss:0.031892016530036926\n",
      "Epoch:4 \n",
      "Iteration:388 \n",
      "Loss:0.02979177050292492\n",
      "Epoch:4 \n",
      "Iteration:389 \n",
      "Loss:0.027069278061389923\n",
      "Epoch:4 \n",
      "Iteration:390 \n",
      "Loss:0.016626836732029915\n",
      "Epoch:4 \n",
      "Iteration:391 \n",
      "Loss:0.09286576509475708\n",
      "Epoch:4 \n",
      "Iteration:392 \n",
      "Loss:0.13967980444431305\n",
      "Epoch:4 \n",
      "Iteration:393 \n",
      "Loss:0.036345526576042175\n",
      "Epoch:4 \n",
      "Iteration:394 \n",
      "Loss:0.011369862593710423\n",
      "Epoch:4 \n",
      "Iteration:395 \n",
      "Loss:0.05397408455610275\n",
      "Epoch:4 \n",
      "Iteration:396 \n",
      "Loss:0.016800187528133392\n",
      "Epoch:4 \n",
      "Iteration:397 \n",
      "Loss:0.019090062007308006\n",
      "Epoch:4 \n",
      "Iteration:398 \n",
      "Loss:0.061925627291202545\n",
      "Epoch:4 \n",
      "Iteration:399 \n",
      "Loss:0.02510826848447323\n",
      "Epoch:4 \n",
      "Iteration:400 \n",
      "Loss:0.1153946965932846\n",
      "Epoch:4 \n",
      "Iteration:401 \n",
      "Loss:0.030572600662708282\n",
      "Epoch:4 \n",
      "Iteration:402 \n",
      "Loss:0.004262290894985199\n",
      "Epoch:4 \n",
      "Iteration:403 \n",
      "Loss:0.00553370825946331\n",
      "Epoch:4 \n",
      "Iteration:404 \n",
      "Loss:0.018416978418827057\n",
      "Epoch:4 \n",
      "Iteration:405 \n",
      "Loss:0.03513094782829285\n",
      "Epoch:4 \n",
      "Iteration:406 \n",
      "Loss:0.03506619110703468\n",
      "Epoch:4 \n",
      "Iteration:407 \n",
      "Loss:0.012176946736872196\n",
      "Epoch:4 \n",
      "Iteration:408 \n",
      "Loss:0.022709794342517853\n",
      "Epoch:4 \n",
      "Iteration:409 \n",
      "Loss:0.08302965760231018\n",
      "Epoch:4 \n",
      "Iteration:410 \n",
      "Loss:0.04280880093574524\n",
      "Epoch:4 \n",
      "Iteration:411 \n",
      "Loss:0.0362771637737751\n",
      "Epoch:4 \n",
      "Iteration:412 \n",
      "Loss:0.08058687299489975\n",
      "Epoch:4 \n",
      "Iteration:413 \n",
      "Loss:0.03942721709609032\n",
      "Epoch:4 \n",
      "Iteration:414 \n",
      "Loss:0.013724220916628838\n",
      "Epoch:4 \n",
      "Iteration:415 \n",
      "Loss:0.006410585716366768\n",
      "Epoch:4 \n",
      "Iteration:416 \n",
      "Loss:0.012704133987426758\n",
      "Epoch:4 \n",
      "Iteration:417 \n",
      "Loss:0.0651843249797821\n",
      "Epoch:4 \n",
      "Iteration:418 \n",
      "Loss:0.0464077852666378\n",
      "Epoch:4 \n",
      "Iteration:419 \n",
      "Loss:0.023719336837530136\n",
      "Epoch:4 \n",
      "Iteration:420 \n",
      "Loss:0.06330014020204544\n",
      "Epoch:4 \n",
      "Iteration:421 \n",
      "Loss:0.025844551622867584\n",
      "Epoch:4 \n",
      "Iteration:422 \n",
      "Loss:0.05381614714860916\n",
      "Epoch:4 \n",
      "Iteration:423 \n",
      "Loss:0.007979905232787132\n",
      "Epoch:4 \n",
      "Iteration:424 \n",
      "Loss:0.0760030746459961\n",
      "Epoch:4 \n",
      "Iteration:425 \n",
      "Loss:0.04318208247423172\n",
      "Epoch:4 \n",
      "Iteration:426 \n",
      "Loss:0.05404441058635712\n",
      "Epoch:4 \n",
      "Iteration:427 \n",
      "Loss:0.04757403954863548\n",
      "Epoch:4 \n",
      "Iteration:428 \n",
      "Loss:0.06503885984420776\n",
      "Epoch:4 \n",
      "Iteration:429 \n",
      "Loss:0.06557431071996689\n",
      "Epoch:4 \n",
      "Iteration:430 \n",
      "Loss:0.04281027242541313\n",
      "Epoch:4 \n",
      "Iteration:431 \n",
      "Loss:0.14199618995189667\n",
      "Epoch:4 \n",
      "Iteration:432 \n",
      "Loss:0.06995493173599243\n",
      "Epoch:4 \n",
      "Iteration:433 \n",
      "Loss:0.011810476891696453\n",
      "Epoch:4 \n",
      "Iteration:434 \n",
      "Loss:0.009954454377293587\n",
      "Epoch:4 \n",
      "Iteration:435 \n",
      "Loss:0.08230546861886978\n",
      "Epoch:4 \n",
      "Iteration:436 \n",
      "Loss:0.07516516745090485\n",
      "Epoch:4 \n",
      "Iteration:437 \n",
      "Loss:0.04371054098010063\n",
      "Epoch:4 \n",
      "Iteration:438 \n",
      "Loss:0.053136616945266724\n",
      "Epoch:4 \n",
      "Iteration:439 \n",
      "Loss:0.0999719649553299\n",
      "Epoch:4 \n",
      "Iteration:440 \n",
      "Loss:0.039290495216846466\n",
      "Epoch:4 \n",
      "Iteration:441 \n",
      "Loss:0.08188962936401367\n",
      "Epoch:4 \n",
      "Iteration:442 \n",
      "Loss:0.02714819833636284\n",
      "Epoch:4 \n",
      "Iteration:443 \n",
      "Loss:0.07733628898859024\n",
      "Epoch:4 \n",
      "Iteration:444 \n",
      "Loss:0.1293228417634964\n",
      "Epoch:4 \n",
      "Iteration:445 \n",
      "Loss:0.03363217040896416\n",
      "Epoch:4 \n",
      "Iteration:446 \n",
      "Loss:0.0377047173678875\n",
      "Epoch:4 \n",
      "Iteration:447 \n",
      "Loss:0.039709605276584625\n",
      "Epoch:4 \n",
      "Iteration:448 \n",
      "Loss:0.01902863010764122\n",
      "Epoch:4 \n",
      "Iteration:449 \n",
      "Loss:0.09580697864294052\n",
      "Epoch:4 \n",
      "Iteration:450 \n",
      "Loss:0.15124060213565826\n",
      "Epoch:4 \n",
      "Iteration:451 \n",
      "Loss:0.005545345135033131\n",
      "Epoch:4 \n",
      "Iteration:452 \n",
      "Loss:0.11923860758543015\n",
      "Epoch:4 \n",
      "Iteration:453 \n",
      "Loss:0.006934755481779575\n",
      "Epoch:4 \n",
      "Iteration:454 \n",
      "Loss:0.07091594487428665\n",
      "Epoch:4 \n",
      "Iteration:455 \n",
      "Loss:0.06721224635839462\n",
      "Epoch:4 \n",
      "Iteration:456 \n",
      "Loss:0.052284542471170425\n",
      "Epoch:4 \n",
      "Iteration:457 \n",
      "Loss:0.032030023634433746\n",
      "Epoch:4 \n",
      "Iteration:458 \n",
      "Loss:0.03915654122829437\n",
      "Epoch:4 \n",
      "Iteration:459 \n",
      "Loss:0.05819541960954666\n",
      "Epoch:4 \n",
      "Iteration:460 \n",
      "Loss:0.12053148448467255\n",
      "Epoch:4 \n",
      "Iteration:461 \n",
      "Loss:0.04694046452641487\n",
      "Epoch:4 \n",
      "Iteration:462 \n",
      "Loss:0.05128633975982666\n",
      "Epoch:4 \n",
      "Iteration:463 \n",
      "Loss:0.042960718274116516\n",
      "Epoch:4 \n",
      "Iteration:464 \n",
      "Loss:0.08014263212680817\n",
      "Epoch:4 \n",
      "Iteration:465 \n",
      "Loss:0.1014503762125969\n",
      "Epoch:4 \n",
      "Iteration:466 \n",
      "Loss:0.04885342717170715\n",
      "Epoch:4 \n",
      "Iteration:467 \n",
      "Loss:0.044103365391492844\n",
      "Epoch:4 \n",
      "Iteration:468 \n",
      "Loss:0.00812288373708725\n",
      "Epoch:4 \n",
      "Iteration:469 \n",
      "Loss:0.01095378864556551\n",
      "Epoch:4 \n",
      "Iteration:470 \n",
      "Loss:0.018764303997159004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4 \n",
      "Iteration:471 \n",
      "Loss:0.12191998213529587\n",
      "Epoch:4 \n",
      "Iteration:472 \n",
      "Loss:0.017470842227339745\n",
      "Epoch:4 \n",
      "Iteration:473 \n",
      "Loss:0.05841919034719467\n",
      "Epoch:4 \n",
      "Iteration:474 \n",
      "Loss:0.07395163178443909\n",
      "Epoch:4 \n",
      "Iteration:475 \n",
      "Loss:0.03661569952964783\n",
      "Epoch:4 \n",
      "Iteration:476 \n",
      "Loss:0.1069280207157135\n",
      "Epoch:4 \n",
      "Iteration:477 \n",
      "Loss:0.020640652626752853\n",
      "Epoch:4 \n",
      "Iteration:478 \n",
      "Loss:0.051563773304224014\n",
      "Epoch:4 \n",
      "Iteration:479 \n",
      "Loss:0.03260396420955658\n",
      "Epoch:4 \n",
      "Iteration:480 \n",
      "Loss:0.04502737149596214\n",
      "Epoch:4 \n",
      "Iteration:481 \n",
      "Loss:0.01069057360291481\n",
      "Epoch:4 \n",
      "Iteration:482 \n",
      "Loss:0.04583257809281349\n",
      "Epoch:4 \n",
      "Iteration:483 \n",
      "Loss:0.03359924629330635\n",
      "Epoch:4 \n",
      "Iteration:484 \n",
      "Loss:0.07638128101825714\n",
      "Epoch:4 \n",
      "Iteration:485 \n",
      "Loss:0.11755654215812683\n",
      "Epoch:4 \n",
      "Iteration:486 \n",
      "Loss:0.014886762946844101\n",
      "Epoch:4 \n",
      "Iteration:487 \n",
      "Loss:0.02165236882865429\n",
      "Epoch:4 \n",
      "Iteration:488 \n",
      "Loss:0.07066107541322708\n",
      "Epoch:4 \n",
      "Iteration:489 \n",
      "Loss:0.01928560808300972\n",
      "Epoch:4 \n",
      "Iteration:490 \n",
      "Loss:0.07626079767942429\n",
      "Epoch:4 \n",
      "Iteration:491 \n",
      "Loss:0.0028982048388570547\n",
      "Epoch:4 \n",
      "Iteration:492 \n",
      "Loss:0.031610194593667984\n",
      "Epoch:4 \n",
      "Iteration:493 \n",
      "Loss:0.02797190472483635\n",
      "Epoch:4 \n",
      "Iteration:494 \n",
      "Loss:0.044019054621458054\n",
      "Epoch:4 \n",
      "Iteration:495 \n",
      "Loss:0.07471911609172821\n",
      "Epoch:4 \n",
      "Iteration:496 \n",
      "Loss:0.1199987605214119\n",
      "Epoch:4 \n",
      "Iteration:497 \n",
      "Loss:0.05997126176953316\n",
      "Epoch:4 \n",
      "Iteration:498 \n",
      "Loss:0.11776549369096756\n",
      "Epoch:4 \n",
      "Iteration:499 \n",
      "Loss:0.015714412555098534\n",
      "Epoch:4 \n",
      "Iteration:500 \n",
      "Loss:0.01498736534267664\n",
      "Epoch:4 \n",
      "Iteration:501 \n",
      "Loss:0.005744211841374636\n",
      "Epoch:4 \n",
      "Iteration:502 \n",
      "Loss:0.10182604193687439\n",
      "Epoch:4 \n",
      "Iteration:503 \n",
      "Loss:0.011080545373260975\n",
      "Epoch:4 \n",
      "Iteration:504 \n",
      "Loss:0.032462649047374725\n",
      "Epoch:4 \n",
      "Iteration:505 \n",
      "Loss:0.03780407831072807\n",
      "Epoch:4 \n",
      "Iteration:506 \n",
      "Loss:0.015614187344908714\n",
      "Epoch:4 \n",
      "Iteration:507 \n",
      "Loss:0.06760770082473755\n",
      "Epoch:4 \n",
      "Iteration:508 \n",
      "Loss:0.016738038510084152\n",
      "Epoch:4 \n",
      "Iteration:509 \n",
      "Loss:0.01658971793949604\n",
      "Epoch:4 \n",
      "Iteration:510 \n",
      "Loss:0.024305012077093124\n",
      "Epoch:4 \n",
      "Iteration:511 \n",
      "Loss:0.04759565740823746\n",
      "Epoch:4 \n",
      "Iteration:512 \n",
      "Loss:0.06693946570158005\n",
      "Epoch:4 \n",
      "Iteration:513 \n",
      "Loss:0.022829845547676086\n",
      "Epoch:4 \n",
      "Iteration:514 \n",
      "Loss:0.06104227527976036\n",
      "Epoch:4 \n",
      "Iteration:515 \n",
      "Loss:0.0050639077089726925\n",
      "Epoch:4 \n",
      "Iteration:516 \n",
      "Loss:0.024346036836504936\n",
      "Epoch:4 \n",
      "Iteration:517 \n",
      "Loss:0.10049983859062195\n",
      "Epoch:4 \n",
      "Iteration:518 \n",
      "Loss:0.09313666075468063\n",
      "Epoch:4 \n",
      "Iteration:519 \n",
      "Loss:0.030021915212273598\n",
      "Epoch:4 \n",
      "Iteration:520 \n",
      "Loss:0.04179150238633156\n",
      "Epoch:4 \n",
      "Iteration:521 \n",
      "Loss:0.02456755004823208\n",
      "Epoch:4 \n",
      "Iteration:522 \n",
      "Loss:0.03723936900496483\n",
      "Epoch:4 \n",
      "Iteration:523 \n",
      "Loss:0.07391918450593948\n",
      "Epoch:4 \n",
      "Iteration:524 \n",
      "Loss:0.020798394456505775\n",
      "Epoch:4 \n",
      "Iteration:525 \n",
      "Loss:0.05160648748278618\n",
      "Epoch:4 \n",
      "Iteration:526 \n",
      "Loss:0.03905922919511795\n",
      "Epoch:4 \n",
      "Iteration:527 \n",
      "Loss:0.06144167110323906\n",
      "Epoch:4 \n",
      "Iteration:528 \n",
      "Loss:0.09771379828453064\n",
      "Epoch:4 \n",
      "Iteration:529 \n",
      "Loss:0.027142181992530823\n",
      "Epoch:4 \n",
      "Iteration:530 \n",
      "Loss:0.13097403943538666\n",
      "Epoch:4 \n",
      "Iteration:531 \n",
      "Loss:0.010433226823806763\n",
      "Epoch:4 \n",
      "Iteration:532 \n",
      "Loss:0.016229771077632904\n",
      "Epoch:4 \n",
      "Iteration:533 \n",
      "Loss:0.03643365204334259\n",
      "Epoch:4 \n",
      "Iteration:534 \n",
      "Loss:0.023893823847174644\n",
      "Epoch:4 \n",
      "Iteration:535 \n",
      "Loss:0.006459407042711973\n",
      "Epoch:4 \n",
      "Iteration:536 \n",
      "Loss:0.05996400862932205\n",
      "Epoch:4 \n",
      "Iteration:537 \n",
      "Loss:0.05098329856991768\n",
      "Epoch:4 \n",
      "Iteration:538 \n",
      "Loss:0.06003999710083008\n",
      "Epoch:4 \n",
      "Iteration:539 \n",
      "Loss:0.06455986201763153\n",
      "Epoch:4 \n",
      "Iteration:540 \n",
      "Loss:0.0018409814219921827\n",
      "Epoch:4 \n",
      "Iteration:541 \n",
      "Loss:0.03902461379766464\n",
      "Epoch:4 \n",
      "Iteration:542 \n",
      "Loss:0.04378477483987808\n",
      "Epoch:4 \n",
      "Iteration:543 \n",
      "Loss:0.0061819530092179775\n",
      "Epoch:4 \n",
      "Iteration:544 \n",
      "Loss:0.047182999551296234\n",
      "Epoch:4 \n",
      "Iteration:545 \n",
      "Loss:0.05746840313076973\n",
      "Epoch:4 \n",
      "Iteration:546 \n",
      "Loss:0.01906627230346203\n",
      "Epoch:4 \n",
      "Iteration:547 \n",
      "Loss:0.20936216413974762\n",
      "Epoch:4 \n",
      "Iteration:548 \n",
      "Loss:0.0839746966958046\n",
      "Epoch:4 \n",
      "Iteration:549 \n",
      "Loss:0.024327969178557396\n",
      "Epoch:4 \n",
      "Iteration:550 \n",
      "Loss:0.042461663484573364\n",
      "Epoch:4 \n",
      "Iteration:551 \n",
      "Loss:0.01974553056061268\n",
      "Epoch:4 \n",
      "Iteration:552 \n",
      "Loss:0.05275796353816986\n",
      "Epoch:4 \n",
      "Iteration:553 \n",
      "Loss:0.11748737096786499\n",
      "Epoch:4 \n",
      "Iteration:554 \n",
      "Loss:0.17028820514678955\n",
      "Epoch:4 \n",
      "Iteration:555 \n",
      "Loss:0.012800345197319984\n",
      "Epoch:4 \n",
      "Iteration:556 \n",
      "Loss:0.17666266858577728\n",
      "Epoch:4 \n",
      "Iteration:557 \n",
      "Loss:0.028835639357566833\n",
      "Epoch:4 \n",
      "Iteration:558 \n",
      "Loss:0.04012872651219368\n",
      "Epoch:4 \n",
      "Iteration:559 \n",
      "Loss:0.04176301136612892\n",
      "Epoch:4 \n",
      "Iteration:560 \n",
      "Loss:0.07548405975103378\n",
      "Epoch:4 \n",
      "Iteration:561 \n",
      "Loss:0.0319393165409565\n",
      "Epoch:4 \n",
      "Iteration:562 \n",
      "Loss:0.058027878403663635\n",
      "Epoch:4 \n",
      "Iteration:563 \n",
      "Loss:0.08902982622385025\n",
      "Epoch:4 \n",
      "Iteration:564 \n",
      "Loss:0.04373681917786598\n",
      "Epoch:4 \n",
      "Iteration:565 \n",
      "Loss:0.03629143163561821\n",
      "Epoch:4 \n",
      "Iteration:566 \n",
      "Loss:0.035276684910058975\n",
      "Epoch:4 \n",
      "Iteration:567 \n",
      "Loss:0.07741788774728775\n",
      "Epoch:4 \n",
      "Iteration:568 \n",
      "Loss:0.03359391540288925\n",
      "Epoch:4 \n",
      "Iteration:569 \n",
      "Loss:0.1394750475883484\n",
      "Epoch:4 \n",
      "Iteration:570 \n",
      "Loss:0.10001648217439651\n",
      "Epoch:4 \n",
      "Iteration:571 \n",
      "Loss:0.01608988642692566\n",
      "Epoch:4 \n",
      "Iteration:572 \n",
      "Loss:0.05794857069849968\n",
      "Epoch:4 \n",
      "Iteration:573 \n",
      "Loss:0.01524822972714901\n",
      "Epoch:4 \n",
      "Iteration:574 \n",
      "Loss:0.1591728925704956\n",
      "Epoch:4 \n",
      "Iteration:575 \n",
      "Loss:0.0415838398039341\n",
      "Epoch:4 \n",
      "Iteration:576 \n",
      "Loss:0.05721773952245712\n",
      "Epoch:4 \n",
      "Iteration:577 \n",
      "Loss:0.1552903801202774\n",
      "Epoch:4 \n",
      "Iteration:578 \n",
      "Loss:0.007739701773971319\n",
      "Epoch:4 \n",
      "Iteration:579 \n",
      "Loss:0.029678814113140106\n",
      "Epoch:4 \n",
      "Iteration:580 \n",
      "Loss:0.0794377475976944\n",
      "Epoch:4 \n",
      "Iteration:581 \n",
      "Loss:0.020843911916017532\n",
      "Epoch:4 \n",
      "Iteration:582 \n",
      "Loss:0.07401937991380692\n",
      "Epoch:4 \n",
      "Iteration:583 \n",
      "Loss:0.06163157522678375\n",
      "Epoch:4 \n",
      "Iteration:584 \n",
      "Loss:0.07000530511140823\n",
      "Epoch:4 \n",
      "Iteration:585 \n",
      "Loss:0.030937686562538147\n",
      "Epoch:4 \n",
      "Iteration:586 \n",
      "Loss:0.0781107246875763\n",
      "Epoch:4 \n",
      "Iteration:587 \n",
      "Loss:0.03676345571875572\n",
      "Epoch:4 \n",
      "Iteration:588 \n",
      "Loss:0.008249121718108654\n",
      "Epoch:4 \n",
      "Iteration:589 \n",
      "Loss:0.028176402673125267\n",
      "Epoch:4 \n",
      "Iteration:590 \n",
      "Loss:0.007778591010719538\n",
      "Epoch:4 \n",
      "Iteration:591 \n",
      "Loss:0.09243366122245789\n",
      "Epoch:4 \n",
      "Iteration:592 \n",
      "Loss:0.06417675316333771\n",
      "Epoch:4 \n",
      "Iteration:593 \n",
      "Loss:0.0426030196249485\n",
      "Epoch:4 \n",
      "Iteration:594 \n",
      "Loss:0.12897726893424988\n",
      "Epoch:4 \n",
      "Iteration:595 \n",
      "Loss:0.03154204413294792\n",
      "Epoch:4 \n",
      "Iteration:596 \n",
      "Loss:0.07647789269685745\n",
      "Epoch:4 \n",
      "Iteration:597 \n",
      "Loss:0.006134346127510071\n",
      "Epoch:4 \n",
      "Iteration:598 \n",
      "Loss:0.028373437002301216\n",
      "Epoch:4 \n",
      "Iteration:599 \n",
      "Loss:0.028972206637263298\n",
      "Epoch:4 \n",
      "Iteration:600 \n",
      "Loss:0.17726197838783264\n",
      "\n",
      "Accuracy of network in epoch 4: 98.48833333333333\n",
      "Epoch:5 \n",
      "Iteration:1 \n",
      "Loss:0.008354908786714077\n",
      "Epoch:5 \n",
      "Iteration:2 \n",
      "Loss:0.0021204808726906776\n",
      "Epoch:5 \n",
      "Iteration:3 \n",
      "Loss:0.009838227182626724\n",
      "Epoch:5 \n",
      "Iteration:4 \n",
      "Loss:0.04806382209062576\n",
      "Epoch:5 \n",
      "Iteration:5 \n",
      "Loss:0.016117582097649574\n",
      "Epoch:5 \n",
      "Iteration:6 \n",
      "Loss:0.014580371789634228\n",
      "Epoch:5 \n",
      "Iteration:7 \n",
      "Loss:0.014269322156906128\n",
      "Epoch:5 \n",
      "Iteration:8 \n",
      "Loss:0.019681314006447792\n",
      "Epoch:5 \n",
      "Iteration:9 \n",
      "Loss:0.027688927948474884\n",
      "Epoch:5 \n",
      "Iteration:10 \n",
      "Loss:0.060438722372055054\n",
      "Epoch:5 \n",
      "Iteration:11 \n",
      "Loss:0.0070455027744174\n",
      "Epoch:5 \n",
      "Iteration:12 \n",
      "Loss:0.003211061004549265\n",
      "Epoch:5 \n",
      "Iteration:13 \n",
      "Loss:0.009114285930991173\n",
      "Epoch:5 \n",
      "Iteration:14 \n",
      "Loss:0.17354173958301544\n",
      "Epoch:5 \n",
      "Iteration:15 \n",
      "Loss:0.016876043751835823\n",
      "Epoch:5 \n",
      "Iteration:16 \n",
      "Loss:0.013226073235273361\n",
      "Epoch:5 \n",
      "Iteration:17 \n",
      "Loss:0.0037711127661168575\n",
      "Epoch:5 \n",
      "Iteration:18 \n",
      "Loss:0.02288839779794216\n",
      "Epoch:5 \n",
      "Iteration:19 \n",
      "Loss:0.0270115714520216\n",
      "Epoch:5 \n",
      "Iteration:20 \n",
      "Loss:0.09235724806785583\n",
      "Epoch:5 \n",
      "Iteration:21 \n",
      "Loss:0.013838011771440506\n",
      "Epoch:5 \n",
      "Iteration:22 \n",
      "Loss:0.008335795253515244\n",
      "Epoch:5 \n",
      "Iteration:23 \n",
      "Loss:0.021612191572785378\n",
      "Epoch:5 \n",
      "Iteration:24 \n",
      "Loss:0.029852796345949173\n",
      "Epoch:5 \n",
      "Iteration:25 \n",
      "Loss:0.008187114261090755\n",
      "Epoch:5 \n",
      "Iteration:26 \n",
      "Loss:0.059249456971883774\n",
      "Epoch:5 \n",
      "Iteration:27 \n",
      "Loss:0.04806073009967804\n",
      "Epoch:5 \n",
      "Iteration:28 \n",
      "Loss:0.0015549063682556152\n",
      "Epoch:5 \n",
      "Iteration:29 \n",
      "Loss:0.03204211965203285\n",
      "Epoch:5 \n",
      "Iteration:30 \n",
      "Loss:0.00381019851192832\n",
      "Epoch:5 \n",
      "Iteration:31 \n",
      "Loss:0.024857446551322937\n",
      "Epoch:5 \n",
      "Iteration:32 \n",
      "Loss:0.09588378667831421\n",
      "Epoch:5 \n",
      "Iteration:33 \n",
      "Loss:0.023676997050642967\n",
      "Epoch:5 \n",
      "Iteration:34 \n",
      "Loss:0.005142894573509693\n",
      "Epoch:5 \n",
      "Iteration:35 \n",
      "Loss:0.02087865024805069\n",
      "Epoch:5 \n",
      "Iteration:36 \n",
      "Loss:0.0809541866183281\n",
      "Epoch:5 \n",
      "Iteration:37 \n",
      "Loss:0.005338664632290602\n",
      "Epoch:5 \n",
      "Iteration:38 \n",
      "Loss:0.06231238320469856\n",
      "Epoch:5 \n",
      "Iteration:39 \n",
      "Loss:0.004364416468888521\n",
      "Epoch:5 \n",
      "Iteration:40 \n",
      "Loss:0.006477198097854853\n",
      "Epoch:5 \n",
      "Iteration:41 \n",
      "Loss:0.023357994854450226\n",
      "Epoch:5 \n",
      "Iteration:42 \n",
      "Loss:0.020256878808140755\n",
      "Epoch:5 \n",
      "Iteration:43 \n",
      "Loss:0.021849319338798523\n",
      "Epoch:5 \n",
      "Iteration:44 \n",
      "Loss:0.03112613968551159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5 \n",
      "Iteration:45 \n",
      "Loss:0.0739545151591301\n",
      "Epoch:5 \n",
      "Iteration:46 \n",
      "Loss:0.04631819203495979\n",
      "Epoch:5 \n",
      "Iteration:47 \n",
      "Loss:0.0501219742000103\n",
      "Epoch:5 \n",
      "Iteration:48 \n",
      "Loss:0.036879584193229675\n",
      "Epoch:5 \n",
      "Iteration:49 \n",
      "Loss:0.041280701756477356\n",
      "Epoch:5 \n",
      "Iteration:50 \n",
      "Loss:0.0423673540353775\n",
      "Epoch:5 \n",
      "Iteration:51 \n",
      "Loss:0.09156283736228943\n",
      "Epoch:5 \n",
      "Iteration:52 \n",
      "Loss:0.021895036101341248\n",
      "Epoch:5 \n",
      "Iteration:53 \n",
      "Loss:0.0639810636639595\n",
      "Epoch:5 \n",
      "Iteration:54 \n",
      "Loss:0.05017377808690071\n",
      "Epoch:5 \n",
      "Iteration:55 \n",
      "Loss:0.027040056884288788\n",
      "Epoch:5 \n",
      "Iteration:56 \n",
      "Loss:0.05508287996053696\n",
      "Epoch:5 \n",
      "Iteration:57 \n",
      "Loss:0.02778727561235428\n",
      "Epoch:5 \n",
      "Iteration:58 \n",
      "Loss:0.03794340789318085\n",
      "Epoch:5 \n",
      "Iteration:59 \n",
      "Loss:0.024515200406312943\n",
      "Epoch:5 \n",
      "Iteration:60 \n",
      "Loss:0.01581365242600441\n",
      "Epoch:5 \n",
      "Iteration:61 \n",
      "Loss:0.04929263889789581\n",
      "Epoch:5 \n",
      "Iteration:62 \n",
      "Loss:0.018748585134744644\n",
      "Epoch:5 \n",
      "Iteration:63 \n",
      "Loss:0.017937609925866127\n",
      "Epoch:5 \n",
      "Iteration:64 \n",
      "Loss:0.044574931263923645\n",
      "Epoch:5 \n",
      "Iteration:65 \n",
      "Loss:0.05748237669467926\n",
      "Epoch:5 \n",
      "Iteration:66 \n",
      "Loss:0.022236211225390434\n",
      "Epoch:5 \n",
      "Iteration:67 \n",
      "Loss:0.006080453284084797\n",
      "Epoch:5 \n",
      "Iteration:68 \n",
      "Loss:0.025870464742183685\n",
      "Epoch:5 \n",
      "Iteration:69 \n",
      "Loss:0.003083490766584873\n",
      "Epoch:5 \n",
      "Iteration:70 \n",
      "Loss:0.07590921223163605\n",
      "Epoch:5 \n",
      "Iteration:71 \n",
      "Loss:0.033865828067064285\n",
      "Epoch:5 \n",
      "Iteration:72 \n",
      "Loss:0.015424821525812149\n",
      "Epoch:5 \n",
      "Iteration:73 \n",
      "Loss:0.014791633002460003\n",
      "Epoch:5 \n",
      "Iteration:74 \n",
      "Loss:0.0077199786901474\n",
      "Epoch:5 \n",
      "Iteration:75 \n",
      "Loss:0.012633204460144043\n",
      "Epoch:5 \n",
      "Iteration:76 \n",
      "Loss:0.022363949567079544\n",
      "Epoch:5 \n",
      "Iteration:77 \n",
      "Loss:0.020985035225749016\n",
      "Epoch:5 \n",
      "Iteration:78 \n",
      "Loss:0.0016782167367637157\n",
      "Epoch:5 \n",
      "Iteration:79 \n",
      "Loss:0.08685922622680664\n",
      "Epoch:5 \n",
      "Iteration:80 \n",
      "Loss:0.07231438905000687\n",
      "Epoch:5 \n",
      "Iteration:81 \n",
      "Loss:0.041871003806591034\n",
      "Epoch:5 \n",
      "Iteration:82 \n",
      "Loss:0.00772493053227663\n",
      "Epoch:5 \n",
      "Iteration:83 \n",
      "Loss:0.056029416620731354\n",
      "Epoch:5 \n",
      "Iteration:84 \n",
      "Loss:0.005039834417402744\n",
      "Epoch:5 \n",
      "Iteration:85 \n",
      "Loss:0.0009520672028884292\n",
      "Epoch:5 \n",
      "Iteration:86 \n",
      "Loss:0.0017091198824346066\n",
      "Epoch:5 \n",
      "Iteration:87 \n",
      "Loss:0.02866588905453682\n",
      "Epoch:5 \n",
      "Iteration:88 \n",
      "Loss:0.0071078138425946236\n",
      "Epoch:5 \n",
      "Iteration:89 \n",
      "Loss:0.06789544224739075\n",
      "Epoch:5 \n",
      "Iteration:90 \n",
      "Loss:0.0069314269348979\n",
      "Epoch:5 \n",
      "Iteration:91 \n",
      "Loss:0.017479751259088516\n",
      "Epoch:5 \n",
      "Iteration:92 \n",
      "Loss:0.011844546534121037\n",
      "Epoch:5 \n",
      "Iteration:93 \n",
      "Loss:0.11414621025323868\n",
      "Epoch:5 \n",
      "Iteration:94 \n",
      "Loss:0.11900026351213455\n",
      "Epoch:5 \n",
      "Iteration:95 \n",
      "Loss:0.0513598769903183\n",
      "Epoch:5 \n",
      "Iteration:96 \n",
      "Loss:0.02156989835202694\n",
      "Epoch:5 \n",
      "Iteration:97 \n",
      "Loss:0.10540921241044998\n",
      "Epoch:5 \n",
      "Iteration:98 \n",
      "Loss:0.004759255796670914\n",
      "Epoch:5 \n",
      "Iteration:99 \n",
      "Loss:0.0017172759398818016\n",
      "Epoch:5 \n",
      "Iteration:100 \n",
      "Loss:0.018207835033535957\n",
      "Epoch:5 \n",
      "Iteration:101 \n",
      "Loss:0.05174718797206879\n",
      "Epoch:5 \n",
      "Iteration:102 \n",
      "Loss:0.0614677295088768\n",
      "Epoch:5 \n",
      "Iteration:103 \n",
      "Loss:0.11644528061151505\n",
      "Epoch:5 \n",
      "Iteration:104 \n",
      "Loss:0.062433015555143356\n",
      "Epoch:5 \n",
      "Iteration:105 \n",
      "Loss:0.008429351262748241\n",
      "Epoch:5 \n",
      "Iteration:106 \n",
      "Loss:0.01928906887769699\n",
      "Epoch:5 \n",
      "Iteration:107 \n",
      "Loss:0.015428189188241959\n",
      "Epoch:5 \n",
      "Iteration:108 \n",
      "Loss:0.029354562982916832\n",
      "Epoch:5 \n",
      "Iteration:109 \n",
      "Loss:0.025937790051102638\n",
      "Epoch:5 \n",
      "Iteration:110 \n",
      "Loss:0.056165728718042374\n",
      "Epoch:5 \n",
      "Iteration:111 \n",
      "Loss:0.05902191251516342\n",
      "Epoch:5 \n",
      "Iteration:112 \n",
      "Loss:0.008383464068174362\n",
      "Epoch:5 \n",
      "Iteration:113 \n",
      "Loss:0.007031195797026157\n",
      "Epoch:5 \n",
      "Iteration:114 \n",
      "Loss:0.029090743511915207\n",
      "Epoch:5 \n",
      "Iteration:115 \n",
      "Loss:0.09299366176128387\n",
      "Epoch:5 \n",
      "Iteration:116 \n",
      "Loss:0.021445002406835556\n",
      "Epoch:5 \n",
      "Iteration:117 \n",
      "Loss:0.011151851154863834\n",
      "Epoch:5 \n",
      "Iteration:118 \n",
      "Loss:0.049888186156749725\n",
      "Epoch:5 \n",
      "Iteration:119 \n",
      "Loss:0.05277197062969208\n",
      "Epoch:5 \n",
      "Iteration:120 \n",
      "Loss:0.012252547778189182\n",
      "Epoch:5 \n",
      "Iteration:121 \n",
      "Loss:0.05729968845844269\n",
      "Epoch:5 \n",
      "Iteration:122 \n",
      "Loss:0.013007535599172115\n",
      "Epoch:5 \n",
      "Iteration:123 \n",
      "Loss:0.030860135331749916\n",
      "Epoch:5 \n",
      "Iteration:124 \n",
      "Loss:0.010219590738415718\n",
      "Epoch:5 \n",
      "Iteration:125 \n",
      "Loss:0.01604657992720604\n",
      "Epoch:5 \n",
      "Iteration:126 \n",
      "Loss:0.014644069597125053\n",
      "Epoch:5 \n",
      "Iteration:127 \n",
      "Loss:0.03249460831284523\n",
      "Epoch:5 \n",
      "Iteration:128 \n",
      "Loss:0.02780020423233509\n",
      "Epoch:5 \n",
      "Iteration:129 \n",
      "Loss:0.013386514969170094\n",
      "Epoch:5 \n",
      "Iteration:130 \n",
      "Loss:0.05896317958831787\n",
      "Epoch:5 \n",
      "Iteration:131 \n",
      "Loss:0.007848761044442654\n",
      "Epoch:5 \n",
      "Iteration:132 \n",
      "Loss:0.00525230448693037\n",
      "Epoch:5 \n",
      "Iteration:133 \n",
      "Loss:0.025292033329606056\n",
      "Epoch:5 \n",
      "Iteration:134 \n",
      "Loss:0.09321035444736481\n",
      "Epoch:5 \n",
      "Iteration:135 \n",
      "Loss:0.014823350124061108\n",
      "Epoch:5 \n",
      "Iteration:136 \n",
      "Loss:0.03575638681650162\n",
      "Epoch:5 \n",
      "Iteration:137 \n",
      "Loss:0.09268287569284439\n",
      "Epoch:5 \n",
      "Iteration:138 \n",
      "Loss:0.00418532220646739\n",
      "Epoch:5 \n",
      "Iteration:139 \n",
      "Loss:0.005711697973310947\n",
      "Epoch:5 \n",
      "Iteration:140 \n",
      "Loss:0.017858222126960754\n",
      "Epoch:5 \n",
      "Iteration:141 \n",
      "Loss:0.008881667628884315\n",
      "Epoch:5 \n",
      "Iteration:142 \n",
      "Loss:0.023469991981983185\n",
      "Epoch:5 \n",
      "Iteration:143 \n",
      "Loss:0.031435318291187286\n",
      "Epoch:5 \n",
      "Iteration:144 \n",
      "Loss:0.047724198549985886\n",
      "Epoch:5 \n",
      "Iteration:145 \n",
      "Loss:0.0122687928378582\n",
      "Epoch:5 \n",
      "Iteration:146 \n",
      "Loss:0.01102321594953537\n",
      "Epoch:5 \n",
      "Iteration:147 \n",
      "Loss:0.0547788143157959\n",
      "Epoch:5 \n",
      "Iteration:148 \n",
      "Loss:0.019800670444965363\n",
      "Epoch:5 \n",
      "Iteration:149 \n",
      "Loss:0.007703104056417942\n",
      "Epoch:5 \n",
      "Iteration:150 \n",
      "Loss:0.03032558225095272\n",
      "Epoch:5 \n",
      "Iteration:151 \n",
      "Loss:0.007082965224981308\n",
      "Epoch:5 \n",
      "Iteration:152 \n",
      "Loss:0.08963114768266678\n",
      "Epoch:5 \n",
      "Iteration:153 \n",
      "Loss:0.011886557564139366\n",
      "Epoch:5 \n",
      "Iteration:154 \n",
      "Loss:0.005273689050227404\n",
      "Epoch:5 \n",
      "Iteration:155 \n",
      "Loss:0.0060187410563230515\n",
      "Epoch:5 \n",
      "Iteration:156 \n",
      "Loss:0.0047499011270701885\n",
      "Epoch:5 \n",
      "Iteration:157 \n",
      "Loss:0.001847783220000565\n",
      "Epoch:5 \n",
      "Iteration:158 \n",
      "Loss:0.006017403211444616\n",
      "Epoch:5 \n",
      "Iteration:159 \n",
      "Loss:0.03088475950062275\n",
      "Epoch:5 \n",
      "Iteration:160 \n",
      "Loss:0.022606918588280678\n",
      "Epoch:5 \n",
      "Iteration:161 \n",
      "Loss:0.04354383051395416\n",
      "Epoch:5 \n",
      "Iteration:162 \n",
      "Loss:0.053142450749874115\n",
      "Epoch:5 \n",
      "Iteration:163 \n",
      "Loss:0.0013626903528347611\n",
      "Epoch:5 \n",
      "Iteration:164 \n",
      "Loss:0.0022888618987053633\n",
      "Epoch:5 \n",
      "Iteration:165 \n",
      "Loss:0.10601752996444702\n",
      "Epoch:5 \n",
      "Iteration:166 \n",
      "Loss:0.027201468124985695\n",
      "Epoch:5 \n",
      "Iteration:167 \n",
      "Loss:0.029203757643699646\n",
      "Epoch:5 \n",
      "Iteration:168 \n",
      "Loss:0.003371194237843156\n",
      "Epoch:5 \n",
      "Iteration:169 \n",
      "Loss:0.1367059350013733\n",
      "Epoch:5 \n",
      "Iteration:170 \n",
      "Loss:0.0333084836602211\n",
      "Epoch:5 \n",
      "Iteration:171 \n",
      "Loss:0.0029286888893693686\n",
      "Epoch:5 \n",
      "Iteration:172 \n",
      "Loss:0.043764401227235794\n",
      "Epoch:5 \n",
      "Iteration:173 \n",
      "Loss:0.022074062377214432\n",
      "Epoch:5 \n",
      "Iteration:174 \n",
      "Loss:0.03626113757491112\n",
      "Epoch:5 \n",
      "Iteration:175 \n",
      "Loss:0.027319753542542458\n",
      "Epoch:5 \n",
      "Iteration:176 \n",
      "Loss:0.10259804874658585\n",
      "Epoch:5 \n",
      "Iteration:177 \n",
      "Loss:0.07937386631965637\n",
      "Epoch:5 \n",
      "Iteration:178 \n",
      "Loss:0.0366901233792305\n",
      "Epoch:5 \n",
      "Iteration:179 \n",
      "Loss:0.01942426711320877\n",
      "Epoch:5 \n",
      "Iteration:180 \n",
      "Loss:0.021640989929437637\n",
      "Epoch:5 \n",
      "Iteration:181 \n",
      "Loss:0.021282339468598366\n",
      "Epoch:5 \n",
      "Iteration:182 \n",
      "Loss:0.12226741760969162\n",
      "Epoch:5 \n",
      "Iteration:183 \n",
      "Loss:0.0194413885474205\n",
      "Epoch:5 \n",
      "Iteration:184 \n",
      "Loss:0.022255633026361465\n",
      "Epoch:5 \n",
      "Iteration:185 \n",
      "Loss:0.04461973160505295\n",
      "Epoch:5 \n",
      "Iteration:186 \n",
      "Loss:0.05878574401140213\n",
      "Epoch:5 \n",
      "Iteration:187 \n",
      "Loss:0.10168582946062088\n",
      "Epoch:5 \n",
      "Iteration:188 \n",
      "Loss:0.15556085109710693\n",
      "Epoch:5 \n",
      "Iteration:189 \n",
      "Loss:0.04161043092608452\n",
      "Epoch:5 \n",
      "Iteration:190 \n",
      "Loss:0.02827397733926773\n",
      "Epoch:5 \n",
      "Iteration:191 \n",
      "Loss:0.006266253534704447\n",
      "Epoch:5 \n",
      "Iteration:192 \n",
      "Loss:0.03830084949731827\n",
      "Epoch:5 \n",
      "Iteration:193 \n",
      "Loss:0.01181009691208601\n",
      "Epoch:5 \n",
      "Iteration:194 \n",
      "Loss:0.00519329309463501\n",
      "Epoch:5 \n",
      "Iteration:195 \n",
      "Loss:0.09154728055000305\n",
      "Epoch:5 \n",
      "Iteration:196 \n",
      "Loss:0.011460389941930771\n",
      "Epoch:5 \n",
      "Iteration:197 \n",
      "Loss:0.0032536129001528025\n",
      "Epoch:5 \n",
      "Iteration:198 \n",
      "Loss:0.04917233809828758\n",
      "Epoch:5 \n",
      "Iteration:199 \n",
      "Loss:0.06639426201581955\n",
      "Epoch:5 \n",
      "Iteration:200 \n",
      "Loss:0.03480757400393486\n",
      "Epoch:5 \n",
      "Iteration:201 \n",
      "Loss:0.0013720226706936955\n",
      "Epoch:5 \n",
      "Iteration:202 \n",
      "Loss:0.0681237056851387\n",
      "Epoch:5 \n",
      "Iteration:203 \n",
      "Loss:0.01267420407384634\n",
      "Epoch:5 \n",
      "Iteration:204 \n",
      "Loss:0.05429048836231232\n",
      "Epoch:5 \n",
      "Iteration:205 \n",
      "Loss:0.1858130395412445\n",
      "Epoch:5 \n",
      "Iteration:206 \n",
      "Loss:0.07387594878673553\n",
      "Epoch:5 \n",
      "Iteration:207 \n",
      "Loss:0.0027198835741728544\n",
      "Epoch:5 \n",
      "Iteration:208 \n",
      "Loss:0.026393337175250053\n",
      "Epoch:5 \n",
      "Iteration:209 \n",
      "Loss:0.007987418211996555\n",
      "Epoch:5 \n",
      "Iteration:210 \n",
      "Loss:0.06281453371047974\n",
      "Epoch:5 \n",
      "Iteration:211 \n",
      "Loss:0.04198765382170677\n",
      "Epoch:5 \n",
      "Iteration:212 \n",
      "Loss:0.015796007588505745\n",
      "Epoch:5 \n",
      "Iteration:213 \n",
      "Loss:0.025341691449284554\n",
      "Epoch:5 \n",
      "Iteration:214 \n",
      "Loss:0.04837237298488617\n",
      "Epoch:5 \n",
      "Iteration:215 \n",
      "Loss:0.05099866911768913\n",
      "Epoch:5 \n",
      "Iteration:216 \n",
      "Loss:0.10531184077262878\n",
      "Epoch:5 \n",
      "Iteration:217 \n",
      "Loss:0.020571578294038773\n",
      "Epoch:5 \n",
      "Iteration:218 \n",
      "Loss:0.0203409343957901\n",
      "Epoch:5 \n",
      "Iteration:219 \n",
      "Loss:0.002193605527281761\n",
      "Epoch:5 \n",
      "Iteration:220 \n",
      "Loss:0.01738307811319828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5 \n",
      "Iteration:221 \n",
      "Loss:0.02719496563076973\n",
      "Epoch:5 \n",
      "Iteration:222 \n",
      "Loss:0.011695698834955692\n",
      "Epoch:5 \n",
      "Iteration:223 \n",
      "Loss:0.045908451080322266\n",
      "Epoch:5 \n",
      "Iteration:224 \n",
      "Loss:0.056271396577358246\n",
      "Epoch:5 \n",
      "Iteration:225 \n",
      "Loss:0.0629812702536583\n",
      "Epoch:5 \n",
      "Iteration:226 \n",
      "Loss:0.005832304246723652\n",
      "Epoch:5 \n",
      "Iteration:227 \n",
      "Loss:0.01732846349477768\n",
      "Epoch:5 \n",
      "Iteration:228 \n",
      "Loss:0.04719158634543419\n",
      "Epoch:5 \n",
      "Iteration:229 \n",
      "Loss:0.02598273567855358\n",
      "Epoch:5 \n",
      "Iteration:230 \n",
      "Loss:0.020170358940958977\n",
      "Epoch:5 \n",
      "Iteration:231 \n",
      "Loss:0.009102598764002323\n",
      "Epoch:5 \n",
      "Iteration:232 \n",
      "Loss:0.004015134647488594\n",
      "Epoch:5 \n",
      "Iteration:233 \n",
      "Loss:0.05322112888097763\n",
      "Epoch:5 \n",
      "Iteration:234 \n",
      "Loss:0.0249424297362566\n",
      "Epoch:5 \n",
      "Iteration:235 \n",
      "Loss:0.008256280794739723\n",
      "Epoch:5 \n",
      "Iteration:236 \n",
      "Loss:0.11711681634187698\n",
      "Epoch:5 \n",
      "Iteration:237 \n",
      "Loss:0.012890856713056564\n",
      "Epoch:5 \n",
      "Iteration:238 \n",
      "Loss:0.021939408034086227\n",
      "Epoch:5 \n",
      "Iteration:239 \n",
      "Loss:0.0330989845097065\n",
      "Epoch:5 \n",
      "Iteration:240 \n",
      "Loss:0.014909918420016766\n",
      "Epoch:5 \n",
      "Iteration:241 \n",
      "Loss:0.042488984763622284\n",
      "Epoch:5 \n",
      "Iteration:242 \n",
      "Loss:0.0034189862199127674\n",
      "Epoch:5 \n",
      "Iteration:243 \n",
      "Loss:0.027727391570806503\n",
      "Epoch:5 \n",
      "Iteration:244 \n",
      "Loss:0.005130380392074585\n",
      "Epoch:5 \n",
      "Iteration:245 \n",
      "Loss:0.025456123054027557\n",
      "Epoch:5 \n",
      "Iteration:246 \n",
      "Loss:0.03983313962817192\n",
      "Epoch:5 \n",
      "Iteration:247 \n",
      "Loss:0.04004927724599838\n",
      "Epoch:5 \n",
      "Iteration:248 \n",
      "Loss:0.10825648158788681\n",
      "Epoch:5 \n",
      "Iteration:249 \n",
      "Loss:0.018194502219557762\n",
      "Epoch:5 \n",
      "Iteration:250 \n",
      "Loss:0.019829368218779564\n",
      "Epoch:5 \n",
      "Iteration:251 \n",
      "Loss:0.15166132152080536\n",
      "Epoch:5 \n",
      "Iteration:252 \n",
      "Loss:0.007216374855488539\n",
      "Epoch:5 \n",
      "Iteration:253 \n",
      "Loss:0.021638037636876106\n",
      "Epoch:5 \n",
      "Iteration:254 \n",
      "Loss:0.0780341699719429\n",
      "Epoch:5 \n",
      "Iteration:255 \n",
      "Loss:0.02714572474360466\n",
      "Epoch:5 \n",
      "Iteration:256 \n",
      "Loss:0.010995647870004177\n",
      "Epoch:5 \n",
      "Iteration:257 \n",
      "Loss:0.016640473157167435\n",
      "Epoch:5 \n",
      "Iteration:258 \n",
      "Loss:0.024537472054362297\n",
      "Epoch:5 \n",
      "Iteration:259 \n",
      "Loss:0.056220997124910355\n",
      "Epoch:5 \n",
      "Iteration:260 \n",
      "Loss:0.014782808721065521\n",
      "Epoch:5 \n",
      "Iteration:261 \n",
      "Loss:0.04132203012704849\n",
      "Epoch:5 \n",
      "Iteration:262 \n",
      "Loss:0.05179990828037262\n",
      "Epoch:5 \n",
      "Iteration:263 \n",
      "Loss:0.04839722067117691\n",
      "Epoch:5 \n",
      "Iteration:264 \n",
      "Loss:0.08021727204322815\n",
      "Epoch:5 \n",
      "Iteration:265 \n",
      "Loss:0.22404035925865173\n",
      "Epoch:5 \n",
      "Iteration:266 \n",
      "Loss:0.004304721485823393\n",
      "Epoch:5 \n",
      "Iteration:267 \n",
      "Loss:0.009333536028862\n",
      "Epoch:5 \n",
      "Iteration:268 \n",
      "Loss:0.04572006314992905\n",
      "Epoch:5 \n",
      "Iteration:269 \n",
      "Loss:0.0029876308981329203\n",
      "Epoch:5 \n",
      "Iteration:270 \n",
      "Loss:0.06083708256483078\n",
      "Epoch:5 \n",
      "Iteration:271 \n",
      "Loss:0.06141042709350586\n",
      "Epoch:5 \n",
      "Iteration:272 \n",
      "Loss:0.09740705788135529\n",
      "Epoch:5 \n",
      "Iteration:273 \n",
      "Loss:0.04087788984179497\n",
      "Epoch:5 \n",
      "Iteration:274 \n",
      "Loss:0.04284539818763733\n",
      "Epoch:5 \n",
      "Iteration:275 \n",
      "Loss:0.08911549299955368\n",
      "Epoch:5 \n",
      "Iteration:276 \n",
      "Loss:0.054368551820516586\n",
      "Epoch:5 \n",
      "Iteration:277 \n",
      "Loss:0.051383983343839645\n",
      "Epoch:5 \n",
      "Iteration:278 \n",
      "Loss:0.043430395424366\n",
      "Epoch:5 \n",
      "Iteration:279 \n",
      "Loss:0.021310005336999893\n",
      "Epoch:5 \n",
      "Iteration:280 \n",
      "Loss:0.023348279297351837\n",
      "Epoch:5 \n",
      "Iteration:281 \n",
      "Loss:0.04596295952796936\n",
      "Epoch:5 \n",
      "Iteration:282 \n",
      "Loss:0.006715267896652222\n",
      "Epoch:5 \n",
      "Iteration:283 \n",
      "Loss:0.004026536364108324\n",
      "Epoch:5 \n",
      "Iteration:284 \n",
      "Loss:0.028221596032381058\n",
      "Epoch:5 \n",
      "Iteration:285 \n",
      "Loss:0.080050989985466\n",
      "Epoch:5 \n",
      "Iteration:286 \n",
      "Loss:0.0425339974462986\n",
      "Epoch:5 \n",
      "Iteration:287 \n",
      "Loss:0.006741777062416077\n",
      "Epoch:5 \n",
      "Iteration:288 \n",
      "Loss:0.023670291528105736\n",
      "Epoch:5 \n",
      "Iteration:289 \n",
      "Loss:0.022853979840874672\n",
      "Epoch:5 \n",
      "Iteration:290 \n",
      "Loss:0.0547621063888073\n",
      "Epoch:5 \n",
      "Iteration:291 \n",
      "Loss:0.03657141700387001\n",
      "Epoch:5 \n",
      "Iteration:292 \n",
      "Loss:0.028854265809059143\n",
      "Epoch:5 \n",
      "Iteration:293 \n",
      "Loss:0.014157739467918873\n",
      "Epoch:5 \n",
      "Iteration:294 \n",
      "Loss:0.048996973782777786\n",
      "Epoch:5 \n",
      "Iteration:295 \n",
      "Loss:0.07205921411514282\n",
      "Epoch:5 \n",
      "Iteration:296 \n",
      "Loss:0.07057604938745499\n",
      "Epoch:5 \n",
      "Iteration:297 \n",
      "Loss:0.03098134696483612\n",
      "Epoch:5 \n",
      "Iteration:298 \n",
      "Loss:0.014943258836865425\n",
      "Epoch:5 \n",
      "Iteration:299 \n",
      "Loss:0.005203001666814089\n",
      "Epoch:5 \n",
      "Iteration:300 \n",
      "Loss:0.09293577075004578\n",
      "Epoch:5 \n",
      "Iteration:301 \n",
      "Loss:0.179232656955719\n",
      "Epoch:5 \n",
      "Iteration:302 \n",
      "Loss:0.0018876743270084262\n",
      "Epoch:5 \n",
      "Iteration:303 \n",
      "Loss:0.010010268539190292\n",
      "Epoch:5 \n",
      "Iteration:304 \n",
      "Loss:0.014469531364738941\n",
      "Epoch:5 \n",
      "Iteration:305 \n",
      "Loss:0.025282375514507294\n",
      "Epoch:5 \n",
      "Iteration:306 \n",
      "Loss:0.015078680589795113\n",
      "Epoch:5 \n",
      "Iteration:307 \n",
      "Loss:0.019649401307106018\n",
      "Epoch:5 \n",
      "Iteration:308 \n",
      "Loss:0.007615830283612013\n",
      "Epoch:5 \n",
      "Iteration:309 \n",
      "Loss:0.003454160410910845\n",
      "Epoch:5 \n",
      "Iteration:310 \n",
      "Loss:0.048382796347141266\n",
      "Epoch:5 \n",
      "Iteration:311 \n",
      "Loss:0.04584045708179474\n",
      "Epoch:5 \n",
      "Iteration:312 \n",
      "Loss:0.043960265815258026\n",
      "Epoch:5 \n",
      "Iteration:313 \n",
      "Loss:0.05136370658874512\n",
      "Epoch:5 \n",
      "Iteration:314 \n",
      "Loss:0.014082192443311214\n",
      "Epoch:5 \n",
      "Iteration:315 \n",
      "Loss:0.047573547810316086\n",
      "Epoch:5 \n",
      "Iteration:316 \n",
      "Loss:0.012408256530761719\n",
      "Epoch:5 \n",
      "Iteration:317 \n",
      "Loss:0.06963067501783371\n",
      "Epoch:5 \n",
      "Iteration:318 \n",
      "Loss:0.02974516898393631\n",
      "Epoch:5 \n",
      "Iteration:319 \n",
      "Loss:0.05822201818227768\n",
      "Epoch:5 \n",
      "Iteration:320 \n",
      "Loss:0.08304069191217422\n",
      "Epoch:5 \n",
      "Iteration:321 \n",
      "Loss:0.030409004539251328\n",
      "Epoch:5 \n",
      "Iteration:322 \n",
      "Loss:0.07946910709142685\n",
      "Epoch:5 \n",
      "Iteration:323 \n",
      "Loss:0.04857013374567032\n",
      "Epoch:5 \n",
      "Iteration:324 \n",
      "Loss:0.10957178473472595\n",
      "Epoch:5 \n",
      "Iteration:325 \n",
      "Loss:0.009823182597756386\n",
      "Epoch:5 \n",
      "Iteration:326 \n",
      "Loss:0.005253791343420744\n",
      "Epoch:5 \n",
      "Iteration:327 \n",
      "Loss:0.10247427970170975\n",
      "Epoch:5 \n",
      "Iteration:328 \n",
      "Loss:0.05277135223150253\n",
      "Epoch:5 \n",
      "Iteration:329 \n",
      "Loss:0.007127128075808287\n",
      "Epoch:5 \n",
      "Iteration:330 \n",
      "Loss:0.04397985339164734\n",
      "Epoch:5 \n",
      "Iteration:331 \n",
      "Loss:0.016715778037905693\n",
      "Epoch:5 \n",
      "Iteration:332 \n",
      "Loss:0.04088862985372543\n",
      "Epoch:5 \n",
      "Iteration:333 \n",
      "Loss:0.17076756060123444\n",
      "Epoch:5 \n",
      "Iteration:334 \n",
      "Loss:0.010469253174960613\n",
      "Epoch:5 \n",
      "Iteration:335 \n",
      "Loss:0.00333763868547976\n",
      "Epoch:5 \n",
      "Iteration:336 \n",
      "Loss:0.05751198157668114\n",
      "Epoch:5 \n",
      "Iteration:337 \n",
      "Loss:0.0816745012998581\n",
      "Epoch:5 \n",
      "Iteration:338 \n",
      "Loss:0.0029237940907478333\n",
      "Epoch:5 \n",
      "Iteration:339 \n",
      "Loss:0.05111284554004669\n",
      "Epoch:5 \n",
      "Iteration:340 \n",
      "Loss:0.09387700259685516\n",
      "Epoch:5 \n",
      "Iteration:341 \n",
      "Loss:0.03929232060909271\n",
      "Epoch:5 \n",
      "Iteration:342 \n",
      "Loss:0.00708002271130681\n",
      "Epoch:5 \n",
      "Iteration:343 \n",
      "Loss:0.062146686017513275\n",
      "Epoch:5 \n",
      "Iteration:344 \n",
      "Loss:0.013858905993402004\n",
      "Epoch:5 \n",
      "Iteration:345 \n",
      "Loss:0.09932363778352737\n",
      "Epoch:5 \n",
      "Iteration:346 \n",
      "Loss:0.03540554642677307\n",
      "Epoch:5 \n",
      "Iteration:347 \n",
      "Loss:0.013138684444129467\n",
      "Epoch:5 \n",
      "Iteration:348 \n",
      "Loss:0.12957780063152313\n",
      "Epoch:5 \n",
      "Iteration:349 \n",
      "Loss:0.015373778529465199\n",
      "Epoch:5 \n",
      "Iteration:350 \n",
      "Loss:0.05264896899461746\n",
      "Epoch:5 \n",
      "Iteration:351 \n",
      "Loss:0.08456496149301529\n",
      "Epoch:5 \n",
      "Iteration:352 \n",
      "Loss:0.006332049146294594\n",
      "Epoch:5 \n",
      "Iteration:353 \n",
      "Loss:0.046433161944150925\n",
      "Epoch:5 \n",
      "Iteration:354 \n",
      "Loss:0.043002448976039886\n",
      "Epoch:5 \n",
      "Iteration:355 \n",
      "Loss:0.028477050364017487\n",
      "Epoch:5 \n",
      "Iteration:356 \n",
      "Loss:0.12587451934814453\n",
      "Epoch:5 \n",
      "Iteration:357 \n",
      "Loss:0.10533547401428223\n",
      "Epoch:5 \n",
      "Iteration:358 \n",
      "Loss:0.06571564823389053\n",
      "Epoch:5 \n",
      "Iteration:359 \n",
      "Loss:0.019195809960365295\n",
      "Epoch:5 \n",
      "Iteration:360 \n",
      "Loss:0.045375071465969086\n",
      "Epoch:5 \n",
      "Iteration:361 \n",
      "Loss:0.10289410501718521\n",
      "Epoch:5 \n",
      "Iteration:362 \n",
      "Loss:0.028914734721183777\n",
      "Epoch:5 \n",
      "Iteration:363 \n",
      "Loss:0.008770766668021679\n",
      "Epoch:5 \n",
      "Iteration:364 \n",
      "Loss:0.1241755411028862\n",
      "Epoch:5 \n",
      "Iteration:365 \n",
      "Loss:0.00492456741631031\n",
      "Epoch:5 \n",
      "Iteration:366 \n",
      "Loss:0.06998497992753983\n",
      "Epoch:5 \n",
      "Iteration:367 \n",
      "Loss:0.025064587593078613\n",
      "Epoch:5 \n",
      "Iteration:368 \n",
      "Loss:0.10906746983528137\n",
      "Epoch:5 \n",
      "Iteration:369 \n",
      "Loss:0.03760945424437523\n",
      "Epoch:5 \n",
      "Iteration:370 \n",
      "Loss:0.008554023690521717\n",
      "Epoch:5 \n",
      "Iteration:371 \n",
      "Loss:0.1067260280251503\n",
      "Epoch:5 \n",
      "Iteration:372 \n",
      "Loss:0.02043270133435726\n",
      "Epoch:5 \n",
      "Iteration:373 \n",
      "Loss:0.042824264615774155\n",
      "Epoch:5 \n",
      "Iteration:374 \n",
      "Loss:0.07117931544780731\n",
      "Epoch:5 \n",
      "Iteration:375 \n",
      "Loss:0.023089684545993805\n",
      "Epoch:5 \n",
      "Iteration:376 \n",
      "Loss:0.028763189911842346\n",
      "Epoch:5 \n",
      "Iteration:377 \n",
      "Loss:0.024829117581248283\n",
      "Epoch:5 \n",
      "Iteration:378 \n",
      "Loss:0.0016516378382220864\n",
      "Epoch:5 \n",
      "Iteration:379 \n",
      "Loss:0.02502274513244629\n",
      "Epoch:5 \n",
      "Iteration:380 \n",
      "Loss:0.17466101050376892\n",
      "Epoch:5 \n",
      "Iteration:381 \n",
      "Loss:0.023437142372131348\n",
      "Epoch:5 \n",
      "Iteration:382 \n",
      "Loss:0.03697950392961502\n",
      "Epoch:5 \n",
      "Iteration:383 \n",
      "Loss:0.1192232295870781\n",
      "Epoch:5 \n",
      "Iteration:384 \n",
      "Loss:0.10674715042114258\n",
      "Epoch:5 \n",
      "Iteration:385 \n",
      "Loss:0.06372912228107452\n",
      "Epoch:5 \n",
      "Iteration:386 \n",
      "Loss:0.029676303267478943\n",
      "Epoch:5 \n",
      "Iteration:387 \n",
      "Loss:0.039401840418577194\n",
      "Epoch:5 \n",
      "Iteration:388 \n",
      "Loss:0.04313652962446213\n",
      "Epoch:5 \n",
      "Iteration:389 \n",
      "Loss:0.04242105409502983\n",
      "Epoch:5 \n",
      "Iteration:390 \n",
      "Loss:0.046509504318237305\n",
      "Epoch:5 \n",
      "Iteration:391 \n",
      "Loss:0.012387670576572418\n",
      "Epoch:5 \n",
      "Iteration:392 \n",
      "Loss:0.004667161498218775\n",
      "Epoch:5 \n",
      "Iteration:393 \n",
      "Loss:0.00928991287946701\n",
      "Epoch:5 \n",
      "Iteration:394 \n",
      "Loss:0.08838345855474472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5 \n",
      "Iteration:395 \n",
      "Loss:0.024059347808361053\n",
      "Epoch:5 \n",
      "Iteration:396 \n",
      "Loss:0.05232882499694824\n",
      "Epoch:5 \n",
      "Iteration:397 \n",
      "Loss:0.07347489148378372\n",
      "Epoch:5 \n",
      "Iteration:398 \n",
      "Loss:0.04029689356684685\n",
      "Epoch:5 \n",
      "Iteration:399 \n",
      "Loss:0.051091793924570084\n",
      "Epoch:5 \n",
      "Iteration:400 \n",
      "Loss:0.08678067475557327\n",
      "Epoch:5 \n",
      "Iteration:401 \n",
      "Loss:0.005495588295161724\n",
      "Epoch:5 \n",
      "Iteration:402 \n",
      "Loss:0.05553276836872101\n",
      "Epoch:5 \n",
      "Iteration:403 \n",
      "Loss:0.03457173332571983\n",
      "Epoch:5 \n",
      "Iteration:404 \n",
      "Loss:0.031582266092300415\n",
      "Epoch:5 \n",
      "Iteration:405 \n",
      "Loss:0.04680706933140755\n",
      "Epoch:5 \n",
      "Iteration:406 \n",
      "Loss:0.054624851793050766\n",
      "Epoch:5 \n",
      "Iteration:407 \n",
      "Loss:0.05530458316206932\n",
      "Epoch:5 \n",
      "Iteration:408 \n",
      "Loss:0.06197701394557953\n",
      "Epoch:5 \n",
      "Iteration:409 \n",
      "Loss:0.0868837758898735\n",
      "Epoch:5 \n",
      "Iteration:410 \n",
      "Loss:0.11450186371803284\n",
      "Epoch:5 \n",
      "Iteration:411 \n",
      "Loss:0.03578885272145271\n",
      "Epoch:5 \n",
      "Iteration:412 \n",
      "Loss:0.0013503247173503041\n",
      "Epoch:5 \n",
      "Iteration:413 \n",
      "Loss:0.031604208052158356\n",
      "Epoch:5 \n",
      "Iteration:414 \n",
      "Loss:0.0172459464520216\n",
      "Epoch:5 \n",
      "Iteration:415 \n",
      "Loss:0.12543347477912903\n",
      "Epoch:5 \n",
      "Iteration:416 \n",
      "Loss:0.0702885314822197\n",
      "Epoch:5 \n",
      "Iteration:417 \n",
      "Loss:0.03554667532444\n",
      "Epoch:5 \n",
      "Iteration:418 \n",
      "Loss:0.08244433999061584\n",
      "Epoch:5 \n",
      "Iteration:419 \n",
      "Loss:0.04671969264745712\n",
      "Epoch:5 \n",
      "Iteration:420 \n",
      "Loss:0.05437220633029938\n",
      "Epoch:5 \n",
      "Iteration:421 \n",
      "Loss:0.05023599788546562\n",
      "Epoch:5 \n",
      "Iteration:422 \n",
      "Loss:0.06934364140033722\n",
      "Epoch:5 \n",
      "Iteration:423 \n",
      "Loss:0.041576795279979706\n",
      "Epoch:5 \n",
      "Iteration:424 \n",
      "Loss:0.013442312367260456\n",
      "Epoch:5 \n",
      "Iteration:425 \n",
      "Loss:0.06820803880691528\n",
      "Epoch:5 \n",
      "Iteration:426 \n",
      "Loss:0.0948287770152092\n",
      "Epoch:5 \n",
      "Iteration:427 \n",
      "Loss:0.08503484725952148\n",
      "Epoch:5 \n",
      "Iteration:428 \n",
      "Loss:0.04229574277997017\n",
      "Epoch:5 \n",
      "Iteration:429 \n",
      "Loss:0.025745704770088196\n",
      "Epoch:5 \n",
      "Iteration:430 \n",
      "Loss:0.032654061913490295\n",
      "Epoch:5 \n",
      "Iteration:431 \n",
      "Loss:0.01710253767669201\n",
      "Epoch:5 \n",
      "Iteration:432 \n",
      "Loss:0.020856276154518127\n",
      "Epoch:5 \n",
      "Iteration:433 \n",
      "Loss:0.0195842906832695\n",
      "Epoch:5 \n",
      "Iteration:434 \n",
      "Loss:0.04392298310995102\n",
      "Epoch:5 \n",
      "Iteration:435 \n",
      "Loss:0.07840470224618912\n",
      "Epoch:5 \n",
      "Iteration:436 \n",
      "Loss:0.038389235734939575\n",
      "Epoch:5 \n",
      "Iteration:437 \n",
      "Loss:0.03839346393942833\n",
      "Epoch:5 \n",
      "Iteration:438 \n",
      "Loss:0.08290597051382065\n",
      "Epoch:5 \n",
      "Iteration:439 \n",
      "Loss:0.02240535244345665\n",
      "Epoch:5 \n",
      "Iteration:440 \n",
      "Loss:0.055692605674266815\n",
      "Epoch:5 \n",
      "Iteration:441 \n",
      "Loss:0.027674470096826553\n",
      "Epoch:5 \n",
      "Iteration:442 \n",
      "Loss:0.022056199610233307\n",
      "Epoch:5 \n",
      "Iteration:443 \n",
      "Loss:0.026656026020646095\n",
      "Epoch:5 \n",
      "Iteration:444 \n",
      "Loss:0.028085336089134216\n",
      "Epoch:5 \n",
      "Iteration:445 \n",
      "Loss:0.011384980753064156\n",
      "Epoch:5 \n",
      "Iteration:446 \n",
      "Loss:0.05817548185586929\n",
      "Epoch:5 \n",
      "Iteration:447 \n",
      "Loss:0.04625573009252548\n",
      "Epoch:5 \n",
      "Iteration:448 \n",
      "Loss:0.03665824979543686\n",
      "Epoch:5 \n",
      "Iteration:449 \n",
      "Loss:0.043323710560798645\n",
      "Epoch:5 \n",
      "Iteration:450 \n",
      "Loss:0.017426228150725365\n",
      "Epoch:5 \n",
      "Iteration:451 \n",
      "Loss:0.0365583710372448\n",
      "Epoch:5 \n",
      "Iteration:452 \n",
      "Loss:0.05465990677475929\n",
      "Epoch:5 \n",
      "Iteration:453 \n",
      "Loss:0.07669070363044739\n",
      "Epoch:5 \n",
      "Iteration:454 \n",
      "Loss:0.09443493187427521\n",
      "Epoch:5 \n",
      "Iteration:455 \n",
      "Loss:0.08030182123184204\n",
      "Epoch:5 \n",
      "Iteration:456 \n",
      "Loss:0.04013628140091896\n",
      "Epoch:5 \n",
      "Iteration:457 \n",
      "Loss:0.06333023309707642\n",
      "Epoch:5 \n",
      "Iteration:458 \n",
      "Loss:0.041470542550086975\n",
      "Epoch:5 \n",
      "Iteration:459 \n",
      "Loss:0.010485414415597916\n",
      "Epoch:5 \n",
      "Iteration:460 \n",
      "Loss:0.024206167086958885\n",
      "Epoch:5 \n",
      "Iteration:461 \n",
      "Loss:0.007060531992465258\n",
      "Epoch:5 \n",
      "Iteration:462 \n",
      "Loss:0.01706262119114399\n",
      "Epoch:5 \n",
      "Iteration:463 \n",
      "Loss:0.19753897190093994\n",
      "Epoch:5 \n",
      "Iteration:464 \n",
      "Loss:0.04516153410077095\n",
      "Epoch:5 \n",
      "Iteration:465 \n",
      "Loss:0.06909158825874329\n",
      "Epoch:5 \n",
      "Iteration:466 \n",
      "Loss:0.03989846631884575\n",
      "Epoch:5 \n",
      "Iteration:467 \n",
      "Loss:0.03725603222846985\n",
      "Epoch:5 \n",
      "Iteration:468 \n",
      "Loss:0.03270873799920082\n",
      "Epoch:5 \n",
      "Iteration:469 \n",
      "Loss:0.12758269906044006\n",
      "Epoch:5 \n",
      "Iteration:470 \n",
      "Loss:0.02676538936793804\n",
      "Epoch:5 \n",
      "Iteration:471 \n",
      "Loss:0.14908429980278015\n",
      "Epoch:5 \n",
      "Iteration:472 \n",
      "Loss:0.036945320665836334\n",
      "Epoch:5 \n",
      "Iteration:473 \n",
      "Loss:0.031125718727707863\n",
      "Epoch:5 \n",
      "Iteration:474 \n",
      "Loss:0.03110155276954174\n",
      "Epoch:5 \n",
      "Iteration:475 \n",
      "Loss:0.024923808872699738\n",
      "Epoch:5 \n",
      "Iteration:476 \n",
      "Loss:0.008140339516103268\n",
      "Epoch:5 \n",
      "Iteration:477 \n",
      "Loss:0.013475959189236164\n",
      "Epoch:5 \n",
      "Iteration:478 \n",
      "Loss:0.022068817168474197\n",
      "Epoch:5 \n",
      "Iteration:479 \n",
      "Loss:0.021191684529185295\n",
      "Epoch:5 \n",
      "Iteration:480 \n",
      "Loss:0.10442979633808136\n",
      "Epoch:5 \n",
      "Iteration:481 \n",
      "Loss:0.0060240221209824085\n",
      "Epoch:5 \n",
      "Iteration:482 \n",
      "Loss:0.022857988253235817\n",
      "Epoch:5 \n",
      "Iteration:483 \n",
      "Loss:0.04332411661744118\n",
      "Epoch:5 \n",
      "Iteration:484 \n",
      "Loss:0.11777931451797485\n",
      "Epoch:5 \n",
      "Iteration:485 \n",
      "Loss:0.12377697974443436\n",
      "Epoch:5 \n",
      "Iteration:486 \n",
      "Loss:0.04110099375247955\n",
      "Epoch:5 \n",
      "Iteration:487 \n",
      "Loss:0.039203811436891556\n",
      "Epoch:5 \n",
      "Iteration:488 \n",
      "Loss:0.05967998132109642\n",
      "Epoch:5 \n",
      "Iteration:489 \n",
      "Loss:0.003850245149806142\n",
      "Epoch:5 \n",
      "Iteration:490 \n",
      "Loss:0.04961865767836571\n",
      "Epoch:5 \n",
      "Iteration:491 \n",
      "Loss:0.02007894590497017\n",
      "Epoch:5 \n",
      "Iteration:492 \n",
      "Loss:0.0012080129235982895\n",
      "Epoch:5 \n",
      "Iteration:493 \n",
      "Loss:0.06124259531497955\n",
      "Epoch:5 \n",
      "Iteration:494 \n",
      "Loss:0.02832639031112194\n",
      "Epoch:5 \n",
      "Iteration:495 \n",
      "Loss:0.050372667610645294\n",
      "Epoch:5 \n",
      "Iteration:496 \n",
      "Loss:0.009329910390079021\n",
      "Epoch:5 \n",
      "Iteration:497 \n",
      "Loss:0.058764442801475525\n",
      "Epoch:5 \n",
      "Iteration:498 \n",
      "Loss:0.06463534384965897\n",
      "Epoch:5 \n",
      "Iteration:499 \n",
      "Loss:0.030944380909204483\n",
      "Epoch:5 \n",
      "Iteration:500 \n",
      "Loss:0.026387058198451996\n",
      "Epoch:5 \n",
      "Iteration:501 \n",
      "Loss:0.07025323808193207\n",
      "Epoch:5 \n",
      "Iteration:502 \n",
      "Loss:0.0074350880458951\n",
      "Epoch:5 \n",
      "Iteration:503 \n",
      "Loss:0.031213246285915375\n",
      "Epoch:5 \n",
      "Iteration:504 \n",
      "Loss:0.009925741702318192\n",
      "Epoch:5 \n",
      "Iteration:505 \n",
      "Loss:0.1001056581735611\n",
      "Epoch:5 \n",
      "Iteration:506 \n",
      "Loss:0.16858603060245514\n",
      "Epoch:5 \n",
      "Iteration:507 \n",
      "Loss:0.011763614602386951\n",
      "Epoch:5 \n",
      "Iteration:508 \n",
      "Loss:0.07400573045015335\n",
      "Epoch:5 \n",
      "Iteration:509 \n",
      "Loss:0.0965205579996109\n",
      "Epoch:5 \n",
      "Iteration:510 \n",
      "Loss:0.05325038731098175\n",
      "Epoch:5 \n",
      "Iteration:511 \n",
      "Loss:0.10820859670639038\n",
      "Epoch:5 \n",
      "Iteration:512 \n",
      "Loss:0.013834051787853241\n",
      "Epoch:5 \n",
      "Iteration:513 \n",
      "Loss:0.00935860350728035\n",
      "Epoch:5 \n",
      "Iteration:514 \n",
      "Loss:0.03320690989494324\n",
      "Epoch:5 \n",
      "Iteration:515 \n",
      "Loss:0.05758867785334587\n",
      "Epoch:5 \n",
      "Iteration:516 \n",
      "Loss:0.0857243537902832\n",
      "Epoch:5 \n",
      "Iteration:517 \n",
      "Loss:0.007965043187141418\n",
      "Epoch:5 \n",
      "Iteration:518 \n",
      "Loss:0.09572593867778778\n",
      "Epoch:5 \n",
      "Iteration:519 \n",
      "Loss:0.04329109564423561\n",
      "Epoch:5 \n",
      "Iteration:520 \n",
      "Loss:0.012259460985660553\n",
      "Epoch:5 \n",
      "Iteration:521 \n",
      "Loss:0.02906518243253231\n",
      "Epoch:5 \n",
      "Iteration:522 \n",
      "Loss:0.03238600119948387\n",
      "Epoch:5 \n",
      "Iteration:523 \n",
      "Loss:0.07829958945512772\n",
      "Epoch:5 \n",
      "Iteration:524 \n",
      "Loss:0.08853375166654587\n",
      "Epoch:5 \n",
      "Iteration:525 \n",
      "Loss:0.03739073872566223\n",
      "Epoch:5 \n",
      "Iteration:526 \n",
      "Loss:0.009950990788638592\n",
      "Epoch:5 \n",
      "Iteration:527 \n",
      "Loss:0.028542401269078255\n",
      "Epoch:5 \n",
      "Iteration:528 \n",
      "Loss:0.04543047398328781\n",
      "Epoch:5 \n",
      "Iteration:529 \n",
      "Loss:0.036084841936826706\n",
      "Epoch:5 \n",
      "Iteration:530 \n",
      "Loss:0.020066287368535995\n",
      "Epoch:5 \n",
      "Iteration:531 \n",
      "Loss:0.01566131040453911\n",
      "Epoch:5 \n",
      "Iteration:532 \n",
      "Loss:0.06999991834163666\n",
      "Epoch:5 \n",
      "Iteration:533 \n",
      "Loss:0.035319503396749496\n",
      "Epoch:5 \n",
      "Iteration:534 \n",
      "Loss:0.13384738564491272\n",
      "Epoch:5 \n",
      "Iteration:535 \n",
      "Loss:0.05361543223261833\n",
      "Epoch:5 \n",
      "Iteration:536 \n",
      "Loss:0.041168030351400375\n",
      "Epoch:5 \n",
      "Iteration:537 \n",
      "Loss:0.09533627331256866\n",
      "Epoch:5 \n",
      "Iteration:538 \n",
      "Loss:0.02034333162009716\n",
      "Epoch:5 \n",
      "Iteration:539 \n",
      "Loss:0.023726437240839005\n",
      "Epoch:5 \n",
      "Iteration:540 \n",
      "Loss:0.02835550159215927\n",
      "Epoch:5 \n",
      "Iteration:541 \n",
      "Loss:0.009913735091686249\n",
      "Epoch:5 \n",
      "Iteration:542 \n",
      "Loss:0.03806377574801445\n",
      "Epoch:5 \n",
      "Iteration:543 \n",
      "Loss:0.06259583681821823\n",
      "Epoch:5 \n",
      "Iteration:544 \n",
      "Loss:0.031196046620607376\n",
      "Epoch:5 \n",
      "Iteration:545 \n",
      "Loss:0.031106457114219666\n",
      "Epoch:5 \n",
      "Iteration:546 \n",
      "Loss:0.10589409619569778\n",
      "Epoch:5 \n",
      "Iteration:547 \n",
      "Loss:0.08020887523889542\n",
      "Epoch:5 \n",
      "Iteration:548 \n",
      "Loss:0.045092593878507614\n",
      "Epoch:5 \n",
      "Iteration:549 \n",
      "Loss:0.05215921998023987\n",
      "Epoch:5 \n",
      "Iteration:550 \n",
      "Loss:0.016143472865223885\n",
      "Epoch:5 \n",
      "Iteration:551 \n",
      "Loss:0.00945336651057005\n",
      "Epoch:5 \n",
      "Iteration:552 \n",
      "Loss:0.06134295463562012\n",
      "Epoch:5 \n",
      "Iteration:553 \n",
      "Loss:0.034022457897663116\n",
      "Epoch:5 \n",
      "Iteration:554 \n",
      "Loss:0.010873165912926197\n",
      "Epoch:5 \n",
      "Iteration:555 \n",
      "Loss:0.018323291093111038\n",
      "Epoch:5 \n",
      "Iteration:556 \n",
      "Loss:0.020297009497880936\n",
      "Epoch:5 \n",
      "Iteration:557 \n",
      "Loss:0.010244459845125675\n",
      "Epoch:5 \n",
      "Iteration:558 \n",
      "Loss:0.037558481097221375\n",
      "Epoch:5 \n",
      "Iteration:559 \n",
      "Loss:0.01061056274920702\n",
      "Epoch:5 \n",
      "Iteration:560 \n",
      "Loss:0.04536796733736992\n",
      "Epoch:5 \n",
      "Iteration:561 \n",
      "Loss:0.08902322500944138\n",
      "Epoch:5 \n",
      "Iteration:562 \n",
      "Loss:0.04351144656538963\n",
      "Epoch:5 \n",
      "Iteration:563 \n",
      "Loss:0.04123517870903015\n",
      "Epoch:5 \n",
      "Iteration:564 \n",
      "Loss:0.03861682862043381\n",
      "Epoch:5 \n",
      "Iteration:565 \n",
      "Loss:0.04593491554260254\n",
      "Epoch:5 \n",
      "Iteration:566 \n",
      "Loss:0.1014302670955658\n",
      "Epoch:5 \n",
      "Iteration:567 \n",
      "Loss:0.052195657044649124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5 \n",
      "Iteration:568 \n",
      "Loss:0.015729490667581558\n",
      "Epoch:5 \n",
      "Iteration:569 \n",
      "Loss:0.010034353472292423\n",
      "Epoch:5 \n",
      "Iteration:570 \n",
      "Loss:0.11708416044712067\n",
      "Epoch:5 \n",
      "Iteration:571 \n",
      "Loss:0.1108635738492012\n",
      "Epoch:5 \n",
      "Iteration:572 \n",
      "Loss:0.056082434952259064\n",
      "Epoch:5 \n",
      "Iteration:573 \n",
      "Loss:0.012722469866275787\n",
      "Epoch:5 \n",
      "Iteration:574 \n",
      "Loss:0.06322310119867325\n",
      "Epoch:5 \n",
      "Iteration:575 \n",
      "Loss:0.019840845838189125\n",
      "Epoch:5 \n",
      "Iteration:576 \n",
      "Loss:0.06076742708683014\n",
      "Epoch:5 \n",
      "Iteration:577 \n",
      "Loss:0.02260960079729557\n",
      "Epoch:5 \n",
      "Iteration:578 \n",
      "Loss:0.032762885093688965\n",
      "Epoch:5 \n",
      "Iteration:579 \n",
      "Loss:0.0068029360845685005\n",
      "Epoch:5 \n",
      "Iteration:580 \n",
      "Loss:0.05213358998298645\n",
      "Epoch:5 \n",
      "Iteration:581 \n",
      "Loss:0.013620147481560707\n",
      "Epoch:5 \n",
      "Iteration:582 \n",
      "Loss:0.04285508021712303\n",
      "Epoch:5 \n",
      "Iteration:583 \n",
      "Loss:0.04328902065753937\n",
      "Epoch:5 \n",
      "Iteration:584 \n",
      "Loss:0.0504206120967865\n",
      "Epoch:5 \n",
      "Iteration:585 \n",
      "Loss:0.03718406707048416\n",
      "Epoch:5 \n",
      "Iteration:586 \n",
      "Loss:0.0032064556144177914\n",
      "Epoch:5 \n",
      "Iteration:587 \n",
      "Loss:0.00622851587831974\n",
      "Epoch:5 \n",
      "Iteration:588 \n",
      "Loss:0.08114097267389297\n",
      "Epoch:5 \n",
      "Iteration:589 \n",
      "Loss:0.03547229617834091\n",
      "Epoch:5 \n",
      "Iteration:590 \n",
      "Loss:0.006339258514344692\n",
      "Epoch:5 \n",
      "Iteration:591 \n",
      "Loss:0.1124071404337883\n",
      "Epoch:5 \n",
      "Iteration:592 \n",
      "Loss:0.07256704568862915\n",
      "Epoch:5 \n",
      "Iteration:593 \n",
      "Loss:0.09187868982553482\n",
      "Epoch:5 \n",
      "Iteration:594 \n",
      "Loss:0.04057544097304344\n",
      "Epoch:5 \n",
      "Iteration:595 \n",
      "Loss:0.11772045493125916\n",
      "Epoch:5 \n",
      "Iteration:596 \n",
      "Loss:0.052355241030454636\n",
      "Epoch:5 \n",
      "Iteration:597 \n",
      "Loss:0.008064932189881802\n",
      "Epoch:5 \n",
      "Iteration:598 \n",
      "Loss:0.017516866326332092\n",
      "Epoch:5 \n",
      "Iteration:599 \n",
      "Loss:0.0536358505487442\n",
      "Epoch:5 \n",
      "Iteration:600 \n",
      "Loss:0.0071176690980792046\n",
      "\n",
      "Accuracy of network in epoch 5: 98.695\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs = 5):\n",
    "    accuraccy_list = []\n",
    "    for epoch in range(epochs):\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch:{epoch + 1} \\nIteration:{i + 1} \\nLoss:{loss}')\n",
    "            with torch.no_grad():\n",
    "                total += labels.size(0)\n",
    "                _,prediction = torch.max(outputs, 1)\n",
    "                correct += (prediction == labels).sum().item()\n",
    "        print(f'\\nAccuracy of network in epoch {epoch + 1}: {100 * correct / total}')\n",
    "    writer.flush()\n",
    "\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network:98.02\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for data, labels in test_loader:\n",
    "    data = data.to(torch.device(\"cuda:0\"))\n",
    "    with torch.no_grad():\n",
    "        validation = model(data)\n",
    "        _,prediction = torch.max(validation, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (prediction.cpu() == labels).sum().item()\n",
    "    \n",
    "print(f'Accuracy of the network:{100 * correct / total}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "pytorch2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
