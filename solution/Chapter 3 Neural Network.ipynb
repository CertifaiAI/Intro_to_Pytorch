{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Dataloader\n",
    "### What is Dataloader\n",
    "Dataloader is a class that helps with shuffling and organizing the data in minibatches. We can import this class from `torch.utils.data`.\n",
    "\n",
    "The job of a data loader is to sample minibatches from a dataset, giving us the flexibility to choose the size of our minibatch to be use for training in each iteration. The constructor takes a `Dataset` object as input, along with `batch_size` and a `shuffle` boolean variable that indicates whether the data needs to be shuffled at the beginning of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required library\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading/Downloading the FashionMNIST dataset, download might takes some time \n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = '../data',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor()\n",
    "    )\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root = '../data',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset into the DataLoader and input your desired batch size for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size = 32, shuffle = True)\n",
    "test_loader = DataLoader(test_set, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# A view of the DataLoader\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch\n",
    "\n",
    "# Output the size of each batch\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Build your first Neural Network (Subclassing nn.Module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Model Training\n",
    "We had loaded our dataset into training and testing set, now let us build a simple Feedfoward Neural Network to perform classification on this dataset.\n",
    "\n",
    "PyTorch has a whole submodule dedicated to neural networks, called `torch.nn`. It contains the building blocks needed to create all sorts of neural network architectures.\n",
    "\n",
    "To build a Neural Network, it could be done in two ways :\n",
    "- Subclassing `nn.Module` to have more flexibility on designing the network, eg: writing the your own `foward()` method\n",
    "- Calling the `nn.Sequential()` for fast implementation of the network\n",
    "\n",
    "Now let us start building the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to build a 4 layers neural network with ReLU activation function. Apply dropout with 20% probability to reduce the effect of overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(784, 256)\n",
    "        self.act_1 = nn.ReLU()\n",
    "        self.fc_2 = nn.Linear(256, 128)\n",
    "        self.act_2 = nn.ReLU()\n",
    "        self.fc_3 = nn.Linear(128, 64)\n",
    "        self.act_3 = nn.ReLU()\n",
    "        self.fc_4 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.dropout(self.act_1(self.fc_1(x)))\n",
    "        out = self.dropout(self.act_2(self.fc_2(out)))\n",
    "        out = self.dropout(self.act_3(self.fc_3(out)))\n",
    "        out = self.fc_4(out)\n",
    "        return out\n",
    "\n",
    "# Or you can use the Pytorch provided functional API when defining the forward method. Both of these are the same.\n",
    "\n",
    "class Classifier_F(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(784, 256)\n",
    "        self.fc_2 = nn.Linear(256, 128)\n",
    "        self.fc_3 = nn.Linear(128, 64)\n",
    "        self.fc_4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.dropout(F.relu(self.fc_1(x)), p = 0.2)\n",
    "        out = F.dropout(F.relu(self.fc_2(out)), p = 0.2)\n",
    "        out = F.dropout(F.relu(self.fc_3(out)), p = 0.2)\n",
    "        out = self.fc_4(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a wrapper function for our training called `training`. This wrapper function will take on parameters:\n",
    "- n_epochs\n",
    "- optimizer\n",
    "- model\n",
    "- loss_fn\n",
    "- train_loader\n",
    "- writer (Instance of Summary Writer to use TensorBoard for visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch does support TensorBoard which provides the visualization and tooling needed for machine learning experimentation. It is a useful tool that we can use during our training. Now let's define our training loop and implement some of the TensorBoard methods. \n",
    "\n",
    "If you wish to know more on TensorBoard, you can access it at [here](https://pytorch.org/docs/stable/tensorboard.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def training(n_epochs, optimizer, model, loss_fn, train_loader, writer):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            # Clearing gradient from previous mini-batch gradient computation  \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Reshape the tensor so that it fits the dimension of our input layer\n",
    "            # Get predictions output from the model\n",
    "            outputs = model(imgs.view(-1, 784))\n",
    "            \n",
    "            # Calculate the loss for curernt batch\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Calculating the gradient\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updating the weights and biases using optimizer.step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Summing up the loss over each epoch\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "            # Calculating the accuracy\n",
    "            predictions = torch.max(outputs, 1)[1]\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += len(labels)\n",
    "\n",
    "        accuracy = correct * 100 / total\n",
    "        writer.add_scalar('Loss ', loss_train / len(train_loader), epoch)\n",
    "        writer.add_scalar('Accuracy ', accuracy, epoch)\n",
    "        print('Epoch {}, Training loss {} , Accuracy {:.2f} %'.format(epoch, loss_train / len(train_loader), accuracy))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can open our TensorBoard in the terminal with the command of \"tensorboard --logdir=runs\". Do remember change to the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready for training. Let's use SGD as our optimizer and CrossEntropy as loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 2.2894890218098958 , Accuracy 18.39 %\n",
      "Epoch 2, Training loss 2.2399076170603434 , Accuracy 28.73 %\n",
      "Epoch 3, Training loss 2.068951116498311 , Accuracy 29.45 %\n",
      "Epoch 4, Training loss 1.695164651552836 , Accuracy 36.16 %\n",
      "Epoch 5, Training loss 1.4096814838409424 , Accuracy 46.69 %\n",
      "Epoch 6, Training loss 1.2168791191418966 , Accuracy 52.60 %\n",
      "Epoch 7, Training loss 1.1041425074577331 , Accuracy 56.25 %\n",
      "Epoch 8, Training loss 1.0339518047332763 , Accuracy 59.37 %\n",
      "Epoch 9, Training loss 0.9758181870142619 , Accuracy 62.02 %\n",
      "Epoch 10, Training loss 0.9312916868527731 , Accuracy 63.92 %\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model_SGD = Classifier() \n",
    "optimizer = optim.SGD(model_SGD.parameters(), lr = 1e-3) \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(comment = 'SGD')\n",
    "training(\n",
    "    n_epochs = 10,\n",
    "    optimizer = optimizer,\n",
    "    model = model_SGD,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    writer = writer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build another model which we set log softmax as the activation function at the output layer and uses Negative log-likelihood loss function. Compare the results for both of these setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 2.2894890218098958 , Accuracy 18.39 %\n",
      "Epoch 2, Training loss 2.2399076170603434 , Accuracy 28.73 %\n",
      "Epoch 3, Training loss 2.068951116498311 , Accuracy 29.45 %\n",
      "Epoch 4, Training loss 1.695164651552836 , Accuracy 36.16 %\n",
      "Epoch 5, Training loss 1.4096814838409424 , Accuracy 46.69 %\n",
      "Epoch 6, Training loss 1.2168791191418966 , Accuracy 52.60 %\n",
      "Epoch 7, Training loss 1.1041425074577331 , Accuracy 56.25 %\n",
      "Epoch 8, Training loss 1.0339518047332763 , Accuracy 59.37 %\n",
      "Epoch 9, Training loss 0.9758181870142619 , Accuracy 62.02 %\n",
      "Epoch 10, Training loss 0.9312916868527731 , Accuracy 63.92 %\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(784, 256)\n",
    "        self.act_1 = nn.ReLU()\n",
    "        self.fc_2 = nn.Linear(256, 128)\n",
    "        self.act_2 = nn.ReLU()\n",
    "        self.fc_3 = nn.Linear(128, 64)\n",
    "        self.act_3 = nn.ReLU()\n",
    "        self.fc_4 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.dropout(self.act_1(self.fc_1(x)))\n",
    "        out = self.dropout(self.act_2(self.fc_2(out)))\n",
    "        out = self.dropout(self.act_3(self.fc_3(out)))\n",
    "        # adding in softmax\n",
    "        out = F.log_softmax(self.fc_4(out),dim =1 )\n",
    "        return out\n",
    "\n",
    "model_SGD = Classifier() \n",
    "optimizer = optim.SGD(model_SGD.parameters(), lr = 1e-3) \n",
    "loss_fn = nn.NLLLoss()\n",
    "writer = SummaryWriter(comment = 'SGD')\n",
    "training(\n",
    "    n_epochs = 10,\n",
    "    optimizer = optimizer,\n",
    "    model = model_SGD,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    writer = writer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossEntropy Loss is actually performing log softmax and negative log likelihood at the same time. Therefore during the construction of our model we could neglect the declaration of activation function at the output layer and save some memory during the backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try using other optimizer (Adam) to do our training. Optimizer is one of the hyperparameters that we can tune on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.5945944479823112 , Accuracy 78.32 %\n",
      "Epoch 2, Training loss 0.423241344755888 , Accuracy 84.78 %\n",
      "Epoch 3, Training loss 0.38519719421068827 , Accuracy 86.15 %\n",
      "Epoch 4, Training loss 0.36408053546349206 , Accuracy 86.94 %\n",
      "Epoch 5, Training loss 0.35000673046310743 , Accuracy 87.39 %\n",
      "Epoch 6, Training loss 0.3385574172397455 , Accuracy 87.74 %\n",
      "Epoch 7, Training loss 0.32801985016465185 , Accuracy 88.09 %\n",
      "Epoch 8, Training loss 0.3184917394856612 , Accuracy 88.41 %\n",
      "Epoch 9, Training loss 0.31102090905706087 , Accuracy 88.59 %\n",
      "Epoch 10, Training loss 0.3041634604026874 , Accuracy 88.89 %\n"
     ]
    }
   ],
   "source": [
    "model_Adam = Classifier() \n",
    "optimizer = optim.Adam(model_Adam.parameters(), lr = 1e-3) \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(comment = 'Adam')\n",
    "training(\n",
    "    n_epochs = 10,\n",
    "    optimizer = optimizer,\n",
    "    model = model_Adam,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    writer = writer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that Adam is performing better than the SGD with the same setting. Hyperparameter tuning is very important in order to obtain desired result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Model Saving\n",
    "After training the model, we would like to save it for future usages. There are some pretty useful functions you might need to familar with:\n",
    "\n",
    "- `torch.save`: It serialize the object to save to your machine. Models, tensors, and dictionaries of all kinds of objects can be saved using this function.\n",
    "- `torch.load`: This function uses pickle’s unpickling facilities to deserialize pickled object files to memory.\n",
    "- `torch.nn.Module.load_state_dict`: Loads a model’s parameter dictionary using a deserialized state_dict.\n",
    "\n",
    "If you wish to know more on model saving, you can access it at [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving only the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('../generated_model'):\n",
    "    os.mkdir('../generated_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the weights only of the model\n",
    "torch.save(model_Adam.state_dict(),  '../generated_model/mnist_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To load the state_dict, you must have an instance of the model\n",
    "modelLoad = Classifier()\n",
    "modelLoad.load_state_dict(torch.load('../generated_model/mnist_state_dict.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the entire model\n",
    "torch.save(model_Adam, '../generated_model/mnist_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "modelLoad = torch.load('../generated_model/mnist_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add-ons: Saving Model in ONNX format\n",
    "Pytorch also support saving model as ONNX (Open Neural Network Exchange) file type, which is a open format built to represent machine learning models. Let's see how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input : Float(32:784, 784:1),\n",
      "      %fc_1.weight : Float(256:784, 784:1),\n",
      "      %fc_1.bias : Float(256:1),\n",
      "      %fc_2.weight : Float(128:256, 256:1),\n",
      "      %fc_2.bias : Float(128:1),\n",
      "      %fc_3.weight : Float(64:128, 128:1),\n",
      "      %fc_3.bias : Float(64:1),\n",
      "      %fc_4.weight : Float(10:64, 64:1),\n",
      "      %fc_4.bias : Float(10:1)):\n",
      "  %9 : Float(32:256, 256:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%input, %fc_1.weight, %fc_1.bias) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1674:0\n",
      "  %10 : Float(32:256, 256:1) = onnx::Relu(%9) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:973:0\n",
      "  %11 : Float(32:128, 128:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%10, %fc_2.weight, %fc_2.bias) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1674:0\n",
      "  %12 : Float(32:128, 128:1) = onnx::Relu(%11) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:973:0\n",
      "  %13 : Float(32:64, 64:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%12, %fc_3.weight, %fc_3.bias) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1674:0\n",
      "  %14 : Float(32:64, 64:1) = onnx::Relu(%13) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:973:0\n",
      "  %15 : Float(32:10, 10:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%14, %fc_4.weight, %fc_4.bias) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1674:0\n",
      "  %output : Float(32:10, 10:1) = onnx::LogSoftmax[axis=1](%15) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1591:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx \n",
    "dummy_input = torch.randn(32, 784, requires_grad = True)\n",
    "torch.onnx.export(model_Adam, dummy_input, '../generated_model/model.onnx', verbose = True, input_names = ['input'], output_names = ['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "#loading the onnx format model\n",
    "model = onnx.load('../generated_model/model.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Inference\n",
    "Sometimes, we would like to inference on the trained model to evaluate the performance. `model.eval()` will set the model to evaluation(inference) mode to set dropout, batch normalization layers, etc.. to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc_1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (act_1): ReLU()\n",
       "  (fc_2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (act_2): ReLU()\n",
       "  (fc_3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (act_3): ReLU()\n",
       "  (fc_4): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using previous loaded model\n",
    "modelLoad.eval()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting it to inference mode, we could pass in test data with the setting of \n",
    "```python \n",
    "with torch.no_grad():\n",
    "``` \n",
    "as we do not have to calculate the gradient during the inference, this can help us save some memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 88.09 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = modelLoad(images.view(-1, 784))\n",
    "        predictions = torch.max(outputs, 1)[1]\n",
    "        correct += (predictions == labels).sum()\n",
    "        total += len(labels)\n",
    "    accuracy_test = correct.item() * 100 / total\n",
    "print(\"Test Accuracy : {:.2f} %\".format(accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Build your first Neural Network (Sequential Model)\n",
    "### 3.3.1 Model Training\n",
    "\n",
    "Altough there are many other machine learning techniques to tackle multi-variate linear regression, it would be interesting for us to tackle it using deep learning for learning purposes.\n",
    "<br>In this sub-section, we will try to perform said regression using PyTorch `SequentialModel` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Real Estate dataset from the `realEstate.csv` for our linear regression example. \n",
    "\n",
    "Description of data:\n",
    "- House Age\n",
    "- Distance from the unit to MRT station\n",
    "- The number of Convenience Stores around the unit\n",
    "- House Unit Price per 1000 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use pandas to load in the csv.<br>\n",
    "Note that in this dataset there are a total of $3$ features and $1$ label.<br>\n",
    "Thus from the data we will use `.iloc[]` to distinguish the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/Regression/realEstate.csv\", header = 0)\n",
    "n_features = 3\n",
    "X = data.iloc[:, 0:3].values\n",
    "y = data.iloc[:, 3].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, we split our dataset into 70/30 train/test ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, shuffle = True, random_state = 1022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform feature scaling onto `X_train` and `X_test` using `StandardScaler` from `scikit-learn`.<br>\n",
    "*Note: only fit the train_set but transform both train and test sets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 3.1, we've touch on how Dataloaders are initialized and used in model training. It was simple, which is to pass in whatever `Dataset` we need into the Dataloader initializer. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using a custom dataset from a csv file as compared to the previous one which was prepared readily from torchvision. Thus in this case, we will have to build our own by subclassing from `torch.utils.data.Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst subclassing `Dataset`, PyTorch [documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) notes that we have to override the `__getitem__()` method and optionally the `__len__()` method.<br>\n",
    "We will mainly have three methods in this `Dataset` class:\n",
    "- `__init__(self, data, label)`: helps us pass in the feature and labels into the dataset\n",
    "- `__len__(self)`:allows the dataset to know how many instances of data there is \n",
    "- `__getitem__(self, idx)`:allows the dataset to get items from the data and labels by indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype = torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype  = torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After feature scaling, we initialize our custom datasets and put them into `Dataloader` constructor and our data is prepared. The next step will be modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Custom_Dataset(X_train, y_train)\n",
    "test_dataset = Custom_Dataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 128 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we previously stated, there are two approaches of modeling.\n",
    "- Subclassing `nn.Module` \n",
    "- Calling the `nn.Sequential()` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.Sequential` is a simple function that accepts a list of `nn.Modules` and returns a model with all the sequential layers. We will be implementing these few layers:\n",
    "1. nn.Linear(3,50)\n",
    "2. nn.ReLU()\n",
    "3. nn.Linear(50,25)\n",
    "4. nn.ReLU()\n",
    "5. nn.Linear(25,10)\n",
    "6. nn.ReLU()\n",
    "7. nn.Linear(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model_sequential = nn.Sequential(nn.Linear(n_features, 50),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(50, 25),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(25, 10),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(10, 1)\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this regression probelm, the loss/criterion we will use is Mean-Squared-Error loss, which in PyTorch is `nn.MSELoss()`<br>\n",
    "We will also choose to use `Adam` as our optimizer.<br> Remember, `torch.optim.*any_optimizer*` accepts `model.parameters()` to keep track of the model's parameters, hence we should always initialize our model first before our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_sequential.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our modeling is done, let's commence our training with using the training loop that defined previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a wrapper function for our training called `train_model`. This wrapper function will take on parameters:\n",
    "- model\n",
    "- loader\n",
    "- loss_function/criterion\n",
    "- optimizer\n",
    "- number_of_epochs (optional)a\n",
    "- iteration_check (optional): *if False is passed in, losses of each iteration per epoch will not be printed>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below will be an overall workings an explaination of our train_model function:\n",
    "1. In each epoch, each minibatch starts with `optimizer.zero_grad()`. This is to clear previously computed gradients from previous minibatches.\n",
    "2. We get the features and labels by indexing our minibatch.\n",
    "3. Compute forward propagation by calling `model(features)` and assigning it to a variable `prediction`\n",
    "4. Compute the loss by calling `criterion(prediction, torch.unsqueeze(labels, dim=1))`\n",
    "    - the reason we unsqueeze is to make sure the shape of the labels are the same as the predictions, which is (batch_size,1) \n",
    "5. Compute backward_propagation by calling `loss.backward()`\n",
    "6. Update the parameters(learning rate etc.) of the model by calling `optimizer.step()`\n",
    "7. Increment our running_loss with the loss of our current batch\n",
    "8. At the end of each epoch, compute the accuracy by dividing the accumulated loss and the amount of data samples, and finally zero the running_loss for the next epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, criterion, optimizer,epochs=5000):\n",
    "#   this running_loss will keep track of the losses of every epoch from each respective iteration\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for i, data in enumerate(loader):\n",
    "#           zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            features, labels = data[0],data[1]\n",
    "            prediction = model(features)\n",
    "            loss = criterion(prediction, torch.unsqueeze(labels,dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if (epoch % 100 == 0 or epoch == 1):\n",
    "            print(f\"Epoch {epoch} Loss: {running_loss / len(loader)}\")     \n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1559.314471435547\n",
      "Epoch 100 Loss: 61.67083594799042\n",
      "Epoch 200 Loss: 57.53920102566481\n",
      "Epoch 300 Loss: 54.614624582976106\n",
      "Epoch 400 Loss: 51.69376365095377\n",
      "Epoch 500 Loss: 49.110941734910014\n",
      "Epoch 600 Loss: 44.46782956123352\n",
      "Epoch 700 Loss: 45.49254035949707\n",
      "Epoch 800 Loss: 45.39475156664848\n",
      "Epoch 900 Loss: 43.348855590820314\n",
      "Epoch 1000 Loss: 42.04828781485558\n",
      "Epoch 1100 Loss: 39.37081394195557\n",
      "Epoch 1200 Loss: 42.60350239276886\n",
      "Epoch 1300 Loss: 38.945985350012776\n",
      "Epoch 1400 Loss: 39.63016664907336\n",
      "Epoch 1500 Loss: 36.81087758541107\n",
      "Epoch 1600 Loss: 34.936926842236424\n",
      "Epoch 1700 Loss: 35.42953658103943\n",
      "Epoch 1800 Loss: 32.789571383502334\n",
      "Epoch 1900 Loss: 34.93219475212682\n",
      "Epoch 2000 Loss: 33.54853103160858\n",
      "Epoch 2100 Loss: 28.336665666103364\n",
      "Epoch 2200 Loss: 25.664763996377587\n",
      "Epoch 2300 Loss: 24.103572607040405\n",
      "Epoch 2400 Loss: 17.353846311569214\n",
      "Epoch 2500 Loss: 15.863344663381577\n",
      "Epoch 2600 Loss: 13.111431193351745\n",
      "Epoch 2700 Loss: 12.318226540088654\n",
      "Epoch 2800 Loss: 19.141652542352677\n",
      "Epoch 2900 Loss: 17.75134304985404\n",
      "Epoch 3000 Loss: 16.94328822637908\n",
      "Epoch 3100 Loss: 18.66891082525253\n",
      "Epoch 3200 Loss: 17.63850952475368\n",
      "Epoch 3300 Loss: 14.46680794209242\n",
      "Epoch 3400 Loss: 21.239836806058882\n",
      "Epoch 3500 Loss: 20.83810586631298\n",
      "Epoch 3600 Loss: 15.835954087972642\n",
      "Epoch 3700 Loss: 22.814937913417815\n",
      "Epoch 3800 Loss: 18.055061160423794\n",
      "Epoch 3900 Loss: 18.024778324365617\n",
      "Epoch 4000 Loss: 19.730439281463624\n",
      "Epoch 4100 Loss: 16.410921066999435\n",
      "Epoch 4200 Loss: 14.806035457924008\n",
      "Epoch 4300 Loss: 12.136985358595847\n",
      "Epoch 4400 Loss: 20.251971996575595\n",
      "Epoch 4500 Loss: 17.372333994880318\n",
      "Epoch 4600 Loss: 18.757385206222533\n",
      "Epoch 4700 Loss: 18.26095001846552\n",
      "Epoch 4800 Loss: 19.056522417068482\n",
      "Epoch 4900 Loss: 22.027928829193115\n",
      "Epoch 5000 Loss: 16.290424835681915\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "train_model(model_sequential, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Inference\n",
    "\n",
    "Now let's evaluate our model. Use `model.eval()` to set the model to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=25, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=25, out_features=10, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sequential.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say your house age is 10, distance to MRT is 100 meters, and there are 6 convenience stores around the unit, could you predict your house price? Let's use our trained model to find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for your house price is : 54032.859802246094\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inference = torch.tensor([[10, 100, 6]])\n",
    "    inference = torch.from_numpy(scaler.transform(inference))\n",
    "    predict = model_sequential.forward(inference.float())\n",
    "        \n",
    "print(\"The prediction for your house price is :\", predict.item() * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will try to build a classifier for our MNIST Handwriting dataset.\n",
    "\n",
    "Construct transform with the following transforms:\n",
    "- coverting to tensor\n",
    "- normalize the tensor with mean=0.15 and std=0.3081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.15,), (0.3081,))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the MNIST dataset from `torchvision.datasets`. Load them into respective `Dataloaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "train = MNIST(\"../data\", download = True, transform = transform, train = True)\n",
    "test = MNIST(\"../data\", download = True, transform = transform, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, 100, shuffle = True, num_workers = 0)\n",
    "test_loader = DataLoader(test, 100, shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare `SummaryWriter` for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Model with the following layers:\n",
    "- 4 linear/dense layers\n",
    "- First 3 with ReLU activation functions\n",
    "\n",
    "*Note: Remember to resize the incoming tensor first*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features = 28 * 28, out_features = 1000)\n",
    "        self.fc2 = nn.Linear(in_features = 1000, out_features = 500)\n",
    "        self.fc3 = nn.Linear(in_features = 500, out_features = 100)\n",
    "        self.fc4 = nn.Linear(in_features = 100, out_features = 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model and load it to our **GPU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize criterion: CrossEntropyLoss and optimizer Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a wrapper function `train_model` to train the model using `CUDA`. Add_scalar which shows a loss against epoch graph on TensorBoard.<br>\n",
    "Here is a checklist for you to keep check what to do:\n",
    "1. For each iteration in each epoch, zero the gradients of the parameters\n",
    "2. Forward propagate\n",
    "3. Calculate loss\n",
    "4. Write the loss and train to TensorBoard\n",
    "5. Back propagate\n",
    "6. Update the parameters\n",
    "7. For each epoch, calculate the accuracy on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \n",
      "Iteration:1 \n",
      "Loss:2.3120129108428955\n",
      "Epoch:1 \n",
      "Iteration:2 \n",
      "Loss:2.243009090423584\n",
      "Epoch:1 \n",
      "Iteration:3 \n",
      "Loss:2.103304862976074\n",
      "Epoch:1 \n",
      "Iteration:4 \n",
      "Loss:1.938184380531311\n",
      "Epoch:1 \n",
      "Iteration:5 \n",
      "Loss:1.7360073328018188\n",
      "Epoch:1 \n",
      "Iteration:6 \n",
      "Loss:1.4185400009155273\n",
      "Epoch:1 \n",
      "Iteration:7 \n",
      "Loss:1.3077017068862915\n",
      "Epoch:1 \n",
      "Iteration:8 \n",
      "Loss:0.979144811630249\n",
      "Epoch:1 \n",
      "Iteration:9 \n",
      "Loss:0.8673616051673889\n",
      "Epoch:1 \n",
      "Iteration:10 \n",
      "Loss:0.7848854660987854\n",
      "Epoch:1 \n",
      "Iteration:11 \n",
      "Loss:0.9053862690925598\n",
      "Epoch:1 \n",
      "Iteration:12 \n",
      "Loss:0.7195755243301392\n",
      "Epoch:1 \n",
      "Iteration:13 \n",
      "Loss:0.8481418490409851\n",
      "Epoch:1 \n",
      "Iteration:14 \n",
      "Loss:0.7561290860176086\n",
      "Epoch:1 \n",
      "Iteration:15 \n",
      "Loss:0.6766089797019958\n",
      "Epoch:1 \n",
      "Iteration:16 \n",
      "Loss:0.5629891157150269\n",
      "Epoch:1 \n",
      "Iteration:17 \n",
      "Loss:0.6399906277656555\n",
      "Epoch:1 \n",
      "Iteration:18 \n",
      "Loss:0.6289631128311157\n",
      "Epoch:1 \n",
      "Iteration:19 \n",
      "Loss:0.7176056504249573\n",
      "Epoch:1 \n",
      "Iteration:20 \n",
      "Loss:0.6771294474601746\n",
      "Epoch:1 \n",
      "Iteration:21 \n",
      "Loss:0.43189147114753723\n",
      "Epoch:1 \n",
      "Iteration:22 \n",
      "Loss:0.5592374801635742\n",
      "Epoch:1 \n",
      "Iteration:23 \n",
      "Loss:0.6598607897758484\n",
      "Epoch:1 \n",
      "Iteration:24 \n",
      "Loss:0.5911179184913635\n",
      "Epoch:1 \n",
      "Iteration:25 \n",
      "Loss:0.5268451571464539\n",
      "Epoch:1 \n",
      "Iteration:26 \n",
      "Loss:0.34881362318992615\n",
      "Epoch:1 \n",
      "Iteration:27 \n",
      "Loss:0.4407460689544678\n",
      "Epoch:1 \n",
      "Iteration:28 \n",
      "Loss:0.3689185380935669\n",
      "Epoch:1 \n",
      "Iteration:29 \n",
      "Loss:0.4429815411567688\n",
      "Epoch:1 \n",
      "Iteration:30 \n",
      "Loss:0.6116948127746582\n",
      "Epoch:1 \n",
      "Iteration:31 \n",
      "Loss:0.6002848148345947\n",
      "Epoch:1 \n",
      "Iteration:32 \n",
      "Loss:0.5611258149147034\n",
      "Epoch:1 \n",
      "Iteration:33 \n",
      "Loss:0.666802167892456\n",
      "Epoch:1 \n",
      "Iteration:34 \n",
      "Loss:0.42656680941581726\n",
      "Epoch:1 \n",
      "Iteration:35 \n",
      "Loss:0.5240041017532349\n",
      "Epoch:1 \n",
      "Iteration:36 \n",
      "Loss:0.29298585653305054\n",
      "Epoch:1 \n",
      "Iteration:37 \n",
      "Loss:0.508000910282135\n",
      "Epoch:1 \n",
      "Iteration:38 \n",
      "Loss:0.46748197078704834\n",
      "Epoch:1 \n",
      "Iteration:39 \n",
      "Loss:0.33440831303596497\n",
      "Epoch:1 \n",
      "Iteration:40 \n",
      "Loss:0.6263558864593506\n",
      "Epoch:1 \n",
      "Iteration:41 \n",
      "Loss:0.3841659128665924\n",
      "Epoch:1 \n",
      "Iteration:42 \n",
      "Loss:0.3341437876224518\n",
      "Epoch:1 \n",
      "Iteration:43 \n",
      "Loss:0.45079100131988525\n",
      "Epoch:1 \n",
      "Iteration:44 \n",
      "Loss:0.44888055324554443\n",
      "Epoch:1 \n",
      "Iteration:45 \n",
      "Loss:0.46881726384162903\n",
      "Epoch:1 \n",
      "Iteration:46 \n",
      "Loss:0.31650739908218384\n",
      "Epoch:1 \n",
      "Iteration:47 \n",
      "Loss:0.39410457015037537\n",
      "Epoch:1 \n",
      "Iteration:48 \n",
      "Loss:0.41094696521759033\n",
      "Epoch:1 \n",
      "Iteration:49 \n",
      "Loss:0.5384355783462524\n",
      "Epoch:1 \n",
      "Iteration:50 \n",
      "Loss:0.333896279335022\n",
      "Epoch:1 \n",
      "Iteration:51 \n",
      "Loss:0.40745270252227783\n",
      "Epoch:1 \n",
      "Iteration:52 \n",
      "Loss:0.37681883573532104\n",
      "Epoch:1 \n",
      "Iteration:53 \n",
      "Loss:0.30641573667526245\n",
      "Epoch:1 \n",
      "Iteration:54 \n",
      "Loss:0.255819708108902\n",
      "Epoch:1 \n",
      "Iteration:55 \n",
      "Loss:0.34836751222610474\n",
      "Epoch:1 \n",
      "Iteration:56 \n",
      "Loss:0.19757182896137238\n",
      "Epoch:1 \n",
      "Iteration:57 \n",
      "Loss:0.3527303636074066\n",
      "Epoch:1 \n",
      "Iteration:58 \n",
      "Loss:0.26464834809303284\n",
      "Epoch:1 \n",
      "Iteration:59 \n",
      "Loss:0.369119256734848\n",
      "Epoch:1 \n",
      "Iteration:60 \n",
      "Loss:0.2571631073951721\n",
      "Epoch:1 \n",
      "Iteration:61 \n",
      "Loss:0.28121277689933777\n",
      "Epoch:1 \n",
      "Iteration:62 \n",
      "Loss:0.2682133913040161\n",
      "Epoch:1 \n",
      "Iteration:63 \n",
      "Loss:0.29948434233665466\n",
      "Epoch:1 \n",
      "Iteration:64 \n",
      "Loss:0.2584401071071625\n",
      "Epoch:1 \n",
      "Iteration:65 \n",
      "Loss:0.1802312135696411\n",
      "Epoch:1 \n",
      "Iteration:66 \n",
      "Loss:0.3833845555782318\n",
      "Epoch:1 \n",
      "Iteration:67 \n",
      "Loss:0.19513483345508575\n",
      "Epoch:1 \n",
      "Iteration:68 \n",
      "Loss:0.271013081073761\n",
      "Epoch:1 \n",
      "Iteration:69 \n",
      "Loss:0.3728995621204376\n",
      "Epoch:1 \n",
      "Iteration:70 \n",
      "Loss:0.33933544158935547\n",
      "Epoch:1 \n",
      "Iteration:71 \n",
      "Loss:0.24964481592178345\n",
      "Epoch:1 \n",
      "Iteration:72 \n",
      "Loss:0.1458766907453537\n",
      "Epoch:1 \n",
      "Iteration:73 \n",
      "Loss:0.2318328619003296\n",
      "Epoch:1 \n",
      "Iteration:74 \n",
      "Loss:0.35721245408058167\n",
      "Epoch:1 \n",
      "Iteration:75 \n",
      "Loss:0.27163660526275635\n",
      "Epoch:1 \n",
      "Iteration:76 \n",
      "Loss:0.2832474410533905\n",
      "Epoch:1 \n",
      "Iteration:77 \n",
      "Loss:0.28073111176490784\n",
      "Epoch:1 \n",
      "Iteration:78 \n",
      "Loss:0.18299219012260437\n",
      "Epoch:1 \n",
      "Iteration:79 \n",
      "Loss:0.19779279828071594\n",
      "Epoch:1 \n",
      "Iteration:80 \n",
      "Loss:0.32478073239326477\n",
      "Epoch:1 \n",
      "Iteration:81 \n",
      "Loss:0.5900135636329651\n",
      "Epoch:1 \n",
      "Iteration:82 \n",
      "Loss:0.25531479716300964\n",
      "Epoch:1 \n",
      "Iteration:83 \n",
      "Loss:0.1387094408273697\n",
      "Epoch:1 \n",
      "Iteration:84 \n",
      "Loss:0.24132877588272095\n",
      "Epoch:1 \n",
      "Iteration:85 \n",
      "Loss:0.30440443754196167\n",
      "Epoch:1 \n",
      "Iteration:86 \n",
      "Loss:0.36016371846199036\n",
      "Epoch:1 \n",
      "Iteration:87 \n",
      "Loss:0.2922019362449646\n",
      "Epoch:1 \n",
      "Iteration:88 \n",
      "Loss:0.19534306228160858\n",
      "Epoch:1 \n",
      "Iteration:89 \n",
      "Loss:0.25497016310691833\n",
      "Epoch:1 \n",
      "Iteration:90 \n",
      "Loss:0.31121373176574707\n",
      "Epoch:1 \n",
      "Iteration:91 \n",
      "Loss:0.19418592751026154\n",
      "Epoch:1 \n",
      "Iteration:92 \n",
      "Loss:0.31320226192474365\n",
      "Epoch:1 \n",
      "Iteration:93 \n",
      "Loss:0.17160598933696747\n",
      "Epoch:1 \n",
      "Iteration:94 \n",
      "Loss:0.3011602759361267\n",
      "Epoch:1 \n",
      "Iteration:95 \n",
      "Loss:0.2664591073989868\n",
      "Epoch:1 \n",
      "Iteration:96 \n",
      "Loss:0.34600746631622314\n",
      "Epoch:1 \n",
      "Iteration:97 \n",
      "Loss:0.15365272760391235\n",
      "Epoch:1 \n",
      "Iteration:98 \n",
      "Loss:0.3775344789028168\n",
      "Epoch:1 \n",
      "Iteration:99 \n",
      "Loss:0.2457958608865738\n",
      "Epoch:1 \n",
      "Iteration:100 \n",
      "Loss:0.20113806426525116\n",
      "Epoch:1 \n",
      "Iteration:101 \n",
      "Loss:0.2597142457962036\n",
      "Epoch:1 \n",
      "Iteration:102 \n",
      "Loss:0.3785651922225952\n",
      "Epoch:1 \n",
      "Iteration:103 \n",
      "Loss:0.2910976707935333\n",
      "Epoch:1 \n",
      "Iteration:104 \n",
      "Loss:0.32003748416900635\n",
      "Epoch:1 \n",
      "Iteration:105 \n",
      "Loss:0.3702780604362488\n",
      "Epoch:1 \n",
      "Iteration:106 \n",
      "Loss:0.2612571716308594\n",
      "Epoch:1 \n",
      "Iteration:107 \n",
      "Loss:0.20184503495693207\n",
      "Epoch:1 \n",
      "Iteration:108 \n",
      "Loss:0.30470114946365356\n",
      "Epoch:1 \n",
      "Iteration:109 \n",
      "Loss:0.2566049098968506\n",
      "Epoch:1 \n",
      "Iteration:110 \n",
      "Loss:0.30411121249198914\n",
      "Epoch:1 \n",
      "Iteration:111 \n",
      "Loss:0.28380855917930603\n",
      "Epoch:1 \n",
      "Iteration:112 \n",
      "Loss:0.3514578342437744\n",
      "Epoch:1 \n",
      "Iteration:113 \n",
      "Loss:0.3024035096168518\n",
      "Epoch:1 \n",
      "Iteration:114 \n",
      "Loss:0.3007325232028961\n",
      "Epoch:1 \n",
      "Iteration:115 \n",
      "Loss:0.23290877044200897\n",
      "Epoch:1 \n",
      "Iteration:116 \n",
      "Loss:0.3148707449436188\n",
      "Epoch:1 \n",
      "Iteration:117 \n",
      "Loss:0.1848905235528946\n",
      "Epoch:1 \n",
      "Iteration:118 \n",
      "Loss:0.15175089240074158\n",
      "Epoch:1 \n",
      "Iteration:119 \n",
      "Loss:0.21671690046787262\n",
      "Epoch:1 \n",
      "Iteration:120 \n",
      "Loss:0.2821066379547119\n",
      "Epoch:1 \n",
      "Iteration:121 \n",
      "Loss:0.25291892886161804\n",
      "Epoch:1 \n",
      "Iteration:122 \n",
      "Loss:0.21602274477481842\n",
      "Epoch:1 \n",
      "Iteration:123 \n",
      "Loss:0.1897137463092804\n",
      "Epoch:1 \n",
      "Iteration:124 \n",
      "Loss:0.18207958340644836\n",
      "Epoch:1 \n",
      "Iteration:125 \n",
      "Loss:0.22877010703086853\n",
      "Epoch:1 \n",
      "Iteration:126 \n",
      "Loss:0.24930299818515778\n",
      "Epoch:1 \n",
      "Iteration:127 \n",
      "Loss:0.2413063496351242\n",
      "Epoch:1 \n",
      "Iteration:128 \n",
      "Loss:0.24214984476566315\n",
      "Epoch:1 \n",
      "Iteration:129 \n",
      "Loss:0.30584508180618286\n",
      "Epoch:1 \n",
      "Iteration:130 \n",
      "Loss:0.13950319588184357\n",
      "Epoch:1 \n",
      "Iteration:131 \n",
      "Loss:0.3294107913970947\n",
      "Epoch:1 \n",
      "Iteration:132 \n",
      "Loss:0.28691741824150085\n",
      "Epoch:1 \n",
      "Iteration:133 \n",
      "Loss:0.13431206345558167\n",
      "Epoch:1 \n",
      "Iteration:134 \n",
      "Loss:0.22785593569278717\n",
      "Epoch:1 \n",
      "Iteration:135 \n",
      "Loss:0.2721596360206604\n",
      "Epoch:1 \n",
      "Iteration:136 \n",
      "Loss:0.13127829134464264\n",
      "Epoch:1 \n",
      "Iteration:137 \n",
      "Loss:0.252728670835495\n",
      "Epoch:1 \n",
      "Iteration:138 \n",
      "Loss:0.2905021011829376\n",
      "Epoch:1 \n",
      "Iteration:139 \n",
      "Loss:0.22047342360019684\n",
      "Epoch:1 \n",
      "Iteration:140 \n",
      "Loss:0.48403823375701904\n",
      "Epoch:1 \n",
      "Iteration:141 \n",
      "Loss:0.15702365338802338\n",
      "Epoch:1 \n",
      "Iteration:142 \n",
      "Loss:0.2312907576560974\n",
      "Epoch:1 \n",
      "Iteration:143 \n",
      "Loss:0.319017618894577\n",
      "Epoch:1 \n",
      "Iteration:144 \n",
      "Loss:0.30859729647636414\n",
      "Epoch:1 \n",
      "Iteration:145 \n",
      "Loss:0.255501389503479\n",
      "Epoch:1 \n",
      "Iteration:146 \n",
      "Loss:0.19951361417770386\n",
      "Epoch:1 \n",
      "Iteration:147 \n",
      "Loss:0.15566644072532654\n",
      "Epoch:1 \n",
      "Iteration:148 \n",
      "Loss:0.291554719209671\n",
      "Epoch:1 \n",
      "Iteration:149 \n",
      "Loss:0.2254704236984253\n",
      "Epoch:1 \n",
      "Iteration:150 \n",
      "Loss:0.3103751838207245\n",
      "Epoch:1 \n",
      "Iteration:151 \n",
      "Loss:0.23330779373645782\n",
      "Epoch:1 \n",
      "Iteration:152 \n",
      "Loss:0.19241328537464142\n",
      "Epoch:1 \n",
      "Iteration:153 \n",
      "Loss:0.275493860244751\n",
      "Epoch:1 \n",
      "Iteration:154 \n",
      "Loss:0.16837461292743683\n",
      "Epoch:1 \n",
      "Iteration:155 \n",
      "Loss:0.19802793860435486\n",
      "Epoch:1 \n",
      "Iteration:156 \n",
      "Loss:0.3491751551628113\n",
      "Epoch:1 \n",
      "Iteration:157 \n",
      "Loss:0.43757039308547974\n",
      "Epoch:1 \n",
      "Iteration:158 \n",
      "Loss:0.1347665637731552\n",
      "Epoch:1 \n",
      "Iteration:159 \n",
      "Loss:0.06288599222898483\n",
      "Epoch:1 \n",
      "Iteration:160 \n",
      "Loss:0.3957083523273468\n",
      "Epoch:1 \n",
      "Iteration:161 \n",
      "Loss:0.3086799383163452\n",
      "Epoch:1 \n",
      "Iteration:162 \n",
      "Loss:0.12891536951065063\n",
      "Epoch:1 \n",
      "Iteration:163 \n",
      "Loss:0.1177622377872467\n",
      "Epoch:1 \n",
      "Iteration:164 \n",
      "Loss:0.08659529685974121\n",
      "Epoch:1 \n",
      "Iteration:165 \n",
      "Loss:0.21118642389774323\n",
      "Epoch:1 \n",
      "Iteration:166 \n",
      "Loss:0.2700453996658325\n",
      "Epoch:1 \n",
      "Iteration:167 \n",
      "Loss:0.31985050439834595\n",
      "Epoch:1 \n",
      "Iteration:168 \n",
      "Loss:0.19454088807106018\n",
      "Epoch:1 \n",
      "Iteration:169 \n",
      "Loss:0.19190970063209534\n",
      "Epoch:1 \n",
      "Iteration:170 \n",
      "Loss:0.26456591486930847\n",
      "Epoch:1 \n",
      "Iteration:171 \n",
      "Loss:0.27670493721961975\n",
      "Epoch:1 \n",
      "Iteration:172 \n",
      "Loss:0.31067776679992676\n",
      "Epoch:1 \n",
      "Iteration:173 \n",
      "Loss:0.2188132256269455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \n",
      "Iteration:174 \n",
      "Loss:0.23364071547985077\n",
      "Epoch:1 \n",
      "Iteration:175 \n",
      "Loss:0.1508840024471283\n",
      "Epoch:1 \n",
      "Iteration:176 \n",
      "Loss:0.23261654376983643\n",
      "Epoch:1 \n",
      "Iteration:177 \n",
      "Loss:0.23869295418262482\n",
      "Epoch:1 \n",
      "Iteration:178 \n",
      "Loss:0.18907514214515686\n",
      "Epoch:1 \n",
      "Iteration:179 \n",
      "Loss:0.16858088970184326\n",
      "Epoch:1 \n",
      "Iteration:180 \n",
      "Loss:0.3811591863632202\n",
      "Epoch:1 \n",
      "Iteration:181 \n",
      "Loss:0.124379463493824\n",
      "Epoch:1 \n",
      "Iteration:182 \n",
      "Loss:0.31516680121421814\n",
      "Epoch:1 \n",
      "Iteration:183 \n",
      "Loss:0.31521129608154297\n",
      "Epoch:1 \n",
      "Iteration:184 \n",
      "Loss:0.1377878487110138\n",
      "Epoch:1 \n",
      "Iteration:185 \n",
      "Loss:0.18656201660633087\n",
      "Epoch:1 \n",
      "Iteration:186 \n",
      "Loss:0.1332310140132904\n",
      "Epoch:1 \n",
      "Iteration:187 \n",
      "Loss:0.19603176414966583\n",
      "Epoch:1 \n",
      "Iteration:188 \n",
      "Loss:0.19425024092197418\n",
      "Epoch:1 \n",
      "Iteration:189 \n",
      "Loss:0.13017690181732178\n",
      "Epoch:1 \n",
      "Iteration:190 \n",
      "Loss:0.13372930884361267\n",
      "Epoch:1 \n",
      "Iteration:191 \n",
      "Loss:0.1804359406232834\n",
      "Epoch:1 \n",
      "Iteration:192 \n",
      "Loss:0.2930707633495331\n",
      "Epoch:1 \n",
      "Iteration:193 \n",
      "Loss:0.14517799019813538\n",
      "Epoch:1 \n",
      "Iteration:194 \n",
      "Loss:0.32116764783859253\n",
      "Epoch:1 \n",
      "Iteration:195 \n",
      "Loss:0.14592529833316803\n",
      "Epoch:1 \n",
      "Iteration:196 \n",
      "Loss:0.1323719173669815\n",
      "Epoch:1 \n",
      "Iteration:197 \n",
      "Loss:0.23956510424613953\n",
      "Epoch:1 \n",
      "Iteration:198 \n",
      "Loss:0.1127374917268753\n",
      "Epoch:1 \n",
      "Iteration:199 \n",
      "Loss:0.1258353739976883\n",
      "Epoch:1 \n",
      "Iteration:200 \n",
      "Loss:0.13518422842025757\n",
      "Epoch:1 \n",
      "Iteration:201 \n",
      "Loss:0.19253356754779816\n",
      "Epoch:1 \n",
      "Iteration:202 \n",
      "Loss:0.2624629735946655\n",
      "Epoch:1 \n",
      "Iteration:203 \n",
      "Loss:0.19795076549053192\n",
      "Epoch:1 \n",
      "Iteration:204 \n",
      "Loss:0.22124363481998444\n",
      "Epoch:1 \n",
      "Iteration:205 \n",
      "Loss:0.13537326455116272\n",
      "Epoch:1 \n",
      "Iteration:206 \n",
      "Loss:0.07512052357196808\n",
      "Epoch:1 \n",
      "Iteration:207 \n",
      "Loss:0.3330065608024597\n",
      "Epoch:1 \n",
      "Iteration:208 \n",
      "Loss:0.30471986532211304\n",
      "Epoch:1 \n",
      "Iteration:209 \n",
      "Loss:0.16882148385047913\n",
      "Epoch:1 \n",
      "Iteration:210 \n",
      "Loss:0.27691519260406494\n",
      "Epoch:1 \n",
      "Iteration:211 \n",
      "Loss:0.1952899694442749\n",
      "Epoch:1 \n",
      "Iteration:212 \n",
      "Loss:0.19514437019824982\n",
      "Epoch:1 \n",
      "Iteration:213 \n",
      "Loss:0.2352014183998108\n",
      "Epoch:1 \n",
      "Iteration:214 \n",
      "Loss:0.12370365113019943\n",
      "Epoch:1 \n",
      "Iteration:215 \n",
      "Loss:0.274437814950943\n",
      "Epoch:1 \n",
      "Iteration:216 \n",
      "Loss:0.06159636005759239\n",
      "Epoch:1 \n",
      "Iteration:217 \n",
      "Loss:0.32039517164230347\n",
      "Epoch:1 \n",
      "Iteration:218 \n",
      "Loss:0.1663617342710495\n",
      "Epoch:1 \n",
      "Iteration:219 \n",
      "Loss:0.1241391971707344\n",
      "Epoch:1 \n",
      "Iteration:220 \n",
      "Loss:0.2916605472564697\n",
      "Epoch:1 \n",
      "Iteration:221 \n",
      "Loss:0.26142963767051697\n",
      "Epoch:1 \n",
      "Iteration:222 \n",
      "Loss:0.1519048511981964\n",
      "Epoch:1 \n",
      "Iteration:223 \n",
      "Loss:0.16335809230804443\n",
      "Epoch:1 \n",
      "Iteration:224 \n",
      "Loss:0.07499317079782486\n",
      "Epoch:1 \n",
      "Iteration:225 \n",
      "Loss:0.11926721781492233\n",
      "Epoch:1 \n",
      "Iteration:226 \n",
      "Loss:0.1341986507177353\n",
      "Epoch:1 \n",
      "Iteration:227 \n",
      "Loss:0.21553756296634674\n",
      "Epoch:1 \n",
      "Iteration:228 \n",
      "Loss:0.2085375338792801\n",
      "Epoch:1 \n",
      "Iteration:229 \n",
      "Loss:0.13790622353553772\n",
      "Epoch:1 \n",
      "Iteration:230 \n",
      "Loss:0.14417821168899536\n",
      "Epoch:1 \n",
      "Iteration:231 \n",
      "Loss:0.13521558046340942\n",
      "Epoch:1 \n",
      "Iteration:232 \n",
      "Loss:0.25427722930908203\n",
      "Epoch:1 \n",
      "Iteration:233 \n",
      "Loss:0.061873696744441986\n",
      "Epoch:1 \n",
      "Iteration:234 \n",
      "Loss:0.12129952758550644\n",
      "Epoch:1 \n",
      "Iteration:235 \n",
      "Loss:0.08585207164287567\n",
      "Epoch:1 \n",
      "Iteration:236 \n",
      "Loss:0.09578448534011841\n",
      "Epoch:1 \n",
      "Iteration:237 \n",
      "Loss:0.24739769101142883\n",
      "Epoch:1 \n",
      "Iteration:238 \n",
      "Loss:0.20082828402519226\n",
      "Epoch:1 \n",
      "Iteration:239 \n",
      "Loss:0.16660286486148834\n",
      "Epoch:1 \n",
      "Iteration:240 \n",
      "Loss:0.12448081374168396\n",
      "Epoch:1 \n",
      "Iteration:241 \n",
      "Loss:0.1126178577542305\n",
      "Epoch:1 \n",
      "Iteration:242 \n",
      "Loss:0.1494017094373703\n",
      "Epoch:1 \n",
      "Iteration:243 \n",
      "Loss:0.061474427580833435\n",
      "Epoch:1 \n",
      "Iteration:244 \n",
      "Loss:0.08451125025749207\n",
      "Epoch:1 \n",
      "Iteration:245 \n",
      "Loss:0.17013268172740936\n",
      "Epoch:1 \n",
      "Iteration:246 \n",
      "Loss:0.18701007962226868\n",
      "Epoch:1 \n",
      "Iteration:247 \n",
      "Loss:0.16942645609378815\n",
      "Epoch:1 \n",
      "Iteration:248 \n",
      "Loss:0.13958017528057098\n",
      "Epoch:1 \n",
      "Iteration:249 \n",
      "Loss:0.084477499127388\n",
      "Epoch:1 \n",
      "Iteration:250 \n",
      "Loss:0.1355639100074768\n",
      "Epoch:1 \n",
      "Iteration:251 \n",
      "Loss:0.22790712118148804\n",
      "Epoch:1 \n",
      "Iteration:252 \n",
      "Loss:0.16315774619579315\n",
      "Epoch:1 \n",
      "Iteration:253 \n",
      "Loss:0.16369369626045227\n",
      "Epoch:1 \n",
      "Iteration:254 \n",
      "Loss:0.1281852126121521\n",
      "Epoch:1 \n",
      "Iteration:255 \n",
      "Loss:0.34571897983551025\n",
      "Epoch:1 \n",
      "Iteration:256 \n",
      "Loss:0.21460860967636108\n",
      "Epoch:1 \n",
      "Iteration:257 \n",
      "Loss:0.3529081344604492\n",
      "Epoch:1 \n",
      "Iteration:258 \n",
      "Loss:0.14152777194976807\n",
      "Epoch:1 \n",
      "Iteration:259 \n",
      "Loss:0.20369009673595428\n",
      "Epoch:1 \n",
      "Iteration:260 \n",
      "Loss:0.11168790608644485\n",
      "Epoch:1 \n",
      "Iteration:261 \n",
      "Loss:0.20936529338359833\n",
      "Epoch:1 \n",
      "Iteration:262 \n",
      "Loss:0.1992996633052826\n",
      "Epoch:1 \n",
      "Iteration:263 \n",
      "Loss:0.2548293173313141\n",
      "Epoch:1 \n",
      "Iteration:264 \n",
      "Loss:0.22339364886283875\n",
      "Epoch:1 \n",
      "Iteration:265 \n",
      "Loss:0.2226775586605072\n",
      "Epoch:1 \n",
      "Iteration:266 \n",
      "Loss:0.15242652595043182\n",
      "Epoch:1 \n",
      "Iteration:267 \n",
      "Loss:0.11700218170881271\n",
      "Epoch:1 \n",
      "Iteration:268 \n",
      "Loss:0.07112956047058105\n",
      "Epoch:1 \n",
      "Iteration:269 \n",
      "Loss:0.16115371882915497\n",
      "Epoch:1 \n",
      "Iteration:270 \n",
      "Loss:0.09328000247478485\n",
      "Epoch:1 \n",
      "Iteration:271 \n",
      "Loss:0.11228461563587189\n",
      "Epoch:1 \n",
      "Iteration:272 \n",
      "Loss:0.09753935039043427\n",
      "Epoch:1 \n",
      "Iteration:273 \n",
      "Loss:0.08504918962717056\n",
      "Epoch:1 \n",
      "Iteration:274 \n",
      "Loss:0.18780617415905\n",
      "Epoch:1 \n",
      "Iteration:275 \n",
      "Loss:0.12763231992721558\n",
      "Epoch:1 \n",
      "Iteration:276 \n",
      "Loss:0.15267400443553925\n",
      "Epoch:1 \n",
      "Iteration:277 \n",
      "Loss:0.1715623140335083\n",
      "Epoch:1 \n",
      "Iteration:278 \n",
      "Loss:0.18402642011642456\n",
      "Epoch:1 \n",
      "Iteration:279 \n",
      "Loss:0.21424001455307007\n",
      "Epoch:1 \n",
      "Iteration:280 \n",
      "Loss:0.08497539162635803\n",
      "Epoch:1 \n",
      "Iteration:281 \n",
      "Loss:0.1401849389076233\n",
      "Epoch:1 \n",
      "Iteration:282 \n",
      "Loss:0.13807371258735657\n",
      "Epoch:1 \n",
      "Iteration:283 \n",
      "Loss:0.15613220632076263\n",
      "Epoch:1 \n",
      "Iteration:284 \n",
      "Loss:0.17785942554473877\n",
      "Epoch:1 \n",
      "Iteration:285 \n",
      "Loss:0.29615554213523865\n",
      "Epoch:1 \n",
      "Iteration:286 \n",
      "Loss:0.2421244978904724\n",
      "Epoch:1 \n",
      "Iteration:287 \n",
      "Loss:0.17601080238819122\n",
      "Epoch:1 \n",
      "Iteration:288 \n",
      "Loss:0.16622912883758545\n",
      "Epoch:1 \n",
      "Iteration:289 \n",
      "Loss:0.32603874802589417\n",
      "Epoch:1 \n",
      "Iteration:290 \n",
      "Loss:0.1167239248752594\n",
      "Epoch:1 \n",
      "Iteration:291 \n",
      "Loss:0.1310199797153473\n",
      "Epoch:1 \n",
      "Iteration:292 \n",
      "Loss:0.14816546440124512\n",
      "Epoch:1 \n",
      "Iteration:293 \n",
      "Loss:0.1895904541015625\n",
      "Epoch:1 \n",
      "Iteration:294 \n",
      "Loss:0.17430013418197632\n",
      "Epoch:1 \n",
      "Iteration:295 \n",
      "Loss:0.30275866389274597\n",
      "Epoch:1 \n",
      "Iteration:296 \n",
      "Loss:0.1549168825149536\n",
      "Epoch:1 \n",
      "Iteration:297 \n",
      "Loss:0.2380707859992981\n",
      "Epoch:1 \n",
      "Iteration:298 \n",
      "Loss:0.11344513297080994\n",
      "Epoch:1 \n",
      "Iteration:299 \n",
      "Loss:0.2677982449531555\n",
      "Epoch:1 \n",
      "Iteration:300 \n",
      "Loss:0.17318986356258392\n",
      "Epoch:1 \n",
      "Iteration:301 \n",
      "Loss:0.16743168234825134\n",
      "Epoch:1 \n",
      "Iteration:302 \n",
      "Loss:0.05891239270567894\n",
      "Epoch:1 \n",
      "Iteration:303 \n",
      "Loss:0.1507492959499359\n",
      "Epoch:1 \n",
      "Iteration:304 \n",
      "Loss:0.19562767446041107\n",
      "Epoch:1 \n",
      "Iteration:305 \n",
      "Loss:0.04008667171001434\n",
      "Epoch:1 \n",
      "Iteration:306 \n",
      "Loss:0.1803441196680069\n",
      "Epoch:1 \n",
      "Iteration:307 \n",
      "Loss:0.19751280546188354\n",
      "Epoch:1 \n",
      "Iteration:308 \n",
      "Loss:0.18159769475460052\n",
      "Epoch:1 \n",
      "Iteration:309 \n",
      "Loss:0.09003745764493942\n",
      "Epoch:1 \n",
      "Iteration:310 \n",
      "Loss:0.17185933887958527\n",
      "Epoch:1 \n",
      "Iteration:311 \n",
      "Loss:0.10974127799272537\n",
      "Epoch:1 \n",
      "Iteration:312 \n",
      "Loss:0.3070503771305084\n",
      "Epoch:1 \n",
      "Iteration:313 \n",
      "Loss:0.1234973594546318\n",
      "Epoch:1 \n",
      "Iteration:314 \n",
      "Loss:0.13988421857357025\n",
      "Epoch:1 \n",
      "Iteration:315 \n",
      "Loss:0.1775360107421875\n",
      "Epoch:1 \n",
      "Iteration:316 \n",
      "Loss:0.14908771216869354\n",
      "Epoch:1 \n",
      "Iteration:317 \n",
      "Loss:0.08358968794345856\n",
      "Epoch:1 \n",
      "Iteration:318 \n",
      "Loss:0.16719035804271698\n",
      "Epoch:1 \n",
      "Iteration:319 \n",
      "Loss:0.13801692426204681\n",
      "Epoch:1 \n",
      "Iteration:320 \n",
      "Loss:0.15283483266830444\n",
      "Epoch:1 \n",
      "Iteration:321 \n",
      "Loss:0.09023714065551758\n",
      "Epoch:1 \n",
      "Iteration:322 \n",
      "Loss:0.12085916846990585\n",
      "Epoch:1 \n",
      "Iteration:323 \n",
      "Loss:0.19208984076976776\n",
      "Epoch:1 \n",
      "Iteration:324 \n",
      "Loss:0.10534988343715668\n",
      "Epoch:1 \n",
      "Iteration:325 \n",
      "Loss:0.09955122321844101\n",
      "Epoch:1 \n",
      "Iteration:326 \n",
      "Loss:0.10314197838306427\n",
      "Epoch:1 \n",
      "Iteration:327 \n",
      "Loss:0.19099943339824677\n",
      "Epoch:1 \n",
      "Iteration:328 \n",
      "Loss:0.15097354352474213\n",
      "Epoch:1 \n",
      "Iteration:329 \n",
      "Loss:0.05294886603951454\n",
      "Epoch:1 \n",
      "Iteration:330 \n",
      "Loss:0.1789417862892151\n",
      "Epoch:1 \n",
      "Iteration:331 \n",
      "Loss:0.13349762558937073\n",
      "Epoch:1 \n",
      "Iteration:332 \n",
      "Loss:0.08608779311180115\n",
      "Epoch:1 \n",
      "Iteration:333 \n",
      "Loss:0.2064773440361023\n",
      "Epoch:1 \n",
      "Iteration:334 \n",
      "Loss:0.17000742256641388\n",
      "Epoch:1 \n",
      "Iteration:335 \n",
      "Loss:0.3031696379184723\n",
      "Epoch:1 \n",
      "Iteration:336 \n",
      "Loss:0.1303916722536087\n",
      "Epoch:1 \n",
      "Iteration:337 \n",
      "Loss:0.15825255215168\n",
      "Epoch:1 \n",
      "Iteration:338 \n",
      "Loss:0.24353379011154175\n",
      "Epoch:1 \n",
      "Iteration:339 \n",
      "Loss:0.23819094896316528\n",
      "Epoch:1 \n",
      "Iteration:340 \n",
      "Loss:0.23188480734825134\n",
      "Epoch:1 \n",
      "Iteration:341 \n",
      "Loss:0.23926052451133728\n",
      "Epoch:1 \n",
      "Iteration:342 \n",
      "Loss:0.1374325305223465\n",
      "Epoch:1 \n",
      "Iteration:343 \n",
      "Loss:0.12234268337488174\n",
      "Epoch:1 \n",
      "Iteration:344 \n",
      "Loss:0.08939297497272491\n",
      "Epoch:1 \n",
      "Iteration:345 \n",
      "Loss:0.1573348492383957\n",
      "Epoch:1 \n",
      "Iteration:346 \n",
      "Loss:0.04774055629968643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \n",
      "Iteration:347 \n",
      "Loss:0.31348198652267456\n",
      "Epoch:1 \n",
      "Iteration:348 \n",
      "Loss:0.12617792189121246\n",
      "Epoch:1 \n",
      "Iteration:349 \n",
      "Loss:0.12596957385540009\n",
      "Epoch:1 \n",
      "Iteration:350 \n",
      "Loss:0.23239940404891968\n",
      "Epoch:1 \n",
      "Iteration:351 \n",
      "Loss:0.22754289209842682\n",
      "Epoch:1 \n",
      "Iteration:352 \n",
      "Loss:0.10897237807512283\n",
      "Epoch:1 \n",
      "Iteration:353 \n",
      "Loss:0.12211742252111435\n",
      "Epoch:1 \n",
      "Iteration:354 \n",
      "Loss:0.17609861493110657\n",
      "Epoch:1 \n",
      "Iteration:355 \n",
      "Loss:0.16741515696048737\n",
      "Epoch:1 \n",
      "Iteration:356 \n",
      "Loss:0.1579798460006714\n",
      "Epoch:1 \n",
      "Iteration:357 \n",
      "Loss:0.1322966068983078\n",
      "Epoch:1 \n",
      "Iteration:358 \n",
      "Loss:0.17488032579421997\n",
      "Epoch:1 \n",
      "Iteration:359 \n",
      "Loss:0.16777494549751282\n",
      "Epoch:1 \n",
      "Iteration:360 \n",
      "Loss:0.14855535328388214\n",
      "Epoch:1 \n",
      "Iteration:361 \n",
      "Loss:0.22715777158737183\n",
      "Epoch:1 \n",
      "Iteration:362 \n",
      "Loss:0.15137185156345367\n",
      "Epoch:1 \n",
      "Iteration:363 \n",
      "Loss:0.3162267804145813\n",
      "Epoch:1 \n",
      "Iteration:364 \n",
      "Loss:0.1435621976852417\n",
      "Epoch:1 \n",
      "Iteration:365 \n",
      "Loss:0.11869576573371887\n",
      "Epoch:1 \n",
      "Iteration:366 \n",
      "Loss:0.06551741808652878\n",
      "Epoch:1 \n",
      "Iteration:367 \n",
      "Loss:0.12392859160900116\n",
      "Epoch:1 \n",
      "Iteration:368 \n",
      "Loss:0.1441977471113205\n",
      "Epoch:1 \n",
      "Iteration:369 \n",
      "Loss:0.17136967182159424\n",
      "Epoch:1 \n",
      "Iteration:370 \n",
      "Loss:0.3830975592136383\n",
      "Epoch:1 \n",
      "Iteration:371 \n",
      "Loss:0.12549568712711334\n",
      "Epoch:1 \n",
      "Iteration:372 \n",
      "Loss:0.09441132843494415\n",
      "Epoch:1 \n",
      "Iteration:373 \n",
      "Loss:0.11308066546916962\n",
      "Epoch:1 \n",
      "Iteration:374 \n",
      "Loss:0.2946067750453949\n",
      "Epoch:1 \n",
      "Iteration:375 \n",
      "Loss:0.30554139614105225\n",
      "Epoch:1 \n",
      "Iteration:376 \n",
      "Loss:0.29851454496383667\n",
      "Epoch:1 \n",
      "Iteration:377 \n",
      "Loss:0.18765129148960114\n",
      "Epoch:1 \n",
      "Iteration:378 \n",
      "Loss:0.14548975229263306\n",
      "Epoch:1 \n",
      "Iteration:379 \n",
      "Loss:0.23775292932987213\n",
      "Epoch:1 \n",
      "Iteration:380 \n",
      "Loss:0.16772422194480896\n",
      "Epoch:1 \n",
      "Iteration:381 \n",
      "Loss:0.16343961656093597\n",
      "Epoch:1 \n",
      "Iteration:382 \n",
      "Loss:0.0664227083325386\n",
      "Epoch:1 \n",
      "Iteration:383 \n",
      "Loss:0.251888632774353\n",
      "Epoch:1 \n",
      "Iteration:384 \n",
      "Loss:0.17794324457645416\n",
      "Epoch:1 \n",
      "Iteration:385 \n",
      "Loss:0.14521580934524536\n",
      "Epoch:1 \n",
      "Iteration:386 \n",
      "Loss:0.12497042864561081\n",
      "Epoch:1 \n",
      "Iteration:387 \n",
      "Loss:0.2176038920879364\n",
      "Epoch:1 \n",
      "Iteration:388 \n",
      "Loss:0.12465760856866837\n",
      "Epoch:1 \n",
      "Iteration:389 \n",
      "Loss:0.08923342823982239\n",
      "Epoch:1 \n",
      "Iteration:390 \n",
      "Loss:0.23708225786685944\n",
      "Epoch:1 \n",
      "Iteration:391 \n",
      "Loss:0.18922735750675201\n",
      "Epoch:1 \n",
      "Iteration:392 \n",
      "Loss:0.07804815471172333\n",
      "Epoch:1 \n",
      "Iteration:393 \n",
      "Loss:0.18437808752059937\n",
      "Epoch:1 \n",
      "Iteration:394 \n",
      "Loss:0.13805130124092102\n",
      "Epoch:1 \n",
      "Iteration:395 \n",
      "Loss:0.16958487033843994\n",
      "Epoch:1 \n",
      "Iteration:396 \n",
      "Loss:0.16531363129615784\n",
      "Epoch:1 \n",
      "Iteration:397 \n",
      "Loss:0.12704482674598694\n",
      "Epoch:1 \n",
      "Iteration:398 \n",
      "Loss:0.1614457368850708\n",
      "Epoch:1 \n",
      "Iteration:399 \n",
      "Loss:0.22057953476905823\n",
      "Epoch:1 \n",
      "Iteration:400 \n",
      "Loss:0.14540232717990875\n",
      "Epoch:1 \n",
      "Iteration:401 \n",
      "Loss:0.0864008367061615\n",
      "Epoch:1 \n",
      "Iteration:402 \n",
      "Loss:0.14857891201972961\n",
      "Epoch:1 \n",
      "Iteration:403 \n",
      "Loss:0.07551080733537674\n",
      "Epoch:1 \n",
      "Iteration:404 \n",
      "Loss:0.14636777341365814\n",
      "Epoch:1 \n",
      "Iteration:405 \n",
      "Loss:0.22663001716136932\n",
      "Epoch:1 \n",
      "Iteration:406 \n",
      "Loss:0.1538436859846115\n",
      "Epoch:1 \n",
      "Iteration:407 \n",
      "Loss:0.09959450364112854\n",
      "Epoch:1 \n",
      "Iteration:408 \n",
      "Loss:0.24725183844566345\n",
      "Epoch:1 \n",
      "Iteration:409 \n",
      "Loss:0.27957770228385925\n",
      "Epoch:1 \n",
      "Iteration:410 \n",
      "Loss:0.07834064960479736\n",
      "Epoch:1 \n",
      "Iteration:411 \n",
      "Loss:0.14370116591453552\n",
      "Epoch:1 \n",
      "Iteration:412 \n",
      "Loss:0.20632970333099365\n",
      "Epoch:1 \n",
      "Iteration:413 \n",
      "Loss:0.18289625644683838\n",
      "Epoch:1 \n",
      "Iteration:414 \n",
      "Loss:0.22489890456199646\n",
      "Epoch:1 \n",
      "Iteration:415 \n",
      "Loss:0.16003383696079254\n",
      "Epoch:1 \n",
      "Iteration:416 \n",
      "Loss:0.2523242235183716\n",
      "Epoch:1 \n",
      "Iteration:417 \n",
      "Loss:0.28450965881347656\n",
      "Epoch:1 \n",
      "Iteration:418 \n",
      "Loss:0.24086427688598633\n",
      "Epoch:1 \n",
      "Iteration:419 \n",
      "Loss:0.12649831175804138\n",
      "Epoch:1 \n",
      "Iteration:420 \n",
      "Loss:0.18169528245925903\n",
      "Epoch:1 \n",
      "Iteration:421 \n",
      "Loss:0.18053504824638367\n",
      "Epoch:1 \n",
      "Iteration:422 \n",
      "Loss:0.1623576581478119\n",
      "Epoch:1 \n",
      "Iteration:423 \n",
      "Loss:0.09776506572961807\n",
      "Epoch:1 \n",
      "Iteration:424 \n",
      "Loss:0.23213250935077667\n",
      "Epoch:1 \n",
      "Iteration:425 \n",
      "Loss:0.16054318845272064\n",
      "Epoch:1 \n",
      "Iteration:426 \n",
      "Loss:0.15931454300880432\n",
      "Epoch:1 \n",
      "Iteration:427 \n",
      "Loss:0.06718993932008743\n",
      "Epoch:1 \n",
      "Iteration:428 \n",
      "Loss:0.13157907128334045\n",
      "Epoch:1 \n",
      "Iteration:429 \n",
      "Loss:0.15056023001670837\n",
      "Epoch:1 \n",
      "Iteration:430 \n",
      "Loss:0.13571284711360931\n",
      "Epoch:1 \n",
      "Iteration:431 \n",
      "Loss:0.1203511655330658\n",
      "Epoch:1 \n",
      "Iteration:432 \n",
      "Loss:0.1570037603378296\n",
      "Epoch:1 \n",
      "Iteration:433 \n",
      "Loss:0.11855635046958923\n",
      "Epoch:1 \n",
      "Iteration:434 \n",
      "Loss:0.09084481000900269\n",
      "Epoch:1 \n",
      "Iteration:435 \n",
      "Loss:0.19788722693920135\n",
      "Epoch:1 \n",
      "Iteration:436 \n",
      "Loss:0.09556737542152405\n",
      "Epoch:1 \n",
      "Iteration:437 \n",
      "Loss:0.2238909751176834\n",
      "Epoch:1 \n",
      "Iteration:438 \n",
      "Loss:0.11157716065645218\n",
      "Epoch:1 \n",
      "Iteration:439 \n",
      "Loss:0.08179385960102081\n",
      "Epoch:1 \n",
      "Iteration:440 \n",
      "Loss:0.0864664614200592\n",
      "Epoch:1 \n",
      "Iteration:441 \n",
      "Loss:0.11941032111644745\n",
      "Epoch:1 \n",
      "Iteration:442 \n",
      "Loss:0.09747246652841568\n",
      "Epoch:1 \n",
      "Iteration:443 \n",
      "Loss:0.14056435227394104\n",
      "Epoch:1 \n",
      "Iteration:444 \n",
      "Loss:0.12694060802459717\n",
      "Epoch:1 \n",
      "Iteration:445 \n",
      "Loss:0.17020730674266815\n",
      "Epoch:1 \n",
      "Iteration:446 \n",
      "Loss:0.15442584455013275\n",
      "Epoch:1 \n",
      "Iteration:447 \n",
      "Loss:0.1218116283416748\n",
      "Epoch:1 \n",
      "Iteration:448 \n",
      "Loss:0.1488742083311081\n",
      "Epoch:1 \n",
      "Iteration:449 \n",
      "Loss:0.141063392162323\n",
      "Epoch:1 \n",
      "Iteration:450 \n",
      "Loss:0.05923749506473541\n",
      "Epoch:1 \n",
      "Iteration:451 \n",
      "Loss:0.1407221555709839\n",
      "Epoch:1 \n",
      "Iteration:452 \n",
      "Loss:0.08396798372268677\n",
      "Epoch:1 \n",
      "Iteration:453 \n",
      "Loss:0.12817895412445068\n",
      "Epoch:1 \n",
      "Iteration:454 \n",
      "Loss:0.14802919328212738\n",
      "Epoch:1 \n",
      "Iteration:455 \n",
      "Loss:0.06963115930557251\n",
      "Epoch:1 \n",
      "Iteration:456 \n",
      "Loss:0.07898536324501038\n",
      "Epoch:1 \n",
      "Iteration:457 \n",
      "Loss:0.018804341554641724\n",
      "Epoch:1 \n",
      "Iteration:458 \n",
      "Loss:0.18707171082496643\n",
      "Epoch:1 \n",
      "Iteration:459 \n",
      "Loss:0.20028729736804962\n",
      "Epoch:1 \n",
      "Iteration:460 \n",
      "Loss:0.26463189721107483\n",
      "Epoch:1 \n",
      "Iteration:461 \n",
      "Loss:0.06801345944404602\n",
      "Epoch:1 \n",
      "Iteration:462 \n",
      "Loss:0.23379573225975037\n",
      "Epoch:1 \n",
      "Iteration:463 \n",
      "Loss:0.09285418689250946\n",
      "Epoch:1 \n",
      "Iteration:464 \n",
      "Loss:0.1288536787033081\n",
      "Epoch:1 \n",
      "Iteration:465 \n",
      "Loss:0.19858354330062866\n",
      "Epoch:1 \n",
      "Iteration:466 \n",
      "Loss:0.10527375340461731\n",
      "Epoch:1 \n",
      "Iteration:467 \n",
      "Loss:0.21339210867881775\n",
      "Epoch:1 \n",
      "Iteration:468 \n",
      "Loss:0.10691437125205994\n",
      "Epoch:1 \n",
      "Iteration:469 \n",
      "Loss:0.17181193828582764\n",
      "Epoch:1 \n",
      "Iteration:470 \n",
      "Loss:0.12533560395240784\n",
      "Epoch:1 \n",
      "Iteration:471 \n",
      "Loss:0.05817952752113342\n",
      "Epoch:1 \n",
      "Iteration:472 \n",
      "Loss:0.16680331528186798\n",
      "Epoch:1 \n",
      "Iteration:473 \n",
      "Loss:0.09557946026325226\n",
      "Epoch:1 \n",
      "Iteration:474 \n",
      "Loss:0.15664370357990265\n",
      "Epoch:1 \n",
      "Iteration:475 \n",
      "Loss:0.12586389482021332\n",
      "Epoch:1 \n",
      "Iteration:476 \n",
      "Loss:0.1069115623831749\n",
      "Epoch:1 \n",
      "Iteration:477 \n",
      "Loss:0.04154718294739723\n",
      "Epoch:1 \n",
      "Iteration:478 \n",
      "Loss:0.18392904102802277\n",
      "Epoch:1 \n",
      "Iteration:479 \n",
      "Loss:0.13265304267406464\n",
      "Epoch:1 \n",
      "Iteration:480 \n",
      "Loss:0.10919620841741562\n",
      "Epoch:1 \n",
      "Iteration:481 \n",
      "Loss:0.11138859391212463\n",
      "Epoch:1 \n",
      "Iteration:482 \n",
      "Loss:0.07535132765769958\n",
      "Epoch:1 \n",
      "Iteration:483 \n",
      "Loss:0.0642189085483551\n",
      "Epoch:1 \n",
      "Iteration:484 \n",
      "Loss:0.16047172248363495\n",
      "Epoch:1 \n",
      "Iteration:485 \n",
      "Loss:0.1741226613521576\n",
      "Epoch:1 \n",
      "Iteration:486 \n",
      "Loss:0.06477146595716476\n",
      "Epoch:1 \n",
      "Iteration:487 \n",
      "Loss:0.17340566217899323\n",
      "Epoch:1 \n",
      "Iteration:488 \n",
      "Loss:0.10121779143810272\n",
      "Epoch:1 \n",
      "Iteration:489 \n",
      "Loss:0.17965959012508392\n",
      "Epoch:1 \n",
      "Iteration:490 \n",
      "Loss:0.058013156056404114\n",
      "Epoch:1 \n",
      "Iteration:491 \n",
      "Loss:0.12729431688785553\n",
      "Epoch:1 \n",
      "Iteration:492 \n",
      "Loss:0.15683141350746155\n",
      "Epoch:1 \n",
      "Iteration:493 \n",
      "Loss:0.2584000825881958\n",
      "Epoch:1 \n",
      "Iteration:494 \n",
      "Loss:0.053398095071315765\n",
      "Epoch:1 \n",
      "Iteration:495 \n",
      "Loss:0.06947608292102814\n",
      "Epoch:1 \n",
      "Iteration:496 \n",
      "Loss:0.08760039508342743\n",
      "Epoch:1 \n",
      "Iteration:497 \n",
      "Loss:0.16803324222564697\n",
      "Epoch:1 \n",
      "Iteration:498 \n",
      "Loss:0.14669391512870789\n",
      "Epoch:1 \n",
      "Iteration:499 \n",
      "Loss:0.1604669690132141\n",
      "Epoch:1 \n",
      "Iteration:500 \n",
      "Loss:0.03770060837268829\n",
      "Epoch:1 \n",
      "Iteration:501 \n",
      "Loss:0.18763208389282227\n",
      "Epoch:1 \n",
      "Iteration:502 \n",
      "Loss:0.057373832911252975\n",
      "Epoch:1 \n",
      "Iteration:503 \n",
      "Loss:0.11099676787853241\n",
      "Epoch:1 \n",
      "Iteration:504 \n",
      "Loss:0.09875714033842087\n",
      "Epoch:1 \n",
      "Iteration:505 \n",
      "Loss:0.052983954548835754\n",
      "Epoch:1 \n",
      "Iteration:506 \n",
      "Loss:0.05324944481253624\n",
      "Epoch:1 \n",
      "Iteration:507 \n",
      "Loss:0.09516720473766327\n",
      "Epoch:1 \n",
      "Iteration:508 \n",
      "Loss:0.2105506956577301\n",
      "Epoch:1 \n",
      "Iteration:509 \n",
      "Loss:0.16112357378005981\n",
      "Epoch:1 \n",
      "Iteration:510 \n",
      "Loss:0.1232694536447525\n",
      "Epoch:1 \n",
      "Iteration:511 \n",
      "Loss:0.05190546065568924\n",
      "Epoch:1 \n",
      "Iteration:512 \n",
      "Loss:0.12582232058048248\n",
      "Epoch:1 \n",
      "Iteration:513 \n",
      "Loss:0.2602381706237793\n",
      "Epoch:1 \n",
      "Iteration:514 \n",
      "Loss:0.20105312764644623\n",
      "Epoch:1 \n",
      "Iteration:515 \n",
      "Loss:0.11387726664543152\n",
      "Epoch:1 \n",
      "Iteration:516 \n",
      "Loss:0.1497097909450531\n",
      "Epoch:1 \n",
      "Iteration:517 \n",
      "Loss:0.2781929075717926\n",
      "Epoch:1 \n",
      "Iteration:518 \n",
      "Loss:0.14638608694076538\n",
      "Epoch:1 \n",
      "Iteration:519 \n",
      "Loss:0.24004524946212769\n",
      "Epoch:1 \n",
      "Iteration:520 \n",
      "Loss:0.19831907749176025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \n",
      "Iteration:521 \n",
      "Loss:0.05353476479649544\n",
      "Epoch:1 \n",
      "Iteration:522 \n",
      "Loss:0.23325391113758087\n",
      "Epoch:1 \n",
      "Iteration:523 \n",
      "Loss:0.18582992255687714\n",
      "Epoch:1 \n",
      "Iteration:524 \n",
      "Loss:0.09449417889118195\n",
      "Epoch:1 \n",
      "Iteration:525 \n",
      "Loss:0.09688922017812729\n",
      "Epoch:1 \n",
      "Iteration:526 \n",
      "Loss:0.027983758598566055\n",
      "Epoch:1 \n",
      "Iteration:527 \n",
      "Loss:0.11870747804641724\n",
      "Epoch:1 \n",
      "Iteration:528 \n",
      "Loss:0.1011987179517746\n",
      "Epoch:1 \n",
      "Iteration:529 \n",
      "Loss:0.1776442676782608\n",
      "Epoch:1 \n",
      "Iteration:530 \n",
      "Loss:0.14917439222335815\n",
      "Epoch:1 \n",
      "Iteration:531 \n",
      "Loss:0.14289936423301697\n",
      "Epoch:1 \n",
      "Iteration:532 \n",
      "Loss:0.05317782983183861\n",
      "Epoch:1 \n",
      "Iteration:533 \n",
      "Loss:0.14169421792030334\n",
      "Epoch:1 \n",
      "Iteration:534 \n",
      "Loss:0.19296100735664368\n",
      "Epoch:1 \n",
      "Iteration:535 \n",
      "Loss:0.0548328198492527\n",
      "Epoch:1 \n",
      "Iteration:536 \n",
      "Loss:0.11793999373912811\n",
      "Epoch:1 \n",
      "Iteration:537 \n",
      "Loss:0.1974993497133255\n",
      "Epoch:1 \n",
      "Iteration:538 \n",
      "Loss:0.14726309478282928\n",
      "Epoch:1 \n",
      "Iteration:539 \n",
      "Loss:0.05452985689043999\n",
      "Epoch:1 \n",
      "Iteration:540 \n",
      "Loss:0.08887743204832077\n",
      "Epoch:1 \n",
      "Iteration:541 \n",
      "Loss:0.05349624156951904\n",
      "Epoch:1 \n",
      "Iteration:542 \n",
      "Loss:0.09971152245998383\n",
      "Epoch:1 \n",
      "Iteration:543 \n",
      "Loss:0.12198226898908615\n",
      "Epoch:1 \n",
      "Iteration:544 \n",
      "Loss:0.19008606672286987\n",
      "Epoch:1 \n",
      "Iteration:545 \n",
      "Loss:0.1730809360742569\n",
      "Epoch:1 \n",
      "Iteration:546 \n",
      "Loss:0.12990449368953705\n",
      "Epoch:1 \n",
      "Iteration:547 \n",
      "Loss:0.3251248896121979\n",
      "Epoch:1 \n",
      "Iteration:548 \n",
      "Loss:0.04347158968448639\n",
      "Epoch:1 \n",
      "Iteration:549 \n",
      "Loss:0.05970887839794159\n",
      "Epoch:1 \n",
      "Iteration:550 \n",
      "Loss:0.10945221036672592\n",
      "Epoch:1 \n",
      "Iteration:551 \n",
      "Loss:0.15638773143291473\n",
      "Epoch:1 \n",
      "Iteration:552 \n",
      "Loss:0.12025328725576401\n",
      "Epoch:1 \n",
      "Iteration:553 \n",
      "Loss:0.18447792530059814\n",
      "Epoch:1 \n",
      "Iteration:554 \n",
      "Loss:0.09545507282018661\n",
      "Epoch:1 \n",
      "Iteration:555 \n",
      "Loss:0.18379849195480347\n",
      "Epoch:1 \n",
      "Iteration:556 \n",
      "Loss:0.05731698498129845\n",
      "Epoch:1 \n",
      "Iteration:557 \n",
      "Loss:0.17120161652565002\n",
      "Epoch:1 \n",
      "Iteration:558 \n",
      "Loss:0.21065430343151093\n",
      "Epoch:1 \n",
      "Iteration:559 \n",
      "Loss:0.1444886028766632\n",
      "Epoch:1 \n",
      "Iteration:560 \n",
      "Loss:0.08166111260652542\n",
      "Epoch:1 \n",
      "Iteration:561 \n",
      "Loss:0.1046004667878151\n",
      "Epoch:1 \n",
      "Iteration:562 \n",
      "Loss:0.09464655071496964\n",
      "Epoch:1 \n",
      "Iteration:563 \n",
      "Loss:0.12497057020664215\n",
      "Epoch:1 \n",
      "Iteration:564 \n",
      "Loss:0.04581575095653534\n",
      "Epoch:1 \n",
      "Iteration:565 \n",
      "Loss:0.22922943532466888\n",
      "Epoch:1 \n",
      "Iteration:566 \n",
      "Loss:0.28213006258010864\n",
      "Epoch:1 \n",
      "Iteration:567 \n",
      "Loss:0.1906711757183075\n",
      "Epoch:1 \n",
      "Iteration:568 \n",
      "Loss:0.05716053023934364\n",
      "Epoch:1 \n",
      "Iteration:569 \n",
      "Loss:0.08213618397712708\n",
      "Epoch:1 \n",
      "Iteration:570 \n",
      "Loss:0.14924253523349762\n",
      "Epoch:1 \n",
      "Iteration:571 \n",
      "Loss:0.06512930244207382\n",
      "Epoch:1 \n",
      "Iteration:572 \n",
      "Loss:0.05922549217939377\n",
      "Epoch:1 \n",
      "Iteration:573 \n",
      "Loss:0.11661934107542038\n",
      "Epoch:1 \n",
      "Iteration:574 \n",
      "Loss:0.20099715888500214\n",
      "Epoch:1 \n",
      "Iteration:575 \n",
      "Loss:0.1725538671016693\n",
      "Epoch:1 \n",
      "Iteration:576 \n",
      "Loss:0.2321486622095108\n",
      "Epoch:1 \n",
      "Iteration:577 \n",
      "Loss:0.14236022531986237\n",
      "Epoch:1 \n",
      "Iteration:578 \n",
      "Loss:0.10517439246177673\n",
      "Epoch:1 \n",
      "Iteration:579 \n",
      "Loss:0.07205498218536377\n",
      "Epoch:1 \n",
      "Iteration:580 \n",
      "Loss:0.10681230574846268\n",
      "Epoch:1 \n",
      "Iteration:581 \n",
      "Loss:0.1372218132019043\n",
      "Epoch:1 \n",
      "Iteration:582 \n",
      "Loss:0.10903928428888321\n",
      "Epoch:1 \n",
      "Iteration:583 \n",
      "Loss:0.11332202702760696\n",
      "Epoch:1 \n",
      "Iteration:584 \n",
      "Loss:0.08661611378192902\n",
      "Epoch:1 \n",
      "Iteration:585 \n",
      "Loss:0.04033790901303291\n",
      "Epoch:1 \n",
      "Iteration:586 \n",
      "Loss:0.05375756695866585\n",
      "Epoch:1 \n",
      "Iteration:587 \n",
      "Loss:0.07908552139997482\n",
      "Epoch:1 \n",
      "Iteration:588 \n",
      "Loss:0.2648504674434662\n",
      "Epoch:1 \n",
      "Iteration:589 \n",
      "Loss:0.16532693803310394\n",
      "Epoch:1 \n",
      "Iteration:590 \n",
      "Loss:0.14812788367271423\n",
      "Epoch:1 \n",
      "Iteration:591 \n",
      "Loss:0.054632458835840225\n",
      "Epoch:1 \n",
      "Iteration:592 \n",
      "Loss:0.0649971216917038\n",
      "Epoch:1 \n",
      "Iteration:593 \n",
      "Loss:0.09802078455686569\n",
      "Epoch:1 \n",
      "Iteration:594 \n",
      "Loss:0.06874732673168182\n",
      "Epoch:1 \n",
      "Iteration:595 \n",
      "Loss:0.10750783234834671\n",
      "Epoch:1 \n",
      "Iteration:596 \n",
      "Loss:0.065363809466362\n",
      "Epoch:1 \n",
      "Iteration:597 \n",
      "Loss:0.06026590242981911\n",
      "Epoch:1 \n",
      "Iteration:598 \n",
      "Loss:0.07939674705266953\n",
      "Epoch:1 \n",
      "Iteration:599 \n",
      "Loss:0.14536221325397491\n",
      "Epoch:1 \n",
      "Iteration:600 \n",
      "Loss:0.18349143862724304\n",
      "\n",
      "Accuracy of network in epoch 1: 93.03166666666667\n",
      "Epoch:2 \n",
      "Iteration:1 \n",
      "Loss:0.03768860921263695\n",
      "Epoch:2 \n",
      "Iteration:2 \n",
      "Loss:0.1256306916475296\n",
      "Epoch:2 \n",
      "Iteration:3 \n",
      "Loss:0.13695181906223297\n",
      "Epoch:2 \n",
      "Iteration:4 \n",
      "Loss:0.10828784853219986\n",
      "Epoch:2 \n",
      "Iteration:5 \n",
      "Loss:0.09610897302627563\n",
      "Epoch:2 \n",
      "Iteration:6 \n",
      "Loss:0.10789445787668228\n",
      "Epoch:2 \n",
      "Iteration:7 \n",
      "Loss:0.17946983873844147\n",
      "Epoch:2 \n",
      "Iteration:8 \n",
      "Loss:0.11347386986017227\n",
      "Epoch:2 \n",
      "Iteration:9 \n",
      "Loss:0.05046037212014198\n",
      "Epoch:2 \n",
      "Iteration:10 \n",
      "Loss:0.05603288114070892\n",
      "Epoch:2 \n",
      "Iteration:11 \n",
      "Loss:0.049103789031505585\n",
      "Epoch:2 \n",
      "Iteration:12 \n",
      "Loss:0.14062586426734924\n",
      "Epoch:2 \n",
      "Iteration:13 \n",
      "Loss:0.016742099076509476\n",
      "Epoch:2 \n",
      "Iteration:14 \n",
      "Loss:0.14218811690807343\n",
      "Epoch:2 \n",
      "Iteration:15 \n",
      "Loss:0.0857653021812439\n",
      "Epoch:2 \n",
      "Iteration:16 \n",
      "Loss:0.10725483298301697\n",
      "Epoch:2 \n",
      "Iteration:17 \n",
      "Loss:0.06952115893363953\n",
      "Epoch:2 \n",
      "Iteration:18 \n",
      "Loss:0.08461152017116547\n",
      "Epoch:2 \n",
      "Iteration:19 \n",
      "Loss:0.0807526558637619\n",
      "Epoch:2 \n",
      "Iteration:20 \n",
      "Loss:0.04271145164966583\n",
      "Epoch:2 \n",
      "Iteration:21 \n",
      "Loss:0.10021254420280457\n",
      "Epoch:2 \n",
      "Iteration:22 \n",
      "Loss:0.2145097851753235\n",
      "Epoch:2 \n",
      "Iteration:23 \n",
      "Loss:0.1163840964436531\n",
      "Epoch:2 \n",
      "Iteration:24 \n",
      "Loss:0.15726019442081451\n",
      "Epoch:2 \n",
      "Iteration:25 \n",
      "Loss:0.02367512881755829\n",
      "Epoch:2 \n",
      "Iteration:26 \n",
      "Loss:0.1830364167690277\n",
      "Epoch:2 \n",
      "Iteration:27 \n",
      "Loss:0.1608065664768219\n",
      "Epoch:2 \n",
      "Iteration:28 \n",
      "Loss:0.13235792517662048\n",
      "Epoch:2 \n",
      "Iteration:29 \n",
      "Loss:0.08429094403982162\n",
      "Epoch:2 \n",
      "Iteration:30 \n",
      "Loss:0.082579106092453\n",
      "Epoch:2 \n",
      "Iteration:31 \n",
      "Loss:0.08736960589885712\n",
      "Epoch:2 \n",
      "Iteration:32 \n",
      "Loss:0.09068331867456436\n",
      "Epoch:2 \n",
      "Iteration:33 \n",
      "Loss:0.055903684347867966\n",
      "Epoch:2 \n",
      "Iteration:34 \n",
      "Loss:0.19918718934059143\n",
      "Epoch:2 \n",
      "Iteration:35 \n",
      "Loss:0.13910189270973206\n",
      "Epoch:2 \n",
      "Iteration:36 \n",
      "Loss:0.17552827298641205\n",
      "Epoch:2 \n",
      "Iteration:37 \n",
      "Loss:0.1186172366142273\n",
      "Epoch:2 \n",
      "Iteration:38 \n",
      "Loss:0.08279170840978622\n",
      "Epoch:2 \n",
      "Iteration:39 \n",
      "Loss:0.10063126683235168\n",
      "Epoch:2 \n",
      "Iteration:40 \n",
      "Loss:0.03503945469856262\n",
      "Epoch:2 \n",
      "Iteration:41 \n",
      "Loss:0.07224889099597931\n",
      "Epoch:2 \n",
      "Iteration:42 \n",
      "Loss:0.05653027445077896\n",
      "Epoch:2 \n",
      "Iteration:43 \n",
      "Loss:0.16159400343894958\n",
      "Epoch:2 \n",
      "Iteration:44 \n",
      "Loss:0.1234220489859581\n",
      "Epoch:2 \n",
      "Iteration:45 \n",
      "Loss:0.0317281112074852\n",
      "Epoch:2 \n",
      "Iteration:46 \n",
      "Loss:0.06791935116052628\n",
      "Epoch:2 \n",
      "Iteration:47 \n",
      "Loss:0.047314103692770004\n",
      "Epoch:2 \n",
      "Iteration:48 \n",
      "Loss:0.1521657258272171\n",
      "Epoch:2 \n",
      "Iteration:49 \n",
      "Loss:0.08422497659921646\n",
      "Epoch:2 \n",
      "Iteration:50 \n",
      "Loss:0.05944348871707916\n",
      "Epoch:2 \n",
      "Iteration:51 \n",
      "Loss:0.05755031108856201\n",
      "Epoch:2 \n",
      "Iteration:52 \n",
      "Loss:0.09010080248117447\n",
      "Epoch:2 \n",
      "Iteration:53 \n",
      "Loss:0.10535210371017456\n",
      "Epoch:2 \n",
      "Iteration:54 \n",
      "Loss:0.13041329383850098\n",
      "Epoch:2 \n",
      "Iteration:55 \n",
      "Loss:0.09273028373718262\n",
      "Epoch:2 \n",
      "Iteration:56 \n",
      "Loss:0.15835565328598022\n",
      "Epoch:2 \n",
      "Iteration:57 \n",
      "Loss:0.15424878895282745\n",
      "Epoch:2 \n",
      "Iteration:58 \n",
      "Loss:0.19810603559017181\n",
      "Epoch:2 \n",
      "Iteration:59 \n",
      "Loss:0.19962118566036224\n",
      "Epoch:2 \n",
      "Iteration:60 \n",
      "Loss:0.03862475976347923\n",
      "Epoch:2 \n",
      "Iteration:61 \n",
      "Loss:0.0819513201713562\n",
      "Epoch:2 \n",
      "Iteration:62 \n",
      "Loss:0.11862997710704803\n",
      "Epoch:2 \n",
      "Iteration:63 \n",
      "Loss:0.17596612870693207\n",
      "Epoch:2 \n",
      "Iteration:64 \n",
      "Loss:0.16683471202850342\n",
      "Epoch:2 \n",
      "Iteration:65 \n",
      "Loss:0.03789473697543144\n",
      "Epoch:2 \n",
      "Iteration:66 \n",
      "Loss:0.028709804639220238\n",
      "Epoch:2 \n",
      "Iteration:67 \n",
      "Loss:0.15830965340137482\n",
      "Epoch:2 \n",
      "Iteration:68 \n",
      "Loss:0.04561738669872284\n",
      "Epoch:2 \n",
      "Iteration:69 \n",
      "Loss:0.08934885263442993\n",
      "Epoch:2 \n",
      "Iteration:70 \n",
      "Loss:0.054945334792137146\n",
      "Epoch:2 \n",
      "Iteration:71 \n",
      "Loss:0.10217737406492233\n",
      "Epoch:2 \n",
      "Iteration:72 \n",
      "Loss:0.024448256939649582\n",
      "Epoch:2 \n",
      "Iteration:73 \n",
      "Loss:0.10416826605796814\n",
      "Epoch:2 \n",
      "Iteration:74 \n",
      "Loss:0.07856085151433945\n",
      "Epoch:2 \n",
      "Iteration:75 \n",
      "Loss:0.08827532082796097\n",
      "Epoch:2 \n",
      "Iteration:76 \n",
      "Loss:0.0962040051817894\n",
      "Epoch:2 \n",
      "Iteration:77 \n",
      "Loss:0.08688598871231079\n",
      "Epoch:2 \n",
      "Iteration:78 \n",
      "Loss:0.010532799176871777\n",
      "Epoch:2 \n",
      "Iteration:79 \n",
      "Loss:0.06756215542554855\n",
      "Epoch:2 \n",
      "Iteration:80 \n",
      "Loss:0.0935513898730278\n",
      "Epoch:2 \n",
      "Iteration:81 \n",
      "Loss:0.025664575397968292\n",
      "Epoch:2 \n",
      "Iteration:82 \n",
      "Loss:0.22062167525291443\n",
      "Epoch:2 \n",
      "Iteration:83 \n",
      "Loss:0.06896625459194183\n",
      "Epoch:2 \n",
      "Iteration:84 \n",
      "Loss:0.12421489506959915\n",
      "Epoch:2 \n",
      "Iteration:85 \n",
      "Loss:0.08726920187473297\n",
      "Epoch:2 \n",
      "Iteration:86 \n",
      "Loss:0.06861772388219833\n",
      "Epoch:2 \n",
      "Iteration:87 \n",
      "Loss:0.04847275838255882\n",
      "Epoch:2 \n",
      "Iteration:88 \n",
      "Loss:0.13171255588531494\n",
      "Epoch:2 \n",
      "Iteration:89 \n",
      "Loss:0.020263519138097763\n",
      "Epoch:2 \n",
      "Iteration:90 \n",
      "Loss:0.08402882516384125\n",
      "Epoch:2 \n",
      "Iteration:91 \n",
      "Loss:0.051180947571992874\n",
      "Epoch:2 \n",
      "Iteration:92 \n",
      "Loss:0.049127716571092606\n",
      "Epoch:2 \n",
      "Iteration:93 \n",
      "Loss:0.08888084441423416\n",
      "Epoch:2 \n",
      "Iteration:94 \n",
      "Loss:0.17704403400421143\n",
      "Epoch:2 \n",
      "Iteration:95 \n",
      "Loss:0.023156234994530678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2 \n",
      "Iteration:96 \n",
      "Loss:0.050671275705099106\n",
      "Epoch:2 \n",
      "Iteration:97 \n",
      "Loss:0.11057719588279724\n",
      "Epoch:2 \n",
      "Iteration:98 \n",
      "Loss:0.06792283803224564\n",
      "Epoch:2 \n",
      "Iteration:99 \n",
      "Loss:0.1934918463230133\n",
      "Epoch:2 \n",
      "Iteration:100 \n",
      "Loss:0.09845835715532303\n",
      "Epoch:2 \n",
      "Iteration:101 \n",
      "Loss:0.1792984902858734\n",
      "Epoch:2 \n",
      "Iteration:102 \n",
      "Loss:0.13294540345668793\n",
      "Epoch:2 \n",
      "Iteration:103 \n",
      "Loss:0.10784327238798141\n",
      "Epoch:2 \n",
      "Iteration:104 \n",
      "Loss:0.07323049753904343\n",
      "Epoch:2 \n",
      "Iteration:105 \n",
      "Loss:0.04443055018782616\n",
      "Epoch:2 \n",
      "Iteration:106 \n",
      "Loss:0.07564422488212585\n",
      "Epoch:2 \n",
      "Iteration:107 \n",
      "Loss:0.16559630632400513\n",
      "Epoch:2 \n",
      "Iteration:108 \n",
      "Loss:0.11632976680994034\n",
      "Epoch:2 \n",
      "Iteration:109 \n",
      "Loss:0.0820557102560997\n",
      "Epoch:2 \n",
      "Iteration:110 \n",
      "Loss:0.058863043785095215\n",
      "Epoch:2 \n",
      "Iteration:111 \n",
      "Loss:0.0629238709807396\n",
      "Epoch:2 \n",
      "Iteration:112 \n",
      "Loss:0.036725856363773346\n",
      "Epoch:2 \n",
      "Iteration:113 \n",
      "Loss:0.08275307714939117\n",
      "Epoch:2 \n",
      "Iteration:114 \n",
      "Loss:0.12446576356887817\n",
      "Epoch:2 \n",
      "Iteration:115 \n",
      "Loss:0.0786079689860344\n",
      "Epoch:2 \n",
      "Iteration:116 \n",
      "Loss:0.09837708622217178\n",
      "Epoch:2 \n",
      "Iteration:117 \n",
      "Loss:0.11518042534589767\n",
      "Epoch:2 \n",
      "Iteration:118 \n",
      "Loss:0.03974189609289169\n",
      "Epoch:2 \n",
      "Iteration:119 \n",
      "Loss:0.07056482881307602\n",
      "Epoch:2 \n",
      "Iteration:120 \n",
      "Loss:0.10706470161676407\n",
      "Epoch:2 \n",
      "Iteration:121 \n",
      "Loss:0.10666216909885406\n",
      "Epoch:2 \n",
      "Iteration:122 \n",
      "Loss:0.049494221806526184\n",
      "Epoch:2 \n",
      "Iteration:123 \n",
      "Loss:0.11394444108009338\n",
      "Epoch:2 \n",
      "Iteration:124 \n",
      "Loss:0.07226788997650146\n",
      "Epoch:2 \n",
      "Iteration:125 \n",
      "Loss:0.09296377003192902\n",
      "Epoch:2 \n",
      "Iteration:126 \n",
      "Loss:0.10244791209697723\n",
      "Epoch:2 \n",
      "Iteration:127 \n",
      "Loss:0.06408727914094925\n",
      "Epoch:2 \n",
      "Iteration:128 \n",
      "Loss:0.07959353178739548\n",
      "Epoch:2 \n",
      "Iteration:129 \n",
      "Loss:0.11107835918664932\n",
      "Epoch:2 \n",
      "Iteration:130 \n",
      "Loss:0.023910660296678543\n",
      "Epoch:2 \n",
      "Iteration:131 \n",
      "Loss:0.1283545196056366\n",
      "Epoch:2 \n",
      "Iteration:132 \n",
      "Loss:0.10096649825572968\n",
      "Epoch:2 \n",
      "Iteration:133 \n",
      "Loss:0.0758940801024437\n",
      "Epoch:2 \n",
      "Iteration:134 \n",
      "Loss:0.07442004233598709\n",
      "Epoch:2 \n",
      "Iteration:135 \n",
      "Loss:0.10350458323955536\n",
      "Epoch:2 \n",
      "Iteration:136 \n",
      "Loss:0.08413533121347427\n",
      "Epoch:2 \n",
      "Iteration:137 \n",
      "Loss:0.09812232851982117\n",
      "Epoch:2 \n",
      "Iteration:138 \n",
      "Loss:0.056328702718019485\n",
      "Epoch:2 \n",
      "Iteration:139 \n",
      "Loss:0.20106303691864014\n",
      "Epoch:2 \n",
      "Iteration:140 \n",
      "Loss:0.0989891067147255\n",
      "Epoch:2 \n",
      "Iteration:141 \n",
      "Loss:0.1570352017879486\n",
      "Epoch:2 \n",
      "Iteration:142 \n",
      "Loss:0.040356818586587906\n",
      "Epoch:2 \n",
      "Iteration:143 \n",
      "Loss:0.03203118219971657\n",
      "Epoch:2 \n",
      "Iteration:144 \n",
      "Loss:0.15831315517425537\n",
      "Epoch:2 \n",
      "Iteration:145 \n",
      "Loss:0.027550697326660156\n",
      "Epoch:2 \n",
      "Iteration:146 \n",
      "Loss:0.08113782107830048\n",
      "Epoch:2 \n",
      "Iteration:147 \n",
      "Loss:0.13548453152179718\n",
      "Epoch:2 \n",
      "Iteration:148 \n",
      "Loss:0.051072634756565094\n",
      "Epoch:2 \n",
      "Iteration:149 \n",
      "Loss:0.07170893996953964\n",
      "Epoch:2 \n",
      "Iteration:150 \n",
      "Loss:0.08512383699417114\n",
      "Epoch:2 \n",
      "Iteration:151 \n",
      "Loss:0.08447585254907608\n",
      "Epoch:2 \n",
      "Iteration:152 \n",
      "Loss:0.10489287972450256\n",
      "Epoch:2 \n",
      "Iteration:153 \n",
      "Loss:0.12810854613780975\n",
      "Epoch:2 \n",
      "Iteration:154 \n",
      "Loss:0.06750764697790146\n",
      "Epoch:2 \n",
      "Iteration:155 \n",
      "Loss:0.1143108382821083\n",
      "Epoch:2 \n",
      "Iteration:156 \n",
      "Loss:0.07451017200946808\n",
      "Epoch:2 \n",
      "Iteration:157 \n",
      "Loss:0.04295981302857399\n",
      "Epoch:2 \n",
      "Iteration:158 \n",
      "Loss:0.04076758399605751\n",
      "Epoch:2 \n",
      "Iteration:159 \n",
      "Loss:0.05125143378973007\n",
      "Epoch:2 \n",
      "Iteration:160 \n",
      "Loss:0.08065800368785858\n",
      "Epoch:2 \n",
      "Iteration:161 \n",
      "Loss:0.032230257987976074\n",
      "Epoch:2 \n",
      "Iteration:162 \n",
      "Loss:0.04377559572458267\n",
      "Epoch:2 \n",
      "Iteration:163 \n",
      "Loss:0.020634200423955917\n",
      "Epoch:2 \n",
      "Iteration:164 \n",
      "Loss:0.1335270255804062\n",
      "Epoch:2 \n",
      "Iteration:165 \n",
      "Loss:0.034214556217193604\n",
      "Epoch:2 \n",
      "Iteration:166 \n",
      "Loss:0.061434391885995865\n",
      "Epoch:2 \n",
      "Iteration:167 \n",
      "Loss:0.03717609867453575\n",
      "Epoch:2 \n",
      "Iteration:168 \n",
      "Loss:0.23791009187698364\n",
      "Epoch:2 \n",
      "Iteration:169 \n",
      "Loss:0.054906487464904785\n",
      "Epoch:2 \n",
      "Iteration:170 \n",
      "Loss:0.08690745383501053\n",
      "Epoch:2 \n",
      "Iteration:171 \n",
      "Loss:0.20042583346366882\n",
      "Epoch:2 \n",
      "Iteration:172 \n",
      "Loss:0.05465885251760483\n",
      "Epoch:2 \n",
      "Iteration:173 \n",
      "Loss:0.017153052613139153\n",
      "Epoch:2 \n",
      "Iteration:174 \n",
      "Loss:0.12338985502719879\n",
      "Epoch:2 \n",
      "Iteration:175 \n",
      "Loss:0.10872585326433182\n",
      "Epoch:2 \n",
      "Iteration:176 \n",
      "Loss:0.12953558564186096\n",
      "Epoch:2 \n",
      "Iteration:177 \n",
      "Loss:0.1391502320766449\n",
      "Epoch:2 \n",
      "Iteration:178 \n",
      "Loss:0.09906962513923645\n",
      "Epoch:2 \n",
      "Iteration:179 \n",
      "Loss:0.0098823681473732\n",
      "Epoch:2 \n",
      "Iteration:180 \n",
      "Loss:0.07000062614679337\n",
      "Epoch:2 \n",
      "Iteration:181 \n",
      "Loss:0.08142376691102982\n",
      "Epoch:2 \n",
      "Iteration:182 \n",
      "Loss:0.1667347401380539\n",
      "Epoch:2 \n",
      "Iteration:183 \n",
      "Loss:0.030405569821596146\n",
      "Epoch:2 \n",
      "Iteration:184 \n",
      "Loss:0.0505489744246006\n",
      "Epoch:2 \n",
      "Iteration:185 \n",
      "Loss:0.08576603978872299\n",
      "Epoch:2 \n",
      "Iteration:186 \n",
      "Loss:0.060864511877298355\n",
      "Epoch:2 \n",
      "Iteration:187 \n",
      "Loss:0.09758847951889038\n",
      "Epoch:2 \n",
      "Iteration:188 \n",
      "Loss:0.03645620495080948\n",
      "Epoch:2 \n",
      "Iteration:189 \n",
      "Loss:0.17544923722743988\n",
      "Epoch:2 \n",
      "Iteration:190 \n",
      "Loss:0.03714743256568909\n",
      "Epoch:2 \n",
      "Iteration:191 \n",
      "Loss:0.08691917359828949\n",
      "Epoch:2 \n",
      "Iteration:192 \n",
      "Loss:0.06506047397851944\n",
      "Epoch:2 \n",
      "Iteration:193 \n",
      "Loss:0.21158522367477417\n",
      "Epoch:2 \n",
      "Iteration:194 \n",
      "Loss:0.019708724692463875\n",
      "Epoch:2 \n",
      "Iteration:195 \n",
      "Loss:0.13108254969120026\n",
      "Epoch:2 \n",
      "Iteration:196 \n",
      "Loss:0.08538669347763062\n",
      "Epoch:2 \n",
      "Iteration:197 \n",
      "Loss:0.09504768252372742\n",
      "Epoch:2 \n",
      "Iteration:198 \n",
      "Loss:0.042060066014528275\n",
      "Epoch:2 \n",
      "Iteration:199 \n",
      "Loss:0.03852638974785805\n",
      "Epoch:2 \n",
      "Iteration:200 \n",
      "Loss:0.12602072954177856\n",
      "Epoch:2 \n",
      "Iteration:201 \n",
      "Loss:0.08068415522575378\n",
      "Epoch:2 \n",
      "Iteration:202 \n",
      "Loss:0.06731382012367249\n",
      "Epoch:2 \n",
      "Iteration:203 \n",
      "Loss:0.04185939207673073\n",
      "Epoch:2 \n",
      "Iteration:204 \n",
      "Loss:0.06309980899095535\n",
      "Epoch:2 \n",
      "Iteration:205 \n",
      "Loss:0.25756776332855225\n",
      "Epoch:2 \n",
      "Iteration:206 \n",
      "Loss:0.121894970536232\n",
      "Epoch:2 \n",
      "Iteration:207 \n",
      "Loss:0.16582271456718445\n",
      "Epoch:2 \n",
      "Iteration:208 \n",
      "Loss:0.05141178518533707\n",
      "Epoch:2 \n",
      "Iteration:209 \n",
      "Loss:0.1772456020116806\n",
      "Epoch:2 \n",
      "Iteration:210 \n",
      "Loss:0.2845035195350647\n",
      "Epoch:2 \n",
      "Iteration:211 \n",
      "Loss:0.07357021421194077\n",
      "Epoch:2 \n",
      "Iteration:212 \n",
      "Loss:0.06464225053787231\n",
      "Epoch:2 \n",
      "Iteration:213 \n",
      "Loss:0.04828151687979698\n",
      "Epoch:2 \n",
      "Iteration:214 \n",
      "Loss:0.053133681416511536\n",
      "Epoch:2 \n",
      "Iteration:215 \n",
      "Loss:0.13257725536823273\n",
      "Epoch:2 \n",
      "Iteration:216 \n",
      "Loss:0.06481591612100601\n",
      "Epoch:2 \n",
      "Iteration:217 \n",
      "Loss:0.1376100778579712\n",
      "Epoch:2 \n",
      "Iteration:218 \n",
      "Loss:0.14807011187076569\n",
      "Epoch:2 \n",
      "Iteration:219 \n",
      "Loss:0.0719819962978363\n",
      "Epoch:2 \n",
      "Iteration:220 \n",
      "Loss:0.039959631860256195\n",
      "Epoch:2 \n",
      "Iteration:221 \n",
      "Loss:0.15952752530574799\n",
      "Epoch:2 \n",
      "Iteration:222 \n",
      "Loss:0.1926977038383484\n",
      "Epoch:2 \n",
      "Iteration:223 \n",
      "Loss:0.06430050730705261\n",
      "Epoch:2 \n",
      "Iteration:224 \n",
      "Loss:0.024328796193003654\n",
      "Epoch:2 \n",
      "Iteration:225 \n",
      "Loss:0.057211656123399734\n",
      "Epoch:2 \n",
      "Iteration:226 \n",
      "Loss:0.10065016150474548\n",
      "Epoch:2 \n",
      "Iteration:227 \n",
      "Loss:0.06397677212953568\n",
      "Epoch:2 \n",
      "Iteration:228 \n",
      "Loss:0.018466521054506302\n",
      "Epoch:2 \n",
      "Iteration:229 \n",
      "Loss:0.07530060410499573\n",
      "Epoch:2 \n",
      "Iteration:230 \n",
      "Loss:0.10649986565113068\n",
      "Epoch:2 \n",
      "Iteration:231 \n",
      "Loss:0.048574239015579224\n",
      "Epoch:2 \n",
      "Iteration:232 \n",
      "Loss:0.0857330858707428\n",
      "Epoch:2 \n",
      "Iteration:233 \n",
      "Loss:0.12259946018457413\n",
      "Epoch:2 \n",
      "Iteration:234 \n",
      "Loss:0.033085696399211884\n",
      "Epoch:2 \n",
      "Iteration:235 \n",
      "Loss:0.1099027469754219\n",
      "Epoch:2 \n",
      "Iteration:236 \n",
      "Loss:0.10122503340244293\n",
      "Epoch:2 \n",
      "Iteration:237 \n",
      "Loss:0.0727650597691536\n",
      "Epoch:2 \n",
      "Iteration:238 \n",
      "Loss:0.2126000076532364\n",
      "Epoch:2 \n",
      "Iteration:239 \n",
      "Loss:0.08605486899614334\n",
      "Epoch:2 \n",
      "Iteration:240 \n",
      "Loss:0.08131510019302368\n",
      "Epoch:2 \n",
      "Iteration:241 \n",
      "Loss:0.08340586721897125\n",
      "Epoch:2 \n",
      "Iteration:242 \n",
      "Loss:0.048440221697092056\n",
      "Epoch:2 \n",
      "Iteration:243 \n",
      "Loss:0.10631762444972992\n",
      "Epoch:2 \n",
      "Iteration:244 \n",
      "Loss:0.05353979021310806\n",
      "Epoch:2 \n",
      "Iteration:245 \n",
      "Loss:0.09776134788990021\n",
      "Epoch:2 \n",
      "Iteration:246 \n",
      "Loss:0.06274985522031784\n",
      "Epoch:2 \n",
      "Iteration:247 \n",
      "Loss:0.15248923003673553\n",
      "Epoch:2 \n",
      "Iteration:248 \n",
      "Loss:0.036425407975912094\n",
      "Epoch:2 \n",
      "Iteration:249 \n",
      "Loss:0.14450109004974365\n",
      "Epoch:2 \n",
      "Iteration:250 \n",
      "Loss:0.03845297545194626\n",
      "Epoch:2 \n",
      "Iteration:251 \n",
      "Loss:0.16880793869495392\n",
      "Epoch:2 \n",
      "Iteration:252 \n",
      "Loss:0.09434308856725693\n",
      "Epoch:2 \n",
      "Iteration:253 \n",
      "Loss:0.17590011656284332\n",
      "Epoch:2 \n",
      "Iteration:254 \n",
      "Loss:0.0871458426117897\n",
      "Epoch:2 \n",
      "Iteration:255 \n",
      "Loss:0.0561591237783432\n",
      "Epoch:2 \n",
      "Iteration:256 \n",
      "Loss:0.018637211993336678\n",
      "Epoch:2 \n",
      "Iteration:257 \n",
      "Loss:0.06569203734397888\n",
      "Epoch:2 \n",
      "Iteration:258 \n",
      "Loss:0.10015752166509628\n",
      "Epoch:2 \n",
      "Iteration:259 \n",
      "Loss:0.04225564002990723\n",
      "Epoch:2 \n",
      "Iteration:260 \n",
      "Loss:0.10267823189496994\n",
      "Epoch:2 \n",
      "Iteration:261 \n",
      "Loss:0.08296890556812286\n",
      "Epoch:2 \n",
      "Iteration:262 \n",
      "Loss:0.03244716301560402\n",
      "Epoch:2 \n",
      "Iteration:263 \n",
      "Loss:0.18013425171375275\n",
      "Epoch:2 \n",
      "Iteration:264 \n",
      "Loss:0.04046235233545303\n",
      "Epoch:2 \n",
      "Iteration:265 \n",
      "Loss:0.024055728688836098\n",
      "Epoch:2 \n",
      "Iteration:266 \n",
      "Loss:0.11884070187807083\n",
      "Epoch:2 \n",
      "Iteration:267 \n",
      "Loss:0.02998245321214199\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2 \n",
      "Iteration:268 \n",
      "Loss:0.052086878567934036\n",
      "Epoch:2 \n",
      "Iteration:269 \n",
      "Loss:0.09080106765031815\n",
      "Epoch:2 \n",
      "Iteration:270 \n",
      "Loss:0.0629081279039383\n",
      "Epoch:2 \n",
      "Iteration:271 \n",
      "Loss:0.0771857425570488\n",
      "Epoch:2 \n",
      "Iteration:272 \n",
      "Loss:0.018934419378638268\n",
      "Epoch:2 \n",
      "Iteration:273 \n",
      "Loss:0.11793094873428345\n",
      "Epoch:2 \n",
      "Iteration:274 \n",
      "Loss:0.04348869249224663\n",
      "Epoch:2 \n",
      "Iteration:275 \n",
      "Loss:0.09788016974925995\n",
      "Epoch:2 \n",
      "Iteration:276 \n",
      "Loss:0.01922612264752388\n",
      "Epoch:2 \n",
      "Iteration:277 \n",
      "Loss:0.13758008182048798\n",
      "Epoch:2 \n",
      "Iteration:278 \n",
      "Loss:0.022951893508434296\n",
      "Epoch:2 \n",
      "Iteration:279 \n",
      "Loss:0.04002572223544121\n",
      "Epoch:2 \n",
      "Iteration:280 \n",
      "Loss:0.058139171451330185\n",
      "Epoch:2 \n",
      "Iteration:281 \n",
      "Loss:0.05044596642255783\n",
      "Epoch:2 \n",
      "Iteration:282 \n",
      "Loss:0.1323990821838379\n",
      "Epoch:2 \n",
      "Iteration:283 \n",
      "Loss:0.12433477491140366\n",
      "Epoch:2 \n",
      "Iteration:284 \n",
      "Loss:0.07417905330657959\n",
      "Epoch:2 \n",
      "Iteration:285 \n",
      "Loss:0.0989016517996788\n",
      "Epoch:2 \n",
      "Iteration:286 \n",
      "Loss:0.029146023094654083\n",
      "Epoch:2 \n",
      "Iteration:287 \n",
      "Loss:0.07612933963537216\n",
      "Epoch:2 \n",
      "Iteration:288 \n",
      "Loss:0.060041218996047974\n",
      "Epoch:2 \n",
      "Iteration:289 \n",
      "Loss:0.13334660232067108\n",
      "Epoch:2 \n",
      "Iteration:290 \n",
      "Loss:0.019690461456775665\n",
      "Epoch:2 \n",
      "Iteration:291 \n",
      "Loss:0.017331814393401146\n",
      "Epoch:2 \n",
      "Iteration:292 \n",
      "Loss:0.07126867771148682\n",
      "Epoch:2 \n",
      "Iteration:293 \n",
      "Loss:0.025734635069966316\n",
      "Epoch:2 \n",
      "Iteration:294 \n",
      "Loss:0.04423552379012108\n",
      "Epoch:2 \n",
      "Iteration:295 \n",
      "Loss:0.03229411318898201\n",
      "Epoch:2 \n",
      "Iteration:296 \n",
      "Loss:0.05551575496792793\n",
      "Epoch:2 \n",
      "Iteration:297 \n",
      "Loss:0.02387768030166626\n",
      "Epoch:2 \n",
      "Iteration:298 \n",
      "Loss:0.12588995695114136\n",
      "Epoch:2 \n",
      "Iteration:299 \n",
      "Loss:0.10413670539855957\n",
      "Epoch:2 \n",
      "Iteration:300 \n",
      "Loss:0.14976704120635986\n",
      "Epoch:2 \n",
      "Iteration:301 \n",
      "Loss:0.14102274179458618\n",
      "Epoch:2 \n",
      "Iteration:302 \n",
      "Loss:0.10587026923894882\n",
      "Epoch:2 \n",
      "Iteration:303 \n",
      "Loss:0.02973896451294422\n",
      "Epoch:2 \n",
      "Iteration:304 \n",
      "Loss:0.02672480046749115\n",
      "Epoch:2 \n",
      "Iteration:305 \n",
      "Loss:0.08879692852497101\n",
      "Epoch:2 \n",
      "Iteration:306 \n",
      "Loss:0.07866974174976349\n",
      "Epoch:2 \n",
      "Iteration:307 \n",
      "Loss:0.2881695628166199\n",
      "Epoch:2 \n",
      "Iteration:308 \n",
      "Loss:0.2988487482070923\n",
      "Epoch:2 \n",
      "Iteration:309 \n",
      "Loss:0.10340525954961777\n",
      "Epoch:2 \n",
      "Iteration:310 \n",
      "Loss:0.1028824970126152\n",
      "Epoch:2 \n",
      "Iteration:311 \n",
      "Loss:0.15129925310611725\n",
      "Epoch:2 \n",
      "Iteration:312 \n",
      "Loss:0.08706580847501755\n",
      "Epoch:2 \n",
      "Iteration:313 \n",
      "Loss:0.16873355209827423\n",
      "Epoch:2 \n",
      "Iteration:314 \n",
      "Loss:0.061414770781993866\n",
      "Epoch:2 \n",
      "Iteration:315 \n",
      "Loss:0.09770325571298599\n",
      "Epoch:2 \n",
      "Iteration:316 \n",
      "Loss:0.13110196590423584\n",
      "Epoch:2 \n",
      "Iteration:317 \n",
      "Loss:0.1932714283466339\n",
      "Epoch:2 \n",
      "Iteration:318 \n",
      "Loss:0.0851825550198555\n",
      "Epoch:2 \n",
      "Iteration:319 \n",
      "Loss:0.08326251804828644\n",
      "Epoch:2 \n",
      "Iteration:320 \n",
      "Loss:0.07439133524894714\n",
      "Epoch:2 \n",
      "Iteration:321 \n",
      "Loss:0.08978495746850967\n",
      "Epoch:2 \n",
      "Iteration:322 \n",
      "Loss:0.19639675319194794\n",
      "Epoch:2 \n",
      "Iteration:323 \n",
      "Loss:0.04275340959429741\n",
      "Epoch:2 \n",
      "Iteration:324 \n",
      "Loss:0.09018409997224808\n",
      "Epoch:2 \n",
      "Iteration:325 \n",
      "Loss:0.05520032346248627\n",
      "Epoch:2 \n",
      "Iteration:326 \n",
      "Loss:0.09071093797683716\n",
      "Epoch:2 \n",
      "Iteration:327 \n",
      "Loss:0.047196563333272934\n",
      "Epoch:2 \n",
      "Iteration:328 \n",
      "Loss:0.06127800792455673\n",
      "Epoch:2 \n",
      "Iteration:329 \n",
      "Loss:0.07389719784259796\n",
      "Epoch:2 \n",
      "Iteration:330 \n",
      "Loss:0.04119421914219856\n",
      "Epoch:2 \n",
      "Iteration:331 \n",
      "Loss:0.08584859222173691\n",
      "Epoch:2 \n",
      "Iteration:332 \n",
      "Loss:0.05179165303707123\n",
      "Epoch:2 \n",
      "Iteration:333 \n",
      "Loss:0.17456145584583282\n",
      "Epoch:2 \n",
      "Iteration:334 \n",
      "Loss:0.07832734286785126\n",
      "Epoch:2 \n",
      "Iteration:335 \n",
      "Loss:0.05842089653015137\n",
      "Epoch:2 \n",
      "Iteration:336 \n",
      "Loss:0.06287868320941925\n",
      "Epoch:2 \n",
      "Iteration:337 \n",
      "Loss:0.06237107887864113\n",
      "Epoch:2 \n",
      "Iteration:338 \n",
      "Loss:0.02793659269809723\n",
      "Epoch:2 \n",
      "Iteration:339 \n",
      "Loss:0.16835179924964905\n",
      "Epoch:2 \n",
      "Iteration:340 \n",
      "Loss:0.13800548017024994\n",
      "Epoch:2 \n",
      "Iteration:341 \n",
      "Loss:0.18857212364673615\n",
      "Epoch:2 \n",
      "Iteration:342 \n",
      "Loss:0.06029169633984566\n",
      "Epoch:2 \n",
      "Iteration:343 \n",
      "Loss:0.04826518893241882\n",
      "Epoch:2 \n",
      "Iteration:344 \n",
      "Loss:0.14393354952335358\n",
      "Epoch:2 \n",
      "Iteration:345 \n",
      "Loss:0.06934497505426407\n",
      "Epoch:2 \n",
      "Iteration:346 \n",
      "Loss:0.11738409847021103\n",
      "Epoch:2 \n",
      "Iteration:347 \n",
      "Loss:0.06733574718236923\n",
      "Epoch:2 \n",
      "Iteration:348 \n",
      "Loss:0.016522157937288284\n",
      "Epoch:2 \n",
      "Iteration:349 \n",
      "Loss:0.08302673697471619\n",
      "Epoch:2 \n",
      "Iteration:350 \n",
      "Loss:0.04427957162261009\n",
      "Epoch:2 \n",
      "Iteration:351 \n",
      "Loss:0.06360725313425064\n",
      "Epoch:2 \n",
      "Iteration:352 \n",
      "Loss:0.13549792766571045\n",
      "Epoch:2 \n",
      "Iteration:353 \n",
      "Loss:0.028411608189344406\n",
      "Epoch:2 \n",
      "Iteration:354 \n",
      "Loss:0.12854833900928497\n",
      "Epoch:2 \n",
      "Iteration:355 \n",
      "Loss:0.14960280060768127\n",
      "Epoch:2 \n",
      "Iteration:356 \n",
      "Loss:0.03722245618700981\n",
      "Epoch:2 \n",
      "Iteration:357 \n",
      "Loss:0.09251126646995544\n",
      "Epoch:2 \n",
      "Iteration:358 \n",
      "Loss:0.06516646593809128\n",
      "Epoch:2 \n",
      "Iteration:359 \n",
      "Loss:0.06562315672636032\n",
      "Epoch:2 \n",
      "Iteration:360 \n",
      "Loss:0.042375531047582626\n",
      "Epoch:2 \n",
      "Iteration:361 \n",
      "Loss:0.06699983775615692\n",
      "Epoch:2 \n",
      "Iteration:362 \n",
      "Loss:0.13150759041309357\n",
      "Epoch:2 \n",
      "Iteration:363 \n",
      "Loss:0.04030489921569824\n",
      "Epoch:2 \n",
      "Iteration:364 \n",
      "Loss:0.08023533970117569\n",
      "Epoch:2 \n",
      "Iteration:365 \n",
      "Loss:0.06807806342840195\n",
      "Epoch:2 \n",
      "Iteration:366 \n",
      "Loss:0.07604335993528366\n",
      "Epoch:2 \n",
      "Iteration:367 \n",
      "Loss:0.06862395256757736\n",
      "Epoch:2 \n",
      "Iteration:368 \n",
      "Loss:0.14980755746364594\n",
      "Epoch:2 \n",
      "Iteration:369 \n",
      "Loss:0.034443341195583344\n",
      "Epoch:2 \n",
      "Iteration:370 \n",
      "Loss:0.04800425469875336\n",
      "Epoch:2 \n",
      "Iteration:371 \n",
      "Loss:0.051439229398965836\n",
      "Epoch:2 \n",
      "Iteration:372 \n",
      "Loss:0.09709055721759796\n",
      "Epoch:2 \n",
      "Iteration:373 \n",
      "Loss:0.08504204452037811\n",
      "Epoch:2 \n",
      "Iteration:374 \n",
      "Loss:0.1123960092663765\n",
      "Epoch:2 \n",
      "Iteration:375 \n",
      "Loss:0.1340862363576889\n",
      "Epoch:2 \n",
      "Iteration:376 \n",
      "Loss:0.07038675248622894\n",
      "Epoch:2 \n",
      "Iteration:377 \n",
      "Loss:0.18167418241500854\n",
      "Epoch:2 \n",
      "Iteration:378 \n",
      "Loss:0.06205783411860466\n",
      "Epoch:2 \n",
      "Iteration:379 \n",
      "Loss:0.0854194164276123\n",
      "Epoch:2 \n",
      "Iteration:380 \n",
      "Loss:0.02837475575506687\n",
      "Epoch:2 \n",
      "Iteration:381 \n",
      "Loss:0.06367925554513931\n",
      "Epoch:2 \n",
      "Iteration:382 \n",
      "Loss:0.03235067427158356\n",
      "Epoch:2 \n",
      "Iteration:383 \n",
      "Loss:0.06539309769868851\n",
      "Epoch:2 \n",
      "Iteration:384 \n",
      "Loss:0.00424795551225543\n",
      "Epoch:2 \n",
      "Iteration:385 \n",
      "Loss:0.03472180664539337\n",
      "Epoch:2 \n",
      "Iteration:386 \n",
      "Loss:0.09082392603158951\n",
      "Epoch:2 \n",
      "Iteration:387 \n",
      "Loss:0.03933430835604668\n",
      "Epoch:2 \n",
      "Iteration:388 \n",
      "Loss:0.17413005232810974\n",
      "Epoch:2 \n",
      "Iteration:389 \n",
      "Loss:0.03767053782939911\n",
      "Epoch:2 \n",
      "Iteration:390 \n",
      "Loss:0.12306584417819977\n",
      "Epoch:2 \n",
      "Iteration:391 \n",
      "Loss:0.03919290006160736\n",
      "Epoch:2 \n",
      "Iteration:392 \n",
      "Loss:0.015977347269654274\n",
      "Epoch:2 \n",
      "Iteration:393 \n",
      "Loss:0.11206331104040146\n",
      "Epoch:2 \n",
      "Iteration:394 \n",
      "Loss:0.09667129814624786\n",
      "Epoch:2 \n",
      "Iteration:395 \n",
      "Loss:0.07877352833747864\n",
      "Epoch:2 \n",
      "Iteration:396 \n",
      "Loss:0.10411285609006882\n",
      "Epoch:2 \n",
      "Iteration:397 \n",
      "Loss:0.06105979532003403\n",
      "Epoch:2 \n",
      "Iteration:398 \n",
      "Loss:0.12042148411273956\n",
      "Epoch:2 \n",
      "Iteration:399 \n",
      "Loss:0.01514794584363699\n",
      "Epoch:2 \n",
      "Iteration:400 \n",
      "Loss:0.0318511500954628\n",
      "Epoch:2 \n",
      "Iteration:401 \n",
      "Loss:0.2070971429347992\n",
      "Epoch:2 \n",
      "Iteration:402 \n",
      "Loss:0.1391669511795044\n",
      "Epoch:2 \n",
      "Iteration:403 \n",
      "Loss:0.03206668049097061\n",
      "Epoch:2 \n",
      "Iteration:404 \n",
      "Loss:0.21106724441051483\n",
      "Epoch:2 \n",
      "Iteration:405 \n",
      "Loss:0.10386157780885696\n",
      "Epoch:2 \n",
      "Iteration:406 \n",
      "Loss:0.01880810223519802\n",
      "Epoch:2 \n",
      "Iteration:407 \n",
      "Loss:0.08602999895811081\n",
      "Epoch:2 \n",
      "Iteration:408 \n",
      "Loss:0.19997011125087738\n",
      "Epoch:2 \n",
      "Iteration:409 \n",
      "Loss:0.055298689752817154\n",
      "Epoch:2 \n",
      "Iteration:410 \n",
      "Loss:0.0473397858440876\n",
      "Epoch:2 \n",
      "Iteration:411 \n",
      "Loss:0.20255659520626068\n",
      "Epoch:2 \n",
      "Iteration:412 \n",
      "Loss:0.045019883662462234\n",
      "Epoch:2 \n",
      "Iteration:413 \n",
      "Loss:0.1812419891357422\n",
      "Epoch:2 \n",
      "Iteration:414 \n",
      "Loss:0.08829294145107269\n",
      "Epoch:2 \n",
      "Iteration:415 \n",
      "Loss:0.10385873913764954\n",
      "Epoch:2 \n",
      "Iteration:416 \n",
      "Loss:0.041494231671094894\n",
      "Epoch:2 \n",
      "Iteration:417 \n",
      "Loss:0.030125033110380173\n",
      "Epoch:2 \n",
      "Iteration:418 \n",
      "Loss:0.07113087177276611\n",
      "Epoch:2 \n",
      "Iteration:419 \n",
      "Loss:0.1513689011335373\n",
      "Epoch:2 \n",
      "Iteration:420 \n",
      "Loss:0.08001048117876053\n",
      "Epoch:2 \n",
      "Iteration:421 \n",
      "Loss:0.08398983627557755\n",
      "Epoch:2 \n",
      "Iteration:422 \n",
      "Loss:0.06247904151678085\n",
      "Epoch:2 \n",
      "Iteration:423 \n",
      "Loss:0.08114616572856903\n",
      "Epoch:2 \n",
      "Iteration:424 \n",
      "Loss:0.04297105222940445\n",
      "Epoch:2 \n",
      "Iteration:425 \n",
      "Loss:0.09259044378995895\n",
      "Epoch:2 \n",
      "Iteration:426 \n",
      "Loss:0.1321702003479004\n",
      "Epoch:2 \n",
      "Iteration:427 \n",
      "Loss:0.17949098348617554\n",
      "Epoch:2 \n",
      "Iteration:428 \n",
      "Loss:0.06876982003450394\n",
      "Epoch:2 \n",
      "Iteration:429 \n",
      "Loss:0.09539525955915451\n",
      "Epoch:2 \n",
      "Iteration:430 \n",
      "Loss:0.10135524719953537\n",
      "Epoch:2 \n",
      "Iteration:431 \n",
      "Loss:0.035346776247024536\n",
      "Epoch:2 \n",
      "Iteration:432 \n",
      "Loss:0.12643378973007202\n",
      "Epoch:2 \n",
      "Iteration:433 \n",
      "Loss:0.02688165195286274\n",
      "Epoch:2 \n",
      "Iteration:434 \n",
      "Loss:0.052182093262672424\n",
      "Epoch:2 \n",
      "Iteration:435 \n",
      "Loss:0.23109851777553558\n",
      "Epoch:2 \n",
      "Iteration:436 \n",
      "Loss:0.1420464664697647\n",
      "Epoch:2 \n",
      "Iteration:437 \n",
      "Loss:0.09991009533405304\n",
      "Epoch:2 \n",
      "Iteration:438 \n",
      "Loss:0.03685600310564041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2 \n",
      "Iteration:439 \n",
      "Loss:0.10966259986162186\n",
      "Epoch:2 \n",
      "Iteration:440 \n",
      "Loss:0.14692892134189606\n",
      "Epoch:2 \n",
      "Iteration:441 \n",
      "Loss:0.07716568559408188\n",
      "Epoch:2 \n",
      "Iteration:442 \n",
      "Loss:0.10208268463611603\n",
      "Epoch:2 \n",
      "Iteration:443 \n",
      "Loss:0.1128358393907547\n",
      "Epoch:2 \n",
      "Iteration:444 \n",
      "Loss:0.062188971787691116\n",
      "Epoch:2 \n",
      "Iteration:445 \n",
      "Loss:0.08713003993034363\n",
      "Epoch:2 \n",
      "Iteration:446 \n",
      "Loss:0.09090324491262436\n",
      "Epoch:2 \n",
      "Iteration:447 \n",
      "Loss:0.11777570843696594\n",
      "Epoch:2 \n",
      "Iteration:448 \n",
      "Loss:0.061372045427560806\n",
      "Epoch:2 \n",
      "Iteration:449 \n",
      "Loss:0.07631354033946991\n",
      "Epoch:2 \n",
      "Iteration:450 \n",
      "Loss:0.05208823084831238\n",
      "Epoch:2 \n",
      "Iteration:451 \n",
      "Loss:0.08311394602060318\n",
      "Epoch:2 \n",
      "Iteration:452 \n",
      "Loss:0.0697915107011795\n",
      "Epoch:2 \n",
      "Iteration:453 \n",
      "Loss:0.04550196975469589\n",
      "Epoch:2 \n",
      "Iteration:454 \n",
      "Loss:0.01881393976509571\n",
      "Epoch:2 \n",
      "Iteration:455 \n",
      "Loss:0.09792184829711914\n",
      "Epoch:2 \n",
      "Iteration:456 \n",
      "Loss:0.0425495021045208\n",
      "Epoch:2 \n",
      "Iteration:457 \n",
      "Loss:0.048559416085481644\n",
      "Epoch:2 \n",
      "Iteration:458 \n",
      "Loss:0.20353494584560394\n",
      "Epoch:2 \n",
      "Iteration:459 \n",
      "Loss:0.0651637613773346\n",
      "Epoch:2 \n",
      "Iteration:460 \n",
      "Loss:0.068580262362957\n",
      "Epoch:2 \n",
      "Iteration:461 \n",
      "Loss:0.1157805547118187\n",
      "Epoch:2 \n",
      "Iteration:462 \n",
      "Loss:0.09785594791173935\n",
      "Epoch:2 \n",
      "Iteration:463 \n",
      "Loss:0.03745230287313461\n",
      "Epoch:2 \n",
      "Iteration:464 \n",
      "Loss:0.05954829230904579\n",
      "Epoch:2 \n",
      "Iteration:465 \n",
      "Loss:0.08531931787729263\n",
      "Epoch:2 \n",
      "Iteration:466 \n",
      "Loss:0.027128970250487328\n",
      "Epoch:2 \n",
      "Iteration:467 \n",
      "Loss:0.12149880081415176\n",
      "Epoch:2 \n",
      "Iteration:468 \n",
      "Loss:0.16803333163261414\n",
      "Epoch:2 \n",
      "Iteration:469 \n",
      "Loss:0.05600021407008171\n",
      "Epoch:2 \n",
      "Iteration:470 \n",
      "Loss:0.11090677231550217\n",
      "Epoch:2 \n",
      "Iteration:471 \n",
      "Loss:0.06198808550834656\n",
      "Epoch:2 \n",
      "Iteration:472 \n",
      "Loss:0.10487203299999237\n",
      "Epoch:2 \n",
      "Iteration:473 \n",
      "Loss:0.1134944036602974\n",
      "Epoch:2 \n",
      "Iteration:474 \n",
      "Loss:0.027583608403801918\n",
      "Epoch:2 \n",
      "Iteration:475 \n",
      "Loss:0.03191682696342468\n",
      "Epoch:2 \n",
      "Iteration:476 \n",
      "Loss:0.06409953534603119\n",
      "Epoch:2 \n",
      "Iteration:477 \n",
      "Loss:0.10737903416156769\n",
      "Epoch:2 \n",
      "Iteration:478 \n",
      "Loss:0.16667672991752625\n",
      "Epoch:2 \n",
      "Iteration:479 \n",
      "Loss:0.10453347861766815\n",
      "Epoch:2 \n",
      "Iteration:480 \n",
      "Loss:0.06638959050178528\n",
      "Epoch:2 \n",
      "Iteration:481 \n",
      "Loss:0.16401077806949615\n",
      "Epoch:2 \n",
      "Iteration:482 \n",
      "Loss:0.12214735150337219\n",
      "Epoch:2 \n",
      "Iteration:483 \n",
      "Loss:0.043374136090278625\n",
      "Epoch:2 \n",
      "Iteration:484 \n",
      "Loss:0.06023109331727028\n",
      "Epoch:2 \n",
      "Iteration:485 \n",
      "Loss:0.09209244698286057\n",
      "Epoch:2 \n",
      "Iteration:486 \n",
      "Loss:0.17898482084274292\n",
      "Epoch:2 \n",
      "Iteration:487 \n",
      "Loss:0.10224682837724686\n",
      "Epoch:2 \n",
      "Iteration:488 \n",
      "Loss:0.11113987863063812\n",
      "Epoch:2 \n",
      "Iteration:489 \n",
      "Loss:0.05184522643685341\n",
      "Epoch:2 \n",
      "Iteration:490 \n",
      "Loss:0.07663967460393906\n",
      "Epoch:2 \n",
      "Iteration:491 \n",
      "Loss:0.10011367499828339\n",
      "Epoch:2 \n",
      "Iteration:492 \n",
      "Loss:0.04430067539215088\n",
      "Epoch:2 \n",
      "Iteration:493 \n",
      "Loss:0.10282577574253082\n",
      "Epoch:2 \n",
      "Iteration:494 \n",
      "Loss:0.09932348132133484\n",
      "Epoch:2 \n",
      "Iteration:495 \n",
      "Loss:0.05465708300471306\n",
      "Epoch:2 \n",
      "Iteration:496 \n",
      "Loss:0.15924575924873352\n",
      "Epoch:2 \n",
      "Iteration:497 \n",
      "Loss:0.07463816553354263\n",
      "Epoch:2 \n",
      "Iteration:498 \n",
      "Loss:0.04614052176475525\n",
      "Epoch:2 \n",
      "Iteration:499 \n",
      "Loss:0.10149058699607849\n",
      "Epoch:2 \n",
      "Iteration:500 \n",
      "Loss:0.041597623378038406\n",
      "Epoch:2 \n",
      "Iteration:501 \n",
      "Loss:0.10619933903217316\n",
      "Epoch:2 \n",
      "Iteration:502 \n",
      "Loss:0.11472491919994354\n",
      "Epoch:2 \n",
      "Iteration:503 \n",
      "Loss:0.047853611409664154\n",
      "Epoch:2 \n",
      "Iteration:504 \n",
      "Loss:0.09121329337358475\n",
      "Epoch:2 \n",
      "Iteration:505 \n",
      "Loss:0.07136335968971252\n",
      "Epoch:2 \n",
      "Iteration:506 \n",
      "Loss:0.11915867030620575\n",
      "Epoch:2 \n",
      "Iteration:507 \n",
      "Loss:0.07286311686038971\n",
      "Epoch:2 \n",
      "Iteration:508 \n",
      "Loss:0.013039316982030869\n",
      "Epoch:2 \n",
      "Iteration:509 \n",
      "Loss:0.1344774216413498\n",
      "Epoch:2 \n",
      "Iteration:510 \n",
      "Loss:0.06905459612607956\n",
      "Epoch:2 \n",
      "Iteration:511 \n",
      "Loss:0.16367079317569733\n",
      "Epoch:2 \n",
      "Iteration:512 \n",
      "Loss:0.14452095329761505\n",
      "Epoch:2 \n",
      "Iteration:513 \n",
      "Loss:0.09186919033527374\n",
      "Epoch:2 \n",
      "Iteration:514 \n",
      "Loss:0.18742643296718597\n",
      "Epoch:2 \n",
      "Iteration:515 \n",
      "Loss:0.14501602947711945\n",
      "Epoch:2 \n",
      "Iteration:516 \n",
      "Loss:0.056415166705846786\n",
      "Epoch:2 \n",
      "Iteration:517 \n",
      "Loss:0.15252907574176788\n",
      "Epoch:2 \n",
      "Iteration:518 \n",
      "Loss:0.08370443433523178\n",
      "Epoch:2 \n",
      "Iteration:519 \n",
      "Loss:0.05489034205675125\n",
      "Epoch:2 \n",
      "Iteration:520 \n",
      "Loss:0.12789282202720642\n",
      "Epoch:2 \n",
      "Iteration:521 \n",
      "Loss:0.01347032655030489\n",
      "Epoch:2 \n",
      "Iteration:522 \n",
      "Loss:0.16926449537277222\n",
      "Epoch:2 \n",
      "Iteration:523 \n",
      "Loss:0.15164607763290405\n",
      "Epoch:2 \n",
      "Iteration:524 \n",
      "Loss:0.1280922144651413\n",
      "Epoch:2 \n",
      "Iteration:525 \n",
      "Loss:0.042470403015613556\n",
      "Epoch:2 \n",
      "Iteration:526 \n",
      "Loss:0.04008248448371887\n",
      "Epoch:2 \n",
      "Iteration:527 \n",
      "Loss:0.2344820201396942\n",
      "Epoch:2 \n",
      "Iteration:528 \n",
      "Loss:0.15870089828968048\n",
      "Epoch:2 \n",
      "Iteration:529 \n",
      "Loss:0.02038516290485859\n",
      "Epoch:2 \n",
      "Iteration:530 \n",
      "Loss:0.07264091074466705\n",
      "Epoch:2 \n",
      "Iteration:531 \n",
      "Loss:0.13728106021881104\n",
      "Epoch:2 \n",
      "Iteration:532 \n",
      "Loss:0.10198420286178589\n",
      "Epoch:2 \n",
      "Iteration:533 \n",
      "Loss:0.0652262344956398\n",
      "Epoch:2 \n",
      "Iteration:534 \n",
      "Loss:0.028083300217986107\n",
      "Epoch:2 \n",
      "Iteration:535 \n",
      "Loss:0.08931101113557816\n",
      "Epoch:2 \n",
      "Iteration:536 \n",
      "Loss:0.078989677131176\n",
      "Epoch:2 \n",
      "Iteration:537 \n",
      "Loss:0.05935883894562721\n",
      "Epoch:2 \n",
      "Iteration:538 \n",
      "Loss:0.0985092744231224\n",
      "Epoch:2 \n",
      "Iteration:539 \n",
      "Loss:0.09143944084644318\n",
      "Epoch:2 \n",
      "Iteration:540 \n",
      "Loss:0.11642756313085556\n",
      "Epoch:2 \n",
      "Iteration:541 \n",
      "Loss:0.06259001046419144\n",
      "Epoch:2 \n",
      "Iteration:542 \n",
      "Loss:0.03879936784505844\n",
      "Epoch:2 \n",
      "Iteration:543 \n",
      "Loss:0.10426793992519379\n",
      "Epoch:2 \n",
      "Iteration:544 \n",
      "Loss:0.014819770120084286\n",
      "Epoch:2 \n",
      "Iteration:545 \n",
      "Loss:0.036482661962509155\n",
      "Epoch:2 \n",
      "Iteration:546 \n",
      "Loss:0.13940773904323578\n",
      "Epoch:2 \n",
      "Iteration:547 \n",
      "Loss:0.14229685068130493\n",
      "Epoch:2 \n",
      "Iteration:548 \n",
      "Loss:0.20095893740653992\n",
      "Epoch:2 \n",
      "Iteration:549 \n",
      "Loss:0.11181967705488205\n",
      "Epoch:2 \n",
      "Iteration:550 \n",
      "Loss:0.05977347865700722\n",
      "Epoch:2 \n",
      "Iteration:551 \n",
      "Loss:0.08531860262155533\n",
      "Epoch:2 \n",
      "Iteration:552 \n",
      "Loss:0.06092125549912453\n",
      "Epoch:2 \n",
      "Iteration:553 \n",
      "Loss:0.013145566917955875\n",
      "Epoch:2 \n",
      "Iteration:554 \n",
      "Loss:0.17936508357524872\n",
      "Epoch:2 \n",
      "Iteration:555 \n",
      "Loss:0.11644499748945236\n",
      "Epoch:2 \n",
      "Iteration:556 \n",
      "Loss:0.13941636681556702\n",
      "Epoch:2 \n",
      "Iteration:557 \n",
      "Loss:0.05345474183559418\n",
      "Epoch:2 \n",
      "Iteration:558 \n",
      "Loss:0.08788249641656876\n",
      "Epoch:2 \n",
      "Iteration:559 \n",
      "Loss:0.13953299820423126\n",
      "Epoch:2 \n",
      "Iteration:560 \n",
      "Loss:0.1786438524723053\n",
      "Epoch:2 \n",
      "Iteration:561 \n",
      "Loss:0.15164700150489807\n",
      "Epoch:2 \n",
      "Iteration:562 \n",
      "Loss:0.08470664173364639\n",
      "Epoch:2 \n",
      "Iteration:563 \n",
      "Loss:0.13302446901798248\n",
      "Epoch:2 \n",
      "Iteration:564 \n",
      "Loss:0.10721060633659363\n",
      "Epoch:2 \n",
      "Iteration:565 \n",
      "Loss:0.074092335999012\n",
      "Epoch:2 \n",
      "Iteration:566 \n",
      "Loss:0.13409669697284698\n",
      "Epoch:2 \n",
      "Iteration:567 \n",
      "Loss:0.042138777673244476\n",
      "Epoch:2 \n",
      "Iteration:568 \n",
      "Loss:0.12802094221115112\n",
      "Epoch:2 \n",
      "Iteration:569 \n",
      "Loss:0.1266195923089981\n",
      "Epoch:2 \n",
      "Iteration:570 \n",
      "Loss:0.0965368002653122\n",
      "Epoch:2 \n",
      "Iteration:571 \n",
      "Loss:0.06095511093735695\n",
      "Epoch:2 \n",
      "Iteration:572 \n",
      "Loss:0.10491208732128143\n",
      "Epoch:2 \n",
      "Iteration:573 \n",
      "Loss:0.049069810658693314\n",
      "Epoch:2 \n",
      "Iteration:574 \n",
      "Loss:0.06768140941858292\n",
      "Epoch:2 \n",
      "Iteration:575 \n",
      "Loss:0.03246474266052246\n",
      "Epoch:2 \n",
      "Iteration:576 \n",
      "Loss:0.11883799731731415\n",
      "Epoch:2 \n",
      "Iteration:577 \n",
      "Loss:0.06053781881928444\n",
      "Epoch:2 \n",
      "Iteration:578 \n",
      "Loss:0.019635546952486038\n",
      "Epoch:2 \n",
      "Iteration:579 \n",
      "Loss:0.09838902205228806\n",
      "Epoch:2 \n",
      "Iteration:580 \n",
      "Loss:0.16635048389434814\n",
      "Epoch:2 \n",
      "Iteration:581 \n",
      "Loss:0.08783382177352905\n",
      "Epoch:2 \n",
      "Iteration:582 \n",
      "Loss:0.046604935079813004\n",
      "Epoch:2 \n",
      "Iteration:583 \n",
      "Loss:0.06369833648204803\n",
      "Epoch:2 \n",
      "Iteration:584 \n",
      "Loss:0.018946215510368347\n",
      "Epoch:2 \n",
      "Iteration:585 \n",
      "Loss:0.09667161852121353\n",
      "Epoch:2 \n",
      "Iteration:586 \n",
      "Loss:0.12447510659694672\n",
      "Epoch:2 \n",
      "Iteration:587 \n",
      "Loss:0.1134222224354744\n",
      "Epoch:2 \n",
      "Iteration:588 \n",
      "Loss:0.0619090236723423\n",
      "Epoch:2 \n",
      "Iteration:589 \n",
      "Loss:0.03641628101468086\n",
      "Epoch:2 \n",
      "Iteration:590 \n",
      "Loss:0.08511552959680557\n",
      "Epoch:2 \n",
      "Iteration:591 \n",
      "Loss:0.13629785180091858\n",
      "Epoch:2 \n",
      "Iteration:592 \n",
      "Loss:0.037782732397317886\n",
      "Epoch:2 \n",
      "Iteration:593 \n",
      "Loss:0.12178778648376465\n",
      "Epoch:2 \n",
      "Iteration:594 \n",
      "Loss:0.027897292748093605\n",
      "Epoch:2 \n",
      "Iteration:595 \n",
      "Loss:0.0259077288210392\n",
      "Epoch:2 \n",
      "Iteration:596 \n",
      "Loss:0.026701796799898148\n",
      "Epoch:2 \n",
      "Iteration:597 \n",
      "Loss:0.04176654666662216\n",
      "Epoch:2 \n",
      "Iteration:598 \n",
      "Loss:0.03215411677956581\n",
      "Epoch:2 \n",
      "Iteration:599 \n",
      "Loss:0.024355784058570862\n",
      "Epoch:2 \n",
      "Iteration:600 \n",
      "Loss:0.10602573305368423\n",
      "\n",
      "Accuracy of network in epoch 2: 97.215\n",
      "Epoch:3 \n",
      "Iteration:1 \n",
      "Loss:0.06507548689842224\n",
      "Epoch:3 \n",
      "Iteration:2 \n",
      "Loss:0.05685470625758171\n",
      "Epoch:3 \n",
      "Iteration:3 \n",
      "Loss:0.050836361944675446\n",
      "Epoch:3 \n",
      "Iteration:4 \n",
      "Loss:0.1531015932559967\n",
      "Epoch:3 \n",
      "Iteration:5 \n",
      "Loss:0.05276000499725342\n",
      "Epoch:3 \n",
      "Iteration:6 \n",
      "Loss:0.040957726538181305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3 \n",
      "Iteration:7 \n",
      "Loss:0.009984851814806461\n",
      "Epoch:3 \n",
      "Iteration:8 \n",
      "Loss:0.07752212882041931\n",
      "Epoch:3 \n",
      "Iteration:9 \n",
      "Loss:0.055770035833120346\n",
      "Epoch:3 \n",
      "Iteration:10 \n",
      "Loss:0.011395781300961971\n",
      "Epoch:3 \n",
      "Iteration:11 \n",
      "Loss:0.12790292501449585\n",
      "Epoch:3 \n",
      "Iteration:12 \n",
      "Loss:0.026682408526539803\n",
      "Epoch:3 \n",
      "Iteration:13 \n",
      "Loss:0.061499666422605515\n",
      "Epoch:3 \n",
      "Iteration:14 \n",
      "Loss:0.061734963208436966\n",
      "Epoch:3 \n",
      "Iteration:15 \n",
      "Loss:0.11611274629831314\n",
      "Epoch:3 \n",
      "Iteration:16 \n",
      "Loss:0.14121262729167938\n",
      "Epoch:3 \n",
      "Iteration:17 \n",
      "Loss:0.027529515326023102\n",
      "Epoch:3 \n",
      "Iteration:18 \n",
      "Loss:0.0688527449965477\n",
      "Epoch:3 \n",
      "Iteration:19 \n",
      "Loss:0.05044511705636978\n",
      "Epoch:3 \n",
      "Iteration:20 \n",
      "Loss:0.05832422897219658\n",
      "Epoch:3 \n",
      "Iteration:21 \n",
      "Loss:0.06288693100214005\n",
      "Epoch:3 \n",
      "Iteration:22 \n",
      "Loss:0.1361725628376007\n",
      "Epoch:3 \n",
      "Iteration:23 \n",
      "Loss:0.08357694000005722\n",
      "Epoch:3 \n",
      "Iteration:24 \n",
      "Loss:0.008071056567132473\n",
      "Epoch:3 \n",
      "Iteration:25 \n",
      "Loss:0.019149335101246834\n",
      "Epoch:3 \n",
      "Iteration:26 \n",
      "Loss:0.004059730097651482\n",
      "Epoch:3 \n",
      "Iteration:27 \n",
      "Loss:0.046423234045505524\n",
      "Epoch:3 \n",
      "Iteration:28 \n",
      "Loss:0.08173656463623047\n",
      "Epoch:3 \n",
      "Iteration:29 \n",
      "Loss:0.14501319825649261\n",
      "Epoch:3 \n",
      "Iteration:30 \n",
      "Loss:0.06762338429689407\n",
      "Epoch:3 \n",
      "Iteration:31 \n",
      "Loss:0.06413081288337708\n",
      "Epoch:3 \n",
      "Iteration:32 \n",
      "Loss:0.0186809953302145\n",
      "Epoch:3 \n",
      "Iteration:33 \n",
      "Loss:0.055727168917655945\n",
      "Epoch:3 \n",
      "Iteration:34 \n",
      "Loss:0.08849415183067322\n",
      "Epoch:3 \n",
      "Iteration:35 \n",
      "Loss:0.08858504891395569\n",
      "Epoch:3 \n",
      "Iteration:36 \n",
      "Loss:0.07735386490821838\n",
      "Epoch:3 \n",
      "Iteration:37 \n",
      "Loss:0.06992447376251221\n",
      "Epoch:3 \n",
      "Iteration:38 \n",
      "Loss:0.047465477138757706\n",
      "Epoch:3 \n",
      "Iteration:39 \n",
      "Loss:0.10266265273094177\n",
      "Epoch:3 \n",
      "Iteration:40 \n",
      "Loss:0.05181694030761719\n",
      "Epoch:3 \n",
      "Iteration:41 \n",
      "Loss:0.018204815685749054\n",
      "Epoch:3 \n",
      "Iteration:42 \n",
      "Loss:0.07821105420589447\n",
      "Epoch:3 \n",
      "Iteration:43 \n",
      "Loss:0.029496340081095695\n",
      "Epoch:3 \n",
      "Iteration:44 \n",
      "Loss:0.06150510907173157\n",
      "Epoch:3 \n",
      "Iteration:45 \n",
      "Loss:0.09743770956993103\n",
      "Epoch:3 \n",
      "Iteration:46 \n",
      "Loss:0.023362793028354645\n",
      "Epoch:3 \n",
      "Iteration:47 \n",
      "Loss:0.012773985043168068\n",
      "Epoch:3 \n",
      "Iteration:48 \n",
      "Loss:0.026919851079583168\n",
      "Epoch:3 \n",
      "Iteration:49 \n",
      "Loss:0.05501348897814751\n",
      "Epoch:3 \n",
      "Iteration:50 \n",
      "Loss:0.03430558741092682\n",
      "Epoch:3 \n",
      "Iteration:51 \n",
      "Loss:0.06731242686510086\n",
      "Epoch:3 \n",
      "Iteration:52 \n",
      "Loss:0.02945077419281006\n",
      "Epoch:3 \n",
      "Iteration:53 \n",
      "Loss:0.025796061381697655\n",
      "Epoch:3 \n",
      "Iteration:54 \n",
      "Loss:0.036317549645900726\n",
      "Epoch:3 \n",
      "Iteration:55 \n",
      "Loss:0.08765850216150284\n",
      "Epoch:3 \n",
      "Iteration:56 \n",
      "Loss:0.03827374801039696\n",
      "Epoch:3 \n",
      "Iteration:57 \n",
      "Loss:0.10664905607700348\n",
      "Epoch:3 \n",
      "Iteration:58 \n",
      "Loss:0.01799718104302883\n",
      "Epoch:3 \n",
      "Iteration:59 \n",
      "Loss:0.0073890988714993\n",
      "Epoch:3 \n",
      "Iteration:60 \n",
      "Loss:0.06816842406988144\n",
      "Epoch:3 \n",
      "Iteration:61 \n",
      "Loss:0.09402107447385788\n",
      "Epoch:3 \n",
      "Iteration:62 \n",
      "Loss:0.04320809990167618\n",
      "Epoch:3 \n",
      "Iteration:63 \n",
      "Loss:0.02976069040596485\n",
      "Epoch:3 \n",
      "Iteration:64 \n",
      "Loss:0.08735881745815277\n",
      "Epoch:3 \n",
      "Iteration:65 \n",
      "Loss:0.07688412070274353\n",
      "Epoch:3 \n",
      "Iteration:66 \n",
      "Loss:0.06768199056386948\n",
      "Epoch:3 \n",
      "Iteration:67 \n",
      "Loss:0.023938028141856194\n",
      "Epoch:3 \n",
      "Iteration:68 \n",
      "Loss:0.05929949879646301\n",
      "Epoch:3 \n",
      "Iteration:69 \n",
      "Loss:0.02841467782855034\n",
      "Epoch:3 \n",
      "Iteration:70 \n",
      "Loss:0.1679990440607071\n",
      "Epoch:3 \n",
      "Iteration:71 \n",
      "Loss:0.12365128844976425\n",
      "Epoch:3 \n",
      "Iteration:72 \n",
      "Loss:0.2302166223526001\n",
      "Epoch:3 \n",
      "Iteration:73 \n",
      "Loss:0.09471377730369568\n",
      "Epoch:3 \n",
      "Iteration:74 \n",
      "Loss:0.05234023556113243\n",
      "Epoch:3 \n",
      "Iteration:75 \n",
      "Loss:0.07776690274477005\n",
      "Epoch:3 \n",
      "Iteration:76 \n",
      "Loss:0.03717222809791565\n",
      "Epoch:3 \n",
      "Iteration:77 \n",
      "Loss:0.12611238658428192\n",
      "Epoch:3 \n",
      "Iteration:78 \n",
      "Loss:0.049026697874069214\n",
      "Epoch:3 \n",
      "Iteration:79 \n",
      "Loss:0.04879935085773468\n",
      "Epoch:3 \n",
      "Iteration:80 \n",
      "Loss:0.03330663591623306\n",
      "Epoch:3 \n",
      "Iteration:81 \n",
      "Loss:0.053591154515743256\n",
      "Epoch:3 \n",
      "Iteration:82 \n",
      "Loss:0.05310116708278656\n",
      "Epoch:3 \n",
      "Iteration:83 \n",
      "Loss:0.07160241901874542\n",
      "Epoch:3 \n",
      "Iteration:84 \n",
      "Loss:0.046649474650621414\n",
      "Epoch:3 \n",
      "Iteration:85 \n",
      "Loss:0.06963767111301422\n",
      "Epoch:3 \n",
      "Iteration:86 \n",
      "Loss:0.06221383064985275\n",
      "Epoch:3 \n",
      "Iteration:87 \n",
      "Loss:0.037910059094429016\n",
      "Epoch:3 \n",
      "Iteration:88 \n",
      "Loss:0.054913368076086044\n",
      "Epoch:3 \n",
      "Iteration:89 \n",
      "Loss:0.10284756869077682\n",
      "Epoch:3 \n",
      "Iteration:90 \n",
      "Loss:0.022256288677453995\n",
      "Epoch:3 \n",
      "Iteration:91 \n",
      "Loss:0.05771421268582344\n",
      "Epoch:3 \n",
      "Iteration:92 \n",
      "Loss:0.06307995319366455\n",
      "Epoch:3 \n",
      "Iteration:93 \n",
      "Loss:0.08350738883018494\n",
      "Epoch:3 \n",
      "Iteration:94 \n",
      "Loss:0.04036369174718857\n",
      "Epoch:3 \n",
      "Iteration:95 \n",
      "Loss:0.10758436471223831\n",
      "Epoch:3 \n",
      "Iteration:96 \n",
      "Loss:0.052400678396224976\n",
      "Epoch:3 \n",
      "Iteration:97 \n",
      "Loss:0.035749778151512146\n",
      "Epoch:3 \n",
      "Iteration:98 \n",
      "Loss:0.0746956467628479\n",
      "Epoch:3 \n",
      "Iteration:99 \n",
      "Loss:0.01734868809580803\n",
      "Epoch:3 \n",
      "Iteration:100 \n",
      "Loss:0.030712759122252464\n",
      "Epoch:3 \n",
      "Iteration:101 \n",
      "Loss:0.04040813073515892\n",
      "Epoch:3 \n",
      "Iteration:102 \n",
      "Loss:0.0280124731361866\n",
      "Epoch:3 \n",
      "Iteration:103 \n",
      "Loss:0.0047061508521437645\n",
      "Epoch:3 \n",
      "Iteration:104 \n",
      "Loss:0.032903045415878296\n",
      "Epoch:3 \n",
      "Iteration:105 \n",
      "Loss:0.0678003579378128\n",
      "Epoch:3 \n",
      "Iteration:106 \n",
      "Loss:0.07531868666410446\n",
      "Epoch:3 \n",
      "Iteration:107 \n",
      "Loss:0.011015328578650951\n",
      "Epoch:3 \n",
      "Iteration:108 \n",
      "Loss:0.08877000957727432\n",
      "Epoch:3 \n",
      "Iteration:109 \n",
      "Loss:0.039238158613443375\n",
      "Epoch:3 \n",
      "Iteration:110 \n",
      "Loss:0.06281284987926483\n",
      "Epoch:3 \n",
      "Iteration:111 \n",
      "Loss:0.009448271244764328\n",
      "Epoch:3 \n",
      "Iteration:112 \n",
      "Loss:0.011291783303022385\n",
      "Epoch:3 \n",
      "Iteration:113 \n",
      "Loss:0.14380235970020294\n",
      "Epoch:3 \n",
      "Iteration:114 \n",
      "Loss:0.04733504354953766\n",
      "Epoch:3 \n",
      "Iteration:115 \n",
      "Loss:0.0742424800992012\n",
      "Epoch:3 \n",
      "Iteration:116 \n",
      "Loss:0.03219160810112953\n",
      "Epoch:3 \n",
      "Iteration:117 \n",
      "Loss:0.08142344653606415\n",
      "Epoch:3 \n",
      "Iteration:118 \n",
      "Loss:0.0178996492177248\n",
      "Epoch:3 \n",
      "Iteration:119 \n",
      "Loss:0.023358721286058426\n",
      "Epoch:3 \n",
      "Iteration:120 \n",
      "Loss:0.043391358107328415\n",
      "Epoch:3 \n",
      "Iteration:121 \n",
      "Loss:0.006980248261243105\n",
      "Epoch:3 \n",
      "Iteration:122 \n",
      "Loss:0.03172539547085762\n",
      "Epoch:3 \n",
      "Iteration:123 \n",
      "Loss:0.025352146476507187\n",
      "Epoch:3 \n",
      "Iteration:124 \n",
      "Loss:0.10236527770757675\n",
      "Epoch:3 \n",
      "Iteration:125 \n",
      "Loss:0.03848658502101898\n",
      "Epoch:3 \n",
      "Iteration:126 \n",
      "Loss:0.118221215903759\n",
      "Epoch:3 \n",
      "Iteration:127 \n",
      "Loss:0.06300851702690125\n",
      "Epoch:3 \n",
      "Iteration:128 \n",
      "Loss:0.05326468497514725\n",
      "Epoch:3 \n",
      "Iteration:129 \n",
      "Loss:0.040979254990816116\n",
      "Epoch:3 \n",
      "Iteration:130 \n",
      "Loss:0.065855473279953\n",
      "Epoch:3 \n",
      "Iteration:131 \n",
      "Loss:0.1450963169336319\n",
      "Epoch:3 \n",
      "Iteration:132 \n",
      "Loss:0.06824100762605667\n",
      "Epoch:3 \n",
      "Iteration:133 \n",
      "Loss:0.01313074491918087\n",
      "Epoch:3 \n",
      "Iteration:134 \n",
      "Loss:0.02532646618783474\n",
      "Epoch:3 \n",
      "Iteration:135 \n",
      "Loss:0.011330613866448402\n",
      "Epoch:3 \n",
      "Iteration:136 \n",
      "Loss:0.12227289378643036\n",
      "Epoch:3 \n",
      "Iteration:137 \n",
      "Loss:0.042571473866701126\n",
      "Epoch:3 \n",
      "Iteration:138 \n",
      "Loss:0.05427588149905205\n",
      "Epoch:3 \n",
      "Iteration:139 \n",
      "Loss:0.08714954555034637\n",
      "Epoch:3 \n",
      "Iteration:140 \n",
      "Loss:0.19018897414207458\n",
      "Epoch:3 \n",
      "Iteration:141 \n",
      "Loss:0.033999938517808914\n",
      "Epoch:3 \n",
      "Iteration:142 \n",
      "Loss:0.07070242613554001\n",
      "Epoch:3 \n",
      "Iteration:143 \n",
      "Loss:0.05443559214472771\n",
      "Epoch:3 \n",
      "Iteration:144 \n",
      "Loss:0.011954161338508129\n",
      "Epoch:3 \n",
      "Iteration:145 \n",
      "Loss:0.043753813952207565\n",
      "Epoch:3 \n",
      "Iteration:146 \n",
      "Loss:0.036113590002059937\n",
      "Epoch:3 \n",
      "Iteration:147 \n",
      "Loss:0.03572002425789833\n",
      "Epoch:3 \n",
      "Iteration:148 \n",
      "Loss:0.020305495709180832\n",
      "Epoch:3 \n",
      "Iteration:149 \n",
      "Loss:0.13291138410568237\n",
      "Epoch:3 \n",
      "Iteration:150 \n",
      "Loss:0.05774228647351265\n",
      "Epoch:3 \n",
      "Iteration:151 \n",
      "Loss:0.09012045711278915\n",
      "Epoch:3 \n",
      "Iteration:152 \n",
      "Loss:0.07675786316394806\n",
      "Epoch:3 \n",
      "Iteration:153 \n",
      "Loss:0.027684856206178665\n",
      "Epoch:3 \n",
      "Iteration:154 \n",
      "Loss:0.047280844300985336\n",
      "Epoch:3 \n",
      "Iteration:155 \n",
      "Loss:0.03487660735845566\n",
      "Epoch:3 \n",
      "Iteration:156 \n",
      "Loss:0.03292856365442276\n",
      "Epoch:3 \n",
      "Iteration:157 \n",
      "Loss:0.05858949199318886\n",
      "Epoch:3 \n",
      "Iteration:158 \n",
      "Loss:0.08673455566167831\n",
      "Epoch:3 \n",
      "Iteration:159 \n",
      "Loss:0.03973165899515152\n",
      "Epoch:3 \n",
      "Iteration:160 \n",
      "Loss:0.1479998528957367\n",
      "Epoch:3 \n",
      "Iteration:161 \n",
      "Loss:0.04793471470475197\n",
      "Epoch:3 \n",
      "Iteration:162 \n",
      "Loss:0.025470953434705734\n",
      "Epoch:3 \n",
      "Iteration:163 \n",
      "Loss:0.06881735473871231\n",
      "Epoch:3 \n",
      "Iteration:164 \n",
      "Loss:0.017351467162370682\n",
      "Epoch:3 \n",
      "Iteration:165 \n",
      "Loss:0.09779451042413712\n",
      "Epoch:3 \n",
      "Iteration:166 \n",
      "Loss:0.040784209966659546\n",
      "Epoch:3 \n",
      "Iteration:167 \n",
      "Loss:0.07346498966217041\n",
      "Epoch:3 \n",
      "Iteration:168 \n",
      "Loss:0.021280502900481224\n",
      "Epoch:3 \n",
      "Iteration:169 \n",
      "Loss:0.07501194626092911\n",
      "Epoch:3 \n",
      "Iteration:170 \n",
      "Loss:0.0683286115527153\n",
      "Epoch:3 \n",
      "Iteration:171 \n",
      "Loss:0.046312157064676285\n",
      "Epoch:3 \n",
      "Iteration:172 \n",
      "Loss:0.013812290504574776\n",
      "Epoch:3 \n",
      "Iteration:173 \n",
      "Loss:0.08569474518299103\n",
      "Epoch:3 \n",
      "Iteration:174 \n",
      "Loss:0.08229739964008331\n",
      "Epoch:3 \n",
      "Iteration:175 \n",
      "Loss:0.09575134515762329\n",
      "Epoch:3 \n",
      "Iteration:176 \n",
      "Loss:0.05939517542719841\n",
      "Epoch:3 \n",
      "Iteration:177 \n",
      "Loss:0.04195753112435341\n",
      "Epoch:3 \n",
      "Iteration:178 \n",
      "Loss:0.07900586724281311\n",
      "Epoch:3 \n",
      "Iteration:179 \n",
      "Loss:0.11260567605495453\n",
      "Epoch:3 \n",
      "Iteration:180 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.04570407047867775\n",
      "Epoch:3 \n",
      "Iteration:181 \n",
      "Loss:0.11269282549619675\n",
      "Epoch:3 \n",
      "Iteration:182 \n",
      "Loss:0.046963516622781754\n",
      "Epoch:3 \n",
      "Iteration:183 \n",
      "Loss:0.053563155233860016\n",
      "Epoch:3 \n",
      "Iteration:184 \n",
      "Loss:0.14870327711105347\n",
      "Epoch:3 \n",
      "Iteration:185 \n",
      "Loss:0.022914163768291473\n",
      "Epoch:3 \n",
      "Iteration:186 \n",
      "Loss:0.049962759017944336\n",
      "Epoch:3 \n",
      "Iteration:187 \n",
      "Loss:0.0712253674864769\n",
      "Epoch:3 \n",
      "Iteration:188 \n",
      "Loss:0.1353616863489151\n",
      "Epoch:3 \n",
      "Iteration:189 \n",
      "Loss:0.043921440839767456\n",
      "Epoch:3 \n",
      "Iteration:190 \n",
      "Loss:0.09061047434806824\n",
      "Epoch:3 \n",
      "Iteration:191 \n",
      "Loss:0.22199466824531555\n",
      "Epoch:3 \n",
      "Iteration:192 \n",
      "Loss:0.07586213201284409\n",
      "Epoch:3 \n",
      "Iteration:193 \n",
      "Loss:0.0725313276052475\n",
      "Epoch:3 \n",
      "Iteration:194 \n",
      "Loss:0.047340258955955505\n",
      "Epoch:3 \n",
      "Iteration:195 \n",
      "Loss:0.10653276741504669\n",
      "Epoch:3 \n",
      "Iteration:196 \n",
      "Loss:0.09254342317581177\n",
      "Epoch:3 \n",
      "Iteration:197 \n",
      "Loss:0.0783831924200058\n",
      "Epoch:3 \n",
      "Iteration:198 \n",
      "Loss:0.0283550713211298\n",
      "Epoch:3 \n",
      "Iteration:199 \n",
      "Loss:0.029116563498973846\n",
      "Epoch:3 \n",
      "Iteration:200 \n",
      "Loss:0.0898512527346611\n",
      "Epoch:3 \n",
      "Iteration:201 \n",
      "Loss:0.07754413038492203\n",
      "Epoch:3 \n",
      "Iteration:202 \n",
      "Loss:0.27233198285102844\n",
      "Epoch:3 \n",
      "Iteration:203 \n",
      "Loss:0.018377184867858887\n",
      "Epoch:3 \n",
      "Iteration:204 \n",
      "Loss:0.04528307542204857\n",
      "Epoch:3 \n",
      "Iteration:205 \n",
      "Loss:0.02606377750635147\n",
      "Epoch:3 \n",
      "Iteration:206 \n",
      "Loss:0.0827031210064888\n",
      "Epoch:3 \n",
      "Iteration:207 \n",
      "Loss:0.08707594126462936\n",
      "Epoch:3 \n",
      "Iteration:208 \n",
      "Loss:0.10203990340232849\n",
      "Epoch:3 \n",
      "Iteration:209 \n",
      "Loss:0.05694695934653282\n",
      "Epoch:3 \n",
      "Iteration:210 \n",
      "Loss:0.0983322337269783\n",
      "Epoch:3 \n",
      "Iteration:211 \n",
      "Loss:0.0763617530465126\n",
      "Epoch:3 \n",
      "Iteration:212 \n",
      "Loss:0.04667603597044945\n",
      "Epoch:3 \n",
      "Iteration:213 \n",
      "Loss:0.12779290974140167\n",
      "Epoch:3 \n",
      "Iteration:214 \n",
      "Loss:0.09448330104351044\n",
      "Epoch:3 \n",
      "Iteration:215 \n",
      "Loss:0.013670090585947037\n",
      "Epoch:3 \n",
      "Iteration:216 \n",
      "Loss:0.021754853427410126\n",
      "Epoch:3 \n",
      "Iteration:217 \n",
      "Loss:0.18197712302207947\n",
      "Epoch:3 \n",
      "Iteration:218 \n",
      "Loss:0.10635692626237869\n",
      "Epoch:3 \n",
      "Iteration:219 \n",
      "Loss:0.06416255235671997\n",
      "Epoch:3 \n",
      "Iteration:220 \n",
      "Loss:0.06289878487586975\n",
      "Epoch:3 \n",
      "Iteration:221 \n",
      "Loss:0.046780314296483994\n",
      "Epoch:3 \n",
      "Iteration:222 \n",
      "Loss:0.17944437265396118\n",
      "Epoch:3 \n",
      "Iteration:223 \n",
      "Loss:0.04472976177930832\n",
      "Epoch:3 \n",
      "Iteration:224 \n",
      "Loss:0.024951009079813957\n",
      "Epoch:3 \n",
      "Iteration:225 \n",
      "Loss:0.05295839160680771\n",
      "Epoch:3 \n",
      "Iteration:226 \n",
      "Loss:0.07977861166000366\n",
      "Epoch:3 \n",
      "Iteration:227 \n",
      "Loss:0.051057200878858566\n",
      "Epoch:3 \n",
      "Iteration:228 \n",
      "Loss:0.06329610198736191\n",
      "Epoch:3 \n",
      "Iteration:229 \n",
      "Loss:0.032405558973550797\n",
      "Epoch:3 \n",
      "Iteration:230 \n",
      "Loss:0.03913480043411255\n",
      "Epoch:3 \n",
      "Iteration:231 \n",
      "Loss:0.05912960693240166\n",
      "Epoch:3 \n",
      "Iteration:232 \n",
      "Loss:0.04702155664563179\n",
      "Epoch:3 \n",
      "Iteration:233 \n",
      "Loss:0.028853679075837135\n",
      "Epoch:3 \n",
      "Iteration:234 \n",
      "Loss:0.0560724213719368\n",
      "Epoch:3 \n",
      "Iteration:235 \n",
      "Loss:0.08401910960674286\n",
      "Epoch:3 \n",
      "Iteration:236 \n",
      "Loss:0.07216251641511917\n",
      "Epoch:3 \n",
      "Iteration:237 \n",
      "Loss:0.06714190542697906\n",
      "Epoch:3 \n",
      "Iteration:238 \n",
      "Loss:0.018573889508843422\n",
      "Epoch:3 \n",
      "Iteration:239 \n",
      "Loss:0.027786126360297203\n",
      "Epoch:3 \n",
      "Iteration:240 \n",
      "Loss:0.06321242451667786\n",
      "Epoch:3 \n",
      "Iteration:241 \n",
      "Loss:0.07703101634979248\n",
      "Epoch:3 \n",
      "Iteration:242 \n",
      "Loss:0.02014216221868992\n",
      "Epoch:3 \n",
      "Iteration:243 \n",
      "Loss:0.12333828210830688\n",
      "Epoch:3 \n",
      "Iteration:244 \n",
      "Loss:0.08558963984251022\n",
      "Epoch:3 \n",
      "Iteration:245 \n",
      "Loss:0.06808220595121384\n",
      "Epoch:3 \n",
      "Iteration:246 \n",
      "Loss:0.07930892705917358\n",
      "Epoch:3 \n",
      "Iteration:247 \n",
      "Loss:0.04545634239912033\n",
      "Epoch:3 \n",
      "Iteration:248 \n",
      "Loss:0.027262378484010696\n",
      "Epoch:3 \n",
      "Iteration:249 \n",
      "Loss:0.005780881270766258\n",
      "Epoch:3 \n",
      "Iteration:250 \n",
      "Loss:0.0859537348151207\n",
      "Epoch:3 \n",
      "Iteration:251 \n",
      "Loss:0.1368052363395691\n",
      "Epoch:3 \n",
      "Iteration:252 \n",
      "Loss:0.05443878471851349\n",
      "Epoch:3 \n",
      "Iteration:253 \n",
      "Loss:0.04661558195948601\n",
      "Epoch:3 \n",
      "Iteration:254 \n",
      "Loss:0.012230098247528076\n",
      "Epoch:3 \n",
      "Iteration:255 \n",
      "Loss:0.05395623669028282\n",
      "Epoch:3 \n",
      "Iteration:256 \n",
      "Loss:0.04030292108654976\n",
      "Epoch:3 \n",
      "Iteration:257 \n",
      "Loss:0.13883548974990845\n",
      "Epoch:3 \n",
      "Iteration:258 \n",
      "Loss:0.041905637830495834\n",
      "Epoch:3 \n",
      "Iteration:259 \n",
      "Loss:0.04805561527609825\n",
      "Epoch:3 \n",
      "Iteration:260 \n",
      "Loss:0.005061304196715355\n",
      "Epoch:3 \n",
      "Iteration:261 \n",
      "Loss:0.01223828550428152\n",
      "Epoch:3 \n",
      "Iteration:262 \n",
      "Loss:0.06648411601781845\n",
      "Epoch:3 \n",
      "Iteration:263 \n",
      "Loss:0.06587139517068863\n",
      "Epoch:3 \n",
      "Iteration:264 \n",
      "Loss:0.04736965522170067\n",
      "Epoch:3 \n",
      "Iteration:265 \n",
      "Loss:0.18263986706733704\n",
      "Epoch:3 \n",
      "Iteration:266 \n",
      "Loss:0.08784440904855728\n",
      "Epoch:3 \n",
      "Iteration:267 \n",
      "Loss:0.10307471454143524\n",
      "Epoch:3 \n",
      "Iteration:268 \n",
      "Loss:0.023594481870532036\n",
      "Epoch:3 \n",
      "Iteration:269 \n",
      "Loss:0.07121305912733078\n",
      "Epoch:3 \n",
      "Iteration:270 \n",
      "Loss:0.11043193936347961\n",
      "Epoch:3 \n",
      "Iteration:271 \n",
      "Loss:0.0723830983042717\n",
      "Epoch:3 \n",
      "Iteration:272 \n",
      "Loss:0.035881590098142624\n",
      "Epoch:3 \n",
      "Iteration:273 \n",
      "Loss:0.05593635141849518\n",
      "Epoch:3 \n",
      "Iteration:274 \n",
      "Loss:0.14742034673690796\n",
      "Epoch:3 \n",
      "Iteration:275 \n",
      "Loss:0.107148677110672\n",
      "Epoch:3 \n",
      "Iteration:276 \n",
      "Loss:0.02263864129781723\n",
      "Epoch:3 \n",
      "Iteration:277 \n",
      "Loss:0.15307112038135529\n",
      "Epoch:3 \n",
      "Iteration:278 \n",
      "Loss:0.04787875711917877\n",
      "Epoch:3 \n",
      "Iteration:279 \n",
      "Loss:0.01562538929283619\n",
      "Epoch:3 \n",
      "Iteration:280 \n",
      "Loss:0.05024971812963486\n",
      "Epoch:3 \n",
      "Iteration:281 \n",
      "Loss:0.09709260612726212\n",
      "Epoch:3 \n",
      "Iteration:282 \n",
      "Loss:0.11517886817455292\n",
      "Epoch:3 \n",
      "Iteration:283 \n",
      "Loss:0.02666594460606575\n",
      "Epoch:3 \n",
      "Iteration:284 \n",
      "Loss:0.10835277289152145\n",
      "Epoch:3 \n",
      "Iteration:285 \n",
      "Loss:0.06984353065490723\n",
      "Epoch:3 \n",
      "Iteration:286 \n",
      "Loss:0.0071766371838748455\n",
      "Epoch:3 \n",
      "Iteration:287 \n",
      "Loss:0.175770103931427\n",
      "Epoch:3 \n",
      "Iteration:288 \n",
      "Loss:0.17256784439086914\n",
      "Epoch:3 \n",
      "Iteration:289 \n",
      "Loss:0.027325186878442764\n",
      "Epoch:3 \n",
      "Iteration:290 \n",
      "Loss:0.017134616151452065\n",
      "Epoch:3 \n",
      "Iteration:291 \n",
      "Loss:0.10804054141044617\n",
      "Epoch:3 \n",
      "Iteration:292 \n",
      "Loss:0.03294110670685768\n",
      "Epoch:3 \n",
      "Iteration:293 \n",
      "Loss:0.036379944533109665\n",
      "Epoch:3 \n",
      "Iteration:294 \n",
      "Loss:0.04411547631025314\n",
      "Epoch:3 \n",
      "Iteration:295 \n",
      "Loss:0.07565080374479294\n",
      "Epoch:3 \n",
      "Iteration:296 \n",
      "Loss:0.0573832169175148\n",
      "Epoch:3 \n",
      "Iteration:297 \n",
      "Loss:0.08438429981470108\n",
      "Epoch:3 \n",
      "Iteration:298 \n",
      "Loss:0.13010452687740326\n",
      "Epoch:3 \n",
      "Iteration:299 \n",
      "Loss:0.03196795657277107\n",
      "Epoch:3 \n",
      "Iteration:300 \n",
      "Loss:0.01535466592758894\n",
      "Epoch:3 \n",
      "Iteration:301 \n",
      "Loss:0.06136434152722359\n",
      "Epoch:3 \n",
      "Iteration:302 \n",
      "Loss:0.023291394114494324\n",
      "Epoch:3 \n",
      "Iteration:303 \n",
      "Loss:0.0185555312782526\n",
      "Epoch:3 \n",
      "Iteration:304 \n",
      "Loss:0.11194606125354767\n",
      "Epoch:3 \n",
      "Iteration:305 \n",
      "Loss:0.04936112463474274\n",
      "Epoch:3 \n",
      "Iteration:306 \n",
      "Loss:0.05691419914364815\n",
      "Epoch:3 \n",
      "Iteration:307 \n",
      "Loss:0.06733754277229309\n",
      "Epoch:3 \n",
      "Iteration:308 \n",
      "Loss:0.0600847564637661\n",
      "Epoch:3 \n",
      "Iteration:309 \n",
      "Loss:0.128073051571846\n",
      "Epoch:3 \n",
      "Iteration:310 \n",
      "Loss:0.08597646653652191\n",
      "Epoch:3 \n",
      "Iteration:311 \n",
      "Loss:0.09011654555797577\n",
      "Epoch:3 \n",
      "Iteration:312 \n",
      "Loss:0.03729492798447609\n",
      "Epoch:3 \n",
      "Iteration:313 \n",
      "Loss:0.1447417289018631\n",
      "Epoch:3 \n",
      "Iteration:314 \n",
      "Loss:0.026292163878679276\n",
      "Epoch:3 \n",
      "Iteration:315 \n",
      "Loss:0.04945821315050125\n",
      "Epoch:3 \n",
      "Iteration:316 \n",
      "Loss:0.023042850196361542\n",
      "Epoch:3 \n",
      "Iteration:317 \n",
      "Loss:0.043212905526161194\n",
      "Epoch:3 \n",
      "Iteration:318 \n",
      "Loss:0.042006924748420715\n",
      "Epoch:3 \n",
      "Iteration:319 \n",
      "Loss:0.047286033630371094\n",
      "Epoch:3 \n",
      "Iteration:320 \n",
      "Loss:0.05363646149635315\n",
      "Epoch:3 \n",
      "Iteration:321 \n",
      "Loss:0.05747861787676811\n",
      "Epoch:3 \n",
      "Iteration:322 \n",
      "Loss:0.07507288455963135\n",
      "Epoch:3 \n",
      "Iteration:323 \n",
      "Loss:0.049577079713344574\n",
      "Epoch:3 \n",
      "Iteration:324 \n",
      "Loss:0.052529629319906235\n",
      "Epoch:3 \n",
      "Iteration:325 \n",
      "Loss:0.0602988675236702\n",
      "Epoch:3 \n",
      "Iteration:326 \n",
      "Loss:0.08123020082712173\n",
      "Epoch:3 \n",
      "Iteration:327 \n",
      "Loss:0.01547863706946373\n",
      "Epoch:3 \n",
      "Iteration:328 \n",
      "Loss:0.07707434892654419\n",
      "Epoch:3 \n",
      "Iteration:329 \n",
      "Loss:0.04051278531551361\n",
      "Epoch:3 \n",
      "Iteration:330 \n",
      "Loss:0.009196980856359005\n",
      "Epoch:3 \n",
      "Iteration:331 \n",
      "Loss:0.0819530040025711\n",
      "Epoch:3 \n",
      "Iteration:332 \n",
      "Loss:0.20804451406002045\n",
      "Epoch:3 \n",
      "Iteration:333 \n",
      "Loss:0.015630604699254036\n",
      "Epoch:3 \n",
      "Iteration:334 \n",
      "Loss:0.047279275953769684\n",
      "Epoch:3 \n",
      "Iteration:335 \n",
      "Loss:0.015820754691958427\n",
      "Epoch:3 \n",
      "Iteration:336 \n",
      "Loss:0.030621837824583054\n",
      "Epoch:3 \n",
      "Iteration:337 \n",
      "Loss:0.04196449741721153\n",
      "Epoch:3 \n",
      "Iteration:338 \n",
      "Loss:0.26463621854782104\n",
      "Epoch:3 \n",
      "Iteration:339 \n",
      "Loss:0.027494564652442932\n",
      "Epoch:3 \n",
      "Iteration:340 \n",
      "Loss:0.026664409786462784\n",
      "Epoch:3 \n",
      "Iteration:341 \n",
      "Loss:0.021668260917067528\n",
      "Epoch:3 \n",
      "Iteration:342 \n",
      "Loss:0.036023177206516266\n",
      "Epoch:3 \n",
      "Iteration:343 \n",
      "Loss:0.08685459196567535\n",
      "Epoch:3 \n",
      "Iteration:344 \n",
      "Loss:0.03214238956570625\n",
      "Epoch:3 \n",
      "Iteration:345 \n",
      "Loss:0.15978045761585236\n",
      "Epoch:3 \n",
      "Iteration:346 \n",
      "Loss:0.1079966351389885\n",
      "Epoch:3 \n",
      "Iteration:347 \n",
      "Loss:0.016815856099128723\n",
      "Epoch:3 \n",
      "Iteration:348 \n",
      "Loss:0.02298133075237274\n",
      "Epoch:3 \n",
      "Iteration:349 \n",
      "Loss:0.04000164568424225\n",
      "Epoch:3 \n",
      "Iteration:350 \n",
      "Loss:0.028447626158595085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3 \n",
      "Iteration:351 \n",
      "Loss:0.021516937762498856\n",
      "Epoch:3 \n",
      "Iteration:352 \n",
      "Loss:0.07417622953653336\n",
      "Epoch:3 \n",
      "Iteration:353 \n",
      "Loss:0.06155497953295708\n",
      "Epoch:3 \n",
      "Iteration:354 \n",
      "Loss:0.023866722360253334\n",
      "Epoch:3 \n",
      "Iteration:355 \n",
      "Loss:0.1153397485613823\n",
      "Epoch:3 \n",
      "Iteration:356 \n",
      "Loss:0.06120302155613899\n",
      "Epoch:3 \n",
      "Iteration:357 \n",
      "Loss:0.02457522414624691\n",
      "Epoch:3 \n",
      "Iteration:358 \n",
      "Loss:0.05458023026585579\n",
      "Epoch:3 \n",
      "Iteration:359 \n",
      "Loss:0.08095678687095642\n",
      "Epoch:3 \n",
      "Iteration:360 \n",
      "Loss:0.045584581792354584\n",
      "Epoch:3 \n",
      "Iteration:361 \n",
      "Loss:0.026411592960357666\n",
      "Epoch:3 \n",
      "Iteration:362 \n",
      "Loss:0.026539266109466553\n",
      "Epoch:3 \n",
      "Iteration:363 \n",
      "Loss:0.050297483801841736\n",
      "Epoch:3 \n",
      "Iteration:364 \n",
      "Loss:0.016372786834836006\n",
      "Epoch:3 \n",
      "Iteration:365 \n",
      "Loss:0.027314424514770508\n",
      "Epoch:3 \n",
      "Iteration:366 \n",
      "Loss:0.037081822752952576\n",
      "Epoch:3 \n",
      "Iteration:367 \n",
      "Loss:0.15916821360588074\n",
      "Epoch:3 \n",
      "Iteration:368 \n",
      "Loss:0.060564830899238586\n",
      "Epoch:3 \n",
      "Iteration:369 \n",
      "Loss:0.1259833574295044\n",
      "Epoch:3 \n",
      "Iteration:370 \n",
      "Loss:0.10481490939855576\n",
      "Epoch:3 \n",
      "Iteration:371 \n",
      "Loss:0.058637332171201706\n",
      "Epoch:3 \n",
      "Iteration:372 \n",
      "Loss:0.07531488686800003\n",
      "Epoch:3 \n",
      "Iteration:373 \n",
      "Loss:0.19107292592525482\n",
      "Epoch:3 \n",
      "Iteration:374 \n",
      "Loss:0.1496812105178833\n",
      "Epoch:3 \n",
      "Iteration:375 \n",
      "Loss:0.213098406791687\n",
      "Epoch:3 \n",
      "Iteration:376 \n",
      "Loss:0.06078518554568291\n",
      "Epoch:3 \n",
      "Iteration:377 \n",
      "Loss:0.04792303591966629\n",
      "Epoch:3 \n",
      "Iteration:378 \n",
      "Loss:0.12813062965869904\n",
      "Epoch:3 \n",
      "Iteration:379 \n",
      "Loss:0.08556623756885529\n",
      "Epoch:3 \n",
      "Iteration:380 \n",
      "Loss:0.024678921326994896\n",
      "Epoch:3 \n",
      "Iteration:381 \n",
      "Loss:0.0942143127322197\n",
      "Epoch:3 \n",
      "Iteration:382 \n",
      "Loss:0.04351355507969856\n",
      "Epoch:3 \n",
      "Iteration:383 \n",
      "Loss:0.04660025238990784\n",
      "Epoch:3 \n",
      "Iteration:384 \n",
      "Loss:0.06373007595539093\n",
      "Epoch:3 \n",
      "Iteration:385 \n",
      "Loss:0.05564454570412636\n",
      "Epoch:3 \n",
      "Iteration:386 \n",
      "Loss:0.032816123217344284\n",
      "Epoch:3 \n",
      "Iteration:387 \n",
      "Loss:0.08133237808942795\n",
      "Epoch:3 \n",
      "Iteration:388 \n",
      "Loss:0.07473548501729965\n",
      "Epoch:3 \n",
      "Iteration:389 \n",
      "Loss:0.08674032241106033\n",
      "Epoch:3 \n",
      "Iteration:390 \n",
      "Loss:0.03935132920742035\n",
      "Epoch:3 \n",
      "Iteration:391 \n",
      "Loss:0.12914147973060608\n",
      "Epoch:3 \n",
      "Iteration:392 \n",
      "Loss:0.0740227997303009\n",
      "Epoch:3 \n",
      "Iteration:393 \n",
      "Loss:0.07169558852910995\n",
      "Epoch:3 \n",
      "Iteration:394 \n",
      "Loss:0.07901866734027863\n",
      "Epoch:3 \n",
      "Iteration:395 \n",
      "Loss:0.012888790108263493\n",
      "Epoch:3 \n",
      "Iteration:396 \n",
      "Loss:0.023101985454559326\n",
      "Epoch:3 \n",
      "Iteration:397 \n",
      "Loss:0.027346855029463768\n",
      "Epoch:3 \n",
      "Iteration:398 \n",
      "Loss:0.050713587552309036\n",
      "Epoch:3 \n",
      "Iteration:399 \n",
      "Loss:0.13524292409420013\n",
      "Epoch:3 \n",
      "Iteration:400 \n",
      "Loss:0.009454277344048023\n",
      "Epoch:3 \n",
      "Iteration:401 \n",
      "Loss:0.0868653953075409\n",
      "Epoch:3 \n",
      "Iteration:402 \n",
      "Loss:0.09657636284828186\n",
      "Epoch:3 \n",
      "Iteration:403 \n",
      "Loss:0.06118474155664444\n",
      "Epoch:3 \n",
      "Iteration:404 \n",
      "Loss:0.007587381638586521\n",
      "Epoch:3 \n",
      "Iteration:405 \n",
      "Loss:0.07533643394708633\n",
      "Epoch:3 \n",
      "Iteration:406 \n",
      "Loss:0.0638110414147377\n",
      "Epoch:3 \n",
      "Iteration:407 \n",
      "Loss:0.09557557851076126\n",
      "Epoch:3 \n",
      "Iteration:408 \n",
      "Loss:0.18209640681743622\n",
      "Epoch:3 \n",
      "Iteration:409 \n",
      "Loss:0.03930702060461044\n",
      "Epoch:3 \n",
      "Iteration:410 \n",
      "Loss:0.05902566760778427\n",
      "Epoch:3 \n",
      "Iteration:411 \n",
      "Loss:0.030768513679504395\n",
      "Epoch:3 \n",
      "Iteration:412 \n",
      "Loss:0.03843061625957489\n",
      "Epoch:3 \n",
      "Iteration:413 \n",
      "Loss:0.028910653665661812\n",
      "Epoch:3 \n",
      "Iteration:414 \n",
      "Loss:0.05996627360582352\n",
      "Epoch:3 \n",
      "Iteration:415 \n",
      "Loss:0.02921878732740879\n",
      "Epoch:3 \n",
      "Iteration:416 \n",
      "Loss:0.03671879693865776\n",
      "Epoch:3 \n",
      "Iteration:417 \n",
      "Loss:0.033204495906829834\n",
      "Epoch:3 \n",
      "Iteration:418 \n",
      "Loss:0.07315924763679504\n",
      "Epoch:3 \n",
      "Iteration:419 \n",
      "Loss:0.12551261484622955\n",
      "Epoch:3 \n",
      "Iteration:420 \n",
      "Loss:0.029948584735393524\n",
      "Epoch:3 \n",
      "Iteration:421 \n",
      "Loss:0.1145990863442421\n",
      "Epoch:3 \n",
      "Iteration:422 \n",
      "Loss:0.12129977345466614\n",
      "Epoch:3 \n",
      "Iteration:423 \n",
      "Loss:0.11679253727197647\n",
      "Epoch:3 \n",
      "Iteration:424 \n",
      "Loss:0.06504392623901367\n",
      "Epoch:3 \n",
      "Iteration:425 \n",
      "Loss:0.11825316399335861\n",
      "Epoch:3 \n",
      "Iteration:426 \n",
      "Loss:0.021926239132881165\n",
      "Epoch:3 \n",
      "Iteration:427 \n",
      "Loss:0.03096041828393936\n",
      "Epoch:3 \n",
      "Iteration:428 \n",
      "Loss:0.0646766647696495\n",
      "Epoch:3 \n",
      "Iteration:429 \n",
      "Loss:0.08733537048101425\n",
      "Epoch:3 \n",
      "Iteration:430 \n",
      "Loss:0.014815184287726879\n",
      "Epoch:3 \n",
      "Iteration:431 \n",
      "Loss:0.05173107236623764\n",
      "Epoch:3 \n",
      "Iteration:432 \n",
      "Loss:0.00603444641456008\n",
      "Epoch:3 \n",
      "Iteration:433 \n",
      "Loss:0.016772666946053505\n",
      "Epoch:3 \n",
      "Iteration:434 \n",
      "Loss:0.03908924758434296\n",
      "Epoch:3 \n",
      "Iteration:435 \n",
      "Loss:0.03855489566922188\n",
      "Epoch:3 \n",
      "Iteration:436 \n",
      "Loss:0.03819161280989647\n",
      "Epoch:3 \n",
      "Iteration:437 \n",
      "Loss:0.032628774642944336\n",
      "Epoch:3 \n",
      "Iteration:438 \n",
      "Loss:0.036820754408836365\n",
      "Epoch:3 \n",
      "Iteration:439 \n",
      "Loss:0.04800525680184364\n",
      "Epoch:3 \n",
      "Iteration:440 \n",
      "Loss:0.033686865121126175\n",
      "Epoch:3 \n",
      "Iteration:441 \n",
      "Loss:0.0831451416015625\n",
      "Epoch:3 \n",
      "Iteration:442 \n",
      "Loss:0.059462033212184906\n",
      "Epoch:3 \n",
      "Iteration:443 \n",
      "Loss:0.14983554184436798\n",
      "Epoch:3 \n",
      "Iteration:444 \n",
      "Loss:0.012346874922513962\n",
      "Epoch:3 \n",
      "Iteration:445 \n",
      "Loss:0.014506292529404163\n",
      "Epoch:3 \n",
      "Iteration:446 \n",
      "Loss:0.12367135286331177\n",
      "Epoch:3 \n",
      "Iteration:447 \n",
      "Loss:0.017795918509364128\n",
      "Epoch:3 \n",
      "Iteration:448 \n",
      "Loss:0.055968575179576874\n",
      "Epoch:3 \n",
      "Iteration:449 \n",
      "Loss:0.04598839208483696\n",
      "Epoch:3 \n",
      "Iteration:450 \n",
      "Loss:0.015113008208572865\n",
      "Epoch:3 \n",
      "Iteration:451 \n",
      "Loss:0.014321736991405487\n",
      "Epoch:3 \n",
      "Iteration:452 \n",
      "Loss:0.07545002549886703\n",
      "Epoch:3 \n",
      "Iteration:453 \n",
      "Loss:0.019356178119778633\n",
      "Epoch:3 \n",
      "Iteration:454 \n",
      "Loss:0.11621472239494324\n",
      "Epoch:3 \n",
      "Iteration:455 \n",
      "Loss:0.19944007694721222\n",
      "Epoch:3 \n",
      "Iteration:456 \n",
      "Loss:0.04061676561832428\n",
      "Epoch:3 \n",
      "Iteration:457 \n",
      "Loss:0.044181112200021744\n",
      "Epoch:3 \n",
      "Iteration:458 \n",
      "Loss:0.05118435248732567\n",
      "Epoch:3 \n",
      "Iteration:459 \n",
      "Loss:0.12524893879890442\n",
      "Epoch:3 \n",
      "Iteration:460 \n",
      "Loss:0.09160235524177551\n",
      "Epoch:3 \n",
      "Iteration:461 \n",
      "Loss:0.08854662626981735\n",
      "Epoch:3 \n",
      "Iteration:462 \n",
      "Loss:0.018272338435053825\n",
      "Epoch:3 \n",
      "Iteration:463 \n",
      "Loss:0.079646036028862\n",
      "Epoch:3 \n",
      "Iteration:464 \n",
      "Loss:0.017677420750260353\n",
      "Epoch:3 \n",
      "Iteration:465 \n",
      "Loss:0.10727082937955856\n",
      "Epoch:3 \n",
      "Iteration:466 \n",
      "Loss:0.15684157609939575\n",
      "Epoch:3 \n",
      "Iteration:467 \n",
      "Loss:0.07077421993017197\n",
      "Epoch:3 \n",
      "Iteration:468 \n",
      "Loss:0.02392643690109253\n",
      "Epoch:3 \n",
      "Iteration:469 \n",
      "Loss:0.028030812740325928\n",
      "Epoch:3 \n",
      "Iteration:470 \n",
      "Loss:0.023524584248661995\n",
      "Epoch:3 \n",
      "Iteration:471 \n",
      "Loss:0.05504795163869858\n",
      "Epoch:3 \n",
      "Iteration:472 \n",
      "Loss:0.041099730879068375\n",
      "Epoch:3 \n",
      "Iteration:473 \n",
      "Loss:0.05134458839893341\n",
      "Epoch:3 \n",
      "Iteration:474 \n",
      "Loss:0.11096357554197311\n",
      "Epoch:3 \n",
      "Iteration:475 \n",
      "Loss:0.04499058425426483\n",
      "Epoch:3 \n",
      "Iteration:476 \n",
      "Loss:0.036424990743398666\n",
      "Epoch:3 \n",
      "Iteration:477 \n",
      "Loss:0.04963929206132889\n",
      "Epoch:3 \n",
      "Iteration:478 \n",
      "Loss:0.07822341471910477\n",
      "Epoch:3 \n",
      "Iteration:479 \n",
      "Loss:0.038502320647239685\n",
      "Epoch:3 \n",
      "Iteration:480 \n",
      "Loss:0.05495280399918556\n",
      "Epoch:3 \n",
      "Iteration:481 \n",
      "Loss:0.05292540416121483\n",
      "Epoch:3 \n",
      "Iteration:482 \n",
      "Loss:0.08286896347999573\n",
      "Epoch:3 \n",
      "Iteration:483 \n",
      "Loss:0.06266821175813675\n",
      "Epoch:3 \n",
      "Iteration:484 \n",
      "Loss:0.1510794460773468\n",
      "Epoch:3 \n",
      "Iteration:485 \n",
      "Loss:0.02668604627251625\n",
      "Epoch:3 \n",
      "Iteration:486 \n",
      "Loss:0.01735815592110157\n",
      "Epoch:3 \n",
      "Iteration:487 \n",
      "Loss:0.012418835423886776\n",
      "Epoch:3 \n",
      "Iteration:488 \n",
      "Loss:0.026516320183873177\n",
      "Epoch:3 \n",
      "Iteration:489 \n",
      "Loss:0.04831799492239952\n",
      "Epoch:3 \n",
      "Iteration:490 \n",
      "Loss:0.024625005200505257\n",
      "Epoch:3 \n",
      "Iteration:491 \n",
      "Loss:0.031595949083566666\n",
      "Epoch:3 \n",
      "Iteration:492 \n",
      "Loss:0.062487564980983734\n",
      "Epoch:3 \n",
      "Iteration:493 \n",
      "Loss:0.0430024228990078\n",
      "Epoch:3 \n",
      "Iteration:494 \n",
      "Loss:0.09127211570739746\n",
      "Epoch:3 \n",
      "Iteration:495 \n",
      "Loss:0.09712136536836624\n",
      "Epoch:3 \n",
      "Iteration:496 \n",
      "Loss:0.07134747505187988\n",
      "Epoch:3 \n",
      "Iteration:497 \n",
      "Loss:0.121104896068573\n",
      "Epoch:3 \n",
      "Iteration:498 \n",
      "Loss:0.147923082113266\n",
      "Epoch:3 \n",
      "Iteration:499 \n",
      "Loss:0.023370489478111267\n",
      "Epoch:3 \n",
      "Iteration:500 \n",
      "Loss:0.1316874474287033\n",
      "Epoch:3 \n",
      "Iteration:501 \n",
      "Loss:0.031174367293715477\n",
      "Epoch:3 \n",
      "Iteration:502 \n",
      "Loss:0.054284367710351944\n",
      "Epoch:3 \n",
      "Iteration:503 \n",
      "Loss:0.09452662616968155\n",
      "Epoch:3 \n",
      "Iteration:504 \n",
      "Loss:0.006037610117346048\n",
      "Epoch:3 \n",
      "Iteration:505 \n",
      "Loss:0.07172221690416336\n",
      "Epoch:3 \n",
      "Iteration:506 \n",
      "Loss:0.1037885844707489\n",
      "Epoch:3 \n",
      "Iteration:507 \n",
      "Loss:0.03325257450342178\n",
      "Epoch:3 \n",
      "Iteration:508 \n",
      "Loss:0.03974100947380066\n",
      "Epoch:3 \n",
      "Iteration:509 \n",
      "Loss:0.055190183222293854\n",
      "Epoch:3 \n",
      "Iteration:510 \n",
      "Loss:0.12394395470619202\n",
      "Epoch:3 \n",
      "Iteration:511 \n",
      "Loss:0.017157621681690216\n",
      "Epoch:3 \n",
      "Iteration:512 \n",
      "Loss:0.08471744507551193\n",
      "Epoch:3 \n",
      "Iteration:513 \n",
      "Loss:0.19270697236061096\n",
      "Epoch:3 \n",
      "Iteration:514 \n",
      "Loss:0.00915892980992794\n",
      "Epoch:3 \n",
      "Iteration:515 \n",
      "Loss:0.11456885933876038\n",
      "Epoch:3 \n",
      "Iteration:516 \n",
      "Loss:0.05020492896437645\n",
      "Epoch:3 \n",
      "Iteration:517 \n",
      "Loss:0.12951892614364624\n",
      "Epoch:3 \n",
      "Iteration:518 \n",
      "Loss:0.09133592247962952\n",
      "Epoch:3 \n",
      "Iteration:519 \n",
      "Loss:0.06451079249382019\n",
      "Epoch:3 \n",
      "Iteration:520 \n",
      "Loss:0.10322973877191544\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3 \n",
      "Iteration:521 \n",
      "Loss:0.09876592457294464\n",
      "Epoch:3 \n",
      "Iteration:522 \n",
      "Loss:0.071944959461689\n",
      "Epoch:3 \n",
      "Iteration:523 \n",
      "Loss:0.06504212319850922\n",
      "Epoch:3 \n",
      "Iteration:524 \n",
      "Loss:0.08795938640832901\n",
      "Epoch:3 \n",
      "Iteration:525 \n",
      "Loss:0.03814290463924408\n",
      "Epoch:3 \n",
      "Iteration:526 \n",
      "Loss:0.01877966709434986\n",
      "Epoch:3 \n",
      "Iteration:527 \n",
      "Loss:0.05336418002843857\n",
      "Epoch:3 \n",
      "Iteration:528 \n",
      "Loss:0.040779076516628265\n",
      "Epoch:3 \n",
      "Iteration:529 \n",
      "Loss:0.029231682419776917\n",
      "Epoch:3 \n",
      "Iteration:530 \n",
      "Loss:0.022919287905097008\n",
      "Epoch:3 \n",
      "Iteration:531 \n",
      "Loss:0.023215139284729958\n",
      "Epoch:3 \n",
      "Iteration:532 \n",
      "Loss:0.02789212204515934\n",
      "Epoch:3 \n",
      "Iteration:533 \n",
      "Loss:0.08830823749303818\n",
      "Epoch:3 \n",
      "Iteration:534 \n",
      "Loss:0.06086815893650055\n",
      "Epoch:3 \n",
      "Iteration:535 \n",
      "Loss:0.06807231158018112\n",
      "Epoch:3 \n",
      "Iteration:536 \n",
      "Loss:0.045101579278707504\n",
      "Epoch:3 \n",
      "Iteration:537 \n",
      "Loss:0.01850729063153267\n",
      "Epoch:3 \n",
      "Iteration:538 \n",
      "Loss:0.015373787842690945\n",
      "Epoch:3 \n",
      "Iteration:539 \n",
      "Loss:0.07212677597999573\n",
      "Epoch:3 \n",
      "Iteration:540 \n",
      "Loss:0.11486479640007019\n",
      "Epoch:3 \n",
      "Iteration:541 \n",
      "Loss:0.006568409036844969\n",
      "Epoch:3 \n",
      "Iteration:542 \n",
      "Loss:0.04779447987675667\n",
      "Epoch:3 \n",
      "Iteration:543 \n",
      "Loss:0.002478356007486582\n",
      "Epoch:3 \n",
      "Iteration:544 \n",
      "Loss:0.02181730978190899\n",
      "Epoch:3 \n",
      "Iteration:545 \n",
      "Loss:0.16042587161064148\n",
      "Epoch:3 \n",
      "Iteration:546 \n",
      "Loss:0.09131696075201035\n",
      "Epoch:3 \n",
      "Iteration:547 \n",
      "Loss:0.04490528255701065\n",
      "Epoch:3 \n",
      "Iteration:548 \n",
      "Loss:0.05089196562767029\n",
      "Epoch:3 \n",
      "Iteration:549 \n",
      "Loss:0.049557413905858994\n",
      "Epoch:3 \n",
      "Iteration:550 \n",
      "Loss:0.06745120882987976\n",
      "Epoch:3 \n",
      "Iteration:551 \n",
      "Loss:0.07445251196622849\n",
      "Epoch:3 \n",
      "Iteration:552 \n",
      "Loss:0.022643830627202988\n",
      "Epoch:3 \n",
      "Iteration:553 \n",
      "Loss:0.0636422410607338\n",
      "Epoch:3 \n",
      "Iteration:554 \n",
      "Loss:0.034051116555929184\n",
      "Epoch:3 \n",
      "Iteration:555 \n",
      "Loss:0.24185127019882202\n",
      "Epoch:3 \n",
      "Iteration:556 \n",
      "Loss:0.04561919346451759\n",
      "Epoch:3 \n",
      "Iteration:557 \n",
      "Loss:0.07687684148550034\n",
      "Epoch:3 \n",
      "Iteration:558 \n",
      "Loss:0.12885460257530212\n",
      "Epoch:3 \n",
      "Iteration:559 \n",
      "Loss:0.047977544367313385\n",
      "Epoch:3 \n",
      "Iteration:560 \n",
      "Loss:0.041476693004369736\n",
      "Epoch:3 \n",
      "Iteration:561 \n",
      "Loss:0.0528879277408123\n",
      "Epoch:3 \n",
      "Iteration:562 \n",
      "Loss:0.056906808167696\n",
      "Epoch:3 \n",
      "Iteration:563 \n",
      "Loss:0.04545257240533829\n",
      "Epoch:3 \n",
      "Iteration:564 \n",
      "Loss:0.14434482157230377\n",
      "Epoch:3 \n",
      "Iteration:565 \n",
      "Loss:0.09007858484983444\n",
      "Epoch:3 \n",
      "Iteration:566 \n",
      "Loss:0.059502530843019485\n",
      "Epoch:3 \n",
      "Iteration:567 \n",
      "Loss:0.04964425042271614\n",
      "Epoch:3 \n",
      "Iteration:568 \n",
      "Loss:0.1237768679857254\n",
      "Epoch:3 \n",
      "Iteration:569 \n",
      "Loss:0.08240116387605667\n",
      "Epoch:3 \n",
      "Iteration:570 \n",
      "Loss:0.06299039721488953\n",
      "Epoch:3 \n",
      "Iteration:571 \n",
      "Loss:0.07882066816091537\n",
      "Epoch:3 \n",
      "Iteration:572 \n",
      "Loss:0.09825509786605835\n",
      "Epoch:3 \n",
      "Iteration:573 \n",
      "Loss:0.08380954712629318\n",
      "Epoch:3 \n",
      "Iteration:574 \n",
      "Loss:0.10015127062797546\n",
      "Epoch:3 \n",
      "Iteration:575 \n",
      "Loss:0.051494285464286804\n",
      "Epoch:3 \n",
      "Iteration:576 \n",
      "Loss:0.10279891639947891\n",
      "Epoch:3 \n",
      "Iteration:577 \n",
      "Loss:0.09098298847675323\n",
      "Epoch:3 \n",
      "Iteration:578 \n",
      "Loss:0.0633343905210495\n",
      "Epoch:3 \n",
      "Iteration:579 \n",
      "Loss:0.08485736697912216\n",
      "Epoch:3 \n",
      "Iteration:580 \n",
      "Loss:0.08493006974458694\n",
      "Epoch:3 \n",
      "Iteration:581 \n",
      "Loss:0.03729868680238724\n",
      "Epoch:3 \n",
      "Iteration:582 \n",
      "Loss:0.1048179417848587\n",
      "Epoch:3 \n",
      "Iteration:583 \n",
      "Loss:0.10830719769001007\n",
      "Epoch:3 \n",
      "Iteration:584 \n",
      "Loss:0.01557110995054245\n",
      "Epoch:3 \n",
      "Iteration:585 \n",
      "Loss:0.018897496163845062\n",
      "Epoch:3 \n",
      "Iteration:586 \n",
      "Loss:0.10859076678752899\n",
      "Epoch:3 \n",
      "Iteration:587 \n",
      "Loss:0.061787303537130356\n",
      "Epoch:3 \n",
      "Iteration:588 \n",
      "Loss:0.0567367747426033\n",
      "Epoch:3 \n",
      "Iteration:589 \n",
      "Loss:0.02698596566915512\n",
      "Epoch:3 \n",
      "Iteration:590 \n",
      "Loss:0.05311235040426254\n",
      "Epoch:3 \n",
      "Iteration:591 \n",
      "Loss:0.05536394193768501\n",
      "Epoch:3 \n",
      "Iteration:592 \n",
      "Loss:0.09901095181703568\n",
      "Epoch:3 \n",
      "Iteration:593 \n",
      "Loss:0.045028939843177795\n",
      "Epoch:3 \n",
      "Iteration:594 \n",
      "Loss:0.15335804224014282\n",
      "Epoch:3 \n",
      "Iteration:595 \n",
      "Loss:0.04694725573062897\n",
      "Epoch:3 \n",
      "Iteration:596 \n",
      "Loss:0.04241202771663666\n",
      "Epoch:3 \n",
      "Iteration:597 \n",
      "Loss:0.078656405210495\n",
      "Epoch:3 \n",
      "Iteration:598 \n",
      "Loss:0.0703522264957428\n",
      "Epoch:3 \n",
      "Iteration:599 \n",
      "Loss:0.05391570180654526\n",
      "Epoch:3 \n",
      "Iteration:600 \n",
      "Loss:0.08491485565900803\n",
      "\n",
      "Accuracy of network in epoch 3: 97.97833333333334\n",
      "Epoch:4 \n",
      "Iteration:1 \n",
      "Loss:0.01789376325905323\n",
      "Epoch:4 \n",
      "Iteration:2 \n",
      "Loss:0.07889601588249207\n",
      "Epoch:4 \n",
      "Iteration:3 \n",
      "Loss:0.04703843966126442\n",
      "Epoch:4 \n",
      "Iteration:4 \n",
      "Loss:0.04022621735930443\n",
      "Epoch:4 \n",
      "Iteration:5 \n",
      "Loss:0.008627141825854778\n",
      "Epoch:4 \n",
      "Iteration:6 \n",
      "Loss:0.01156854722648859\n",
      "Epoch:4 \n",
      "Iteration:7 \n",
      "Loss:0.01550103910267353\n",
      "Epoch:4 \n",
      "Iteration:8 \n",
      "Loss:0.07879588752985\n",
      "Epoch:4 \n",
      "Iteration:9 \n",
      "Loss:0.008173178881406784\n",
      "Epoch:4 \n",
      "Iteration:10 \n",
      "Loss:0.059393320232629776\n",
      "Epoch:4 \n",
      "Iteration:11 \n",
      "Loss:0.022475985810160637\n",
      "Epoch:4 \n",
      "Iteration:12 \n",
      "Loss:0.03332153707742691\n",
      "Epoch:4 \n",
      "Iteration:13 \n",
      "Loss:0.015555769205093384\n",
      "Epoch:4 \n",
      "Iteration:14 \n",
      "Loss:0.0357709601521492\n",
      "Epoch:4 \n",
      "Iteration:15 \n",
      "Loss:0.06433311104774475\n",
      "Epoch:4 \n",
      "Iteration:16 \n",
      "Loss:0.016791533678770065\n",
      "Epoch:4 \n",
      "Iteration:17 \n",
      "Loss:0.04974867403507233\n",
      "Epoch:4 \n",
      "Iteration:18 \n",
      "Loss:0.005947496276348829\n",
      "Epoch:4 \n",
      "Iteration:19 \n",
      "Loss:0.05682890862226486\n",
      "Epoch:4 \n",
      "Iteration:20 \n",
      "Loss:0.06448543816804886\n",
      "Epoch:4 \n",
      "Iteration:21 \n",
      "Loss:0.04339738190174103\n",
      "Epoch:4 \n",
      "Iteration:22 \n",
      "Loss:0.07480448484420776\n",
      "Epoch:4 \n",
      "Iteration:23 \n",
      "Loss:0.023384351283311844\n",
      "Epoch:4 \n",
      "Iteration:24 \n",
      "Loss:0.10775664448738098\n",
      "Epoch:4 \n",
      "Iteration:25 \n",
      "Loss:0.02458544261753559\n",
      "Epoch:4 \n",
      "Iteration:26 \n",
      "Loss:0.07701321691274643\n",
      "Epoch:4 \n",
      "Iteration:27 \n",
      "Loss:0.12433914095163345\n",
      "Epoch:4 \n",
      "Iteration:28 \n",
      "Loss:0.031119676306843758\n",
      "Epoch:4 \n",
      "Iteration:29 \n",
      "Loss:0.016854573041200638\n",
      "Epoch:4 \n",
      "Iteration:30 \n",
      "Loss:0.003978261258453131\n",
      "Epoch:4 \n",
      "Iteration:31 \n",
      "Loss:0.0025878397282212973\n",
      "Epoch:4 \n",
      "Iteration:32 \n",
      "Loss:0.09982051700353622\n",
      "Epoch:4 \n",
      "Iteration:33 \n",
      "Loss:0.08204580098390579\n",
      "Epoch:4 \n",
      "Iteration:34 \n",
      "Loss:0.01873853988945484\n",
      "Epoch:4 \n",
      "Iteration:35 \n",
      "Loss:0.05223340913653374\n",
      "Epoch:4 \n",
      "Iteration:36 \n",
      "Loss:0.01321937795728445\n",
      "Epoch:4 \n",
      "Iteration:37 \n",
      "Loss:0.023532243445515633\n",
      "Epoch:4 \n",
      "Iteration:38 \n",
      "Loss:0.0490330308675766\n",
      "Epoch:4 \n",
      "Iteration:39 \n",
      "Loss:0.026632240042090416\n",
      "Epoch:4 \n",
      "Iteration:40 \n",
      "Loss:0.03256245702505112\n",
      "Epoch:4 \n",
      "Iteration:41 \n",
      "Loss:0.03067956492304802\n",
      "Epoch:4 \n",
      "Iteration:42 \n",
      "Loss:0.024496756494045258\n",
      "Epoch:4 \n",
      "Iteration:43 \n",
      "Loss:0.020979812368750572\n",
      "Epoch:4 \n",
      "Iteration:44 \n",
      "Loss:0.04607443884015083\n",
      "Epoch:4 \n",
      "Iteration:45 \n",
      "Loss:0.00590901542454958\n",
      "Epoch:4 \n",
      "Iteration:46 \n",
      "Loss:0.03034631535410881\n",
      "Epoch:4 \n",
      "Iteration:47 \n",
      "Loss:0.08082685619592667\n",
      "Epoch:4 \n",
      "Iteration:48 \n",
      "Loss:0.06856287270784378\n",
      "Epoch:4 \n",
      "Iteration:49 \n",
      "Loss:0.04518041014671326\n",
      "Epoch:4 \n",
      "Iteration:50 \n",
      "Loss:0.016538048163056374\n",
      "Epoch:4 \n",
      "Iteration:51 \n",
      "Loss:0.007430723402649164\n",
      "Epoch:4 \n",
      "Iteration:52 \n",
      "Loss:0.017678290605545044\n",
      "Epoch:4 \n",
      "Iteration:53 \n",
      "Loss:0.0033998119179159403\n",
      "Epoch:4 \n",
      "Iteration:54 \n",
      "Loss:0.014363058842718601\n",
      "Epoch:4 \n",
      "Iteration:55 \n",
      "Loss:0.04996103793382645\n",
      "Epoch:4 \n",
      "Iteration:56 \n",
      "Loss:0.016824737191200256\n",
      "Epoch:4 \n",
      "Iteration:57 \n",
      "Loss:0.018151648342609406\n",
      "Epoch:4 \n",
      "Iteration:58 \n",
      "Loss:0.06474575400352478\n",
      "Epoch:4 \n",
      "Iteration:59 \n",
      "Loss:0.06597334146499634\n",
      "Epoch:4 \n",
      "Iteration:60 \n",
      "Loss:0.0074410829693078995\n",
      "Epoch:4 \n",
      "Iteration:61 \n",
      "Loss:0.029403066262602806\n",
      "Epoch:4 \n",
      "Iteration:62 \n",
      "Loss:0.014681065455079079\n",
      "Epoch:4 \n",
      "Iteration:63 \n",
      "Loss:0.021315770223736763\n",
      "Epoch:4 \n",
      "Iteration:64 \n",
      "Loss:0.00859622098505497\n",
      "Epoch:4 \n",
      "Iteration:65 \n",
      "Loss:0.00688653951510787\n",
      "Epoch:4 \n",
      "Iteration:66 \n",
      "Loss:0.04730942100286484\n",
      "Epoch:4 \n",
      "Iteration:67 \n",
      "Loss:0.02298618294298649\n",
      "Epoch:4 \n",
      "Iteration:68 \n",
      "Loss:0.06538284569978714\n",
      "Epoch:4 \n",
      "Iteration:69 \n",
      "Loss:0.011603139340877533\n",
      "Epoch:4 \n",
      "Iteration:70 \n",
      "Loss:0.047224294394254684\n",
      "Epoch:4 \n",
      "Iteration:71 \n",
      "Loss:0.008952029049396515\n",
      "Epoch:4 \n",
      "Iteration:72 \n",
      "Loss:0.02915778197348118\n",
      "Epoch:4 \n",
      "Iteration:73 \n",
      "Loss:0.01331786997616291\n",
      "Epoch:4 \n",
      "Iteration:74 \n",
      "Loss:0.006045385729521513\n",
      "Epoch:4 \n",
      "Iteration:75 \n",
      "Loss:0.013076921924948692\n",
      "Epoch:4 \n",
      "Iteration:76 \n",
      "Loss:0.02799094282090664\n",
      "Epoch:4 \n",
      "Iteration:77 \n",
      "Loss:0.03331561014056206\n",
      "Epoch:4 \n",
      "Iteration:78 \n",
      "Loss:0.005030190572142601\n",
      "Epoch:4 \n",
      "Iteration:79 \n",
      "Loss:0.021165695041418076\n",
      "Epoch:4 \n",
      "Iteration:80 \n",
      "Loss:0.016653193160891533\n",
      "Epoch:4 \n",
      "Iteration:81 \n",
      "Loss:0.029071908444166183\n",
      "Epoch:4 \n",
      "Iteration:82 \n",
      "Loss:0.002818274311721325\n",
      "Epoch:4 \n",
      "Iteration:83 \n",
      "Loss:0.09432516992092133\n",
      "Epoch:4 \n",
      "Iteration:84 \n",
      "Loss:0.03774137422442436\n",
      "Epoch:4 \n",
      "Iteration:85 \n",
      "Loss:0.07432451099157333\n",
      "Epoch:4 \n",
      "Iteration:86 \n",
      "Loss:0.06524107605218887\n",
      "Epoch:4 \n",
      "Iteration:87 \n",
      "Loss:0.007082641124725342\n",
      "Epoch:4 \n",
      "Iteration:88 \n",
      "Loss:0.050045523792505264\n",
      "Epoch:4 \n",
      "Iteration:89 \n",
      "Loss:0.042525481432676315\n",
      "Epoch:4 \n",
      "Iteration:90 \n",
      "Loss:0.09866044670343399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4 \n",
      "Iteration:91 \n",
      "Loss:0.08991432934999466\n",
      "Epoch:4 \n",
      "Iteration:92 \n",
      "Loss:0.08320696651935577\n",
      "Epoch:4 \n",
      "Iteration:93 \n",
      "Loss:0.010825909674167633\n",
      "Epoch:4 \n",
      "Iteration:94 \n",
      "Loss:0.05765775218605995\n",
      "Epoch:4 \n",
      "Iteration:95 \n",
      "Loss:0.08058343082666397\n",
      "Epoch:4 \n",
      "Iteration:96 \n",
      "Loss:0.006746064405888319\n",
      "Epoch:4 \n",
      "Iteration:97 \n",
      "Loss:0.022668354213237762\n",
      "Epoch:4 \n",
      "Iteration:98 \n",
      "Loss:0.003434357000514865\n",
      "Epoch:4 \n",
      "Iteration:99 \n",
      "Loss:0.044832728803157806\n",
      "Epoch:4 \n",
      "Iteration:100 \n",
      "Loss:0.009273026138544083\n",
      "Epoch:4 \n",
      "Iteration:101 \n",
      "Loss:0.011010735295712948\n",
      "Epoch:4 \n",
      "Iteration:102 \n",
      "Loss:0.030676711350679398\n",
      "Epoch:4 \n",
      "Iteration:103 \n",
      "Loss:0.013816446997225285\n",
      "Epoch:4 \n",
      "Iteration:104 \n",
      "Loss:0.08443519473075867\n",
      "Epoch:4 \n",
      "Iteration:105 \n",
      "Loss:0.027454061433672905\n",
      "Epoch:4 \n",
      "Iteration:106 \n",
      "Loss:0.03269563615322113\n",
      "Epoch:4 \n",
      "Iteration:107 \n",
      "Loss:0.038587652146816254\n",
      "Epoch:4 \n",
      "Iteration:108 \n",
      "Loss:0.036297816783189774\n",
      "Epoch:4 \n",
      "Iteration:109 \n",
      "Loss:0.031072886660695076\n",
      "Epoch:4 \n",
      "Iteration:110 \n",
      "Loss:0.03862094134092331\n",
      "Epoch:4 \n",
      "Iteration:111 \n",
      "Loss:0.1002841368317604\n",
      "Epoch:4 \n",
      "Iteration:112 \n",
      "Loss:0.030631277710199356\n",
      "Epoch:4 \n",
      "Iteration:113 \n",
      "Loss:0.07150768488645554\n",
      "Epoch:4 \n",
      "Iteration:114 \n",
      "Loss:0.030221426859498024\n",
      "Epoch:4 \n",
      "Iteration:115 \n",
      "Loss:0.011252054944634438\n",
      "Epoch:4 \n",
      "Iteration:116 \n",
      "Loss:0.07272277772426605\n",
      "Epoch:4 \n",
      "Iteration:117 \n",
      "Loss:0.03852635994553566\n",
      "Epoch:4 \n",
      "Iteration:118 \n",
      "Loss:0.04058416187763214\n",
      "Epoch:4 \n",
      "Iteration:119 \n",
      "Loss:0.007790723815560341\n",
      "Epoch:4 \n",
      "Iteration:120 \n",
      "Loss:0.015223665162920952\n",
      "Epoch:4 \n",
      "Iteration:121 \n",
      "Loss:0.1175655946135521\n",
      "Epoch:4 \n",
      "Iteration:122 \n",
      "Loss:0.10863318294286728\n",
      "Epoch:4 \n",
      "Iteration:123 \n",
      "Loss:0.04736940562725067\n",
      "Epoch:4 \n",
      "Iteration:124 \n",
      "Loss:0.024785034358501434\n",
      "Epoch:4 \n",
      "Iteration:125 \n",
      "Loss:0.02570423111319542\n",
      "Epoch:4 \n",
      "Iteration:126 \n",
      "Loss:0.012892677448689938\n",
      "Epoch:4 \n",
      "Iteration:127 \n",
      "Loss:0.007912449538707733\n",
      "Epoch:4 \n",
      "Iteration:128 \n",
      "Loss:0.041013747453689575\n",
      "Epoch:4 \n",
      "Iteration:129 \n",
      "Loss:0.018509484827518463\n",
      "Epoch:4 \n",
      "Iteration:130 \n",
      "Loss:0.019345315173268318\n",
      "Epoch:4 \n",
      "Iteration:131 \n",
      "Loss:0.10311723500490189\n",
      "Epoch:4 \n",
      "Iteration:132 \n",
      "Loss:0.05665489658713341\n",
      "Epoch:4 \n",
      "Iteration:133 \n",
      "Loss:0.10963281989097595\n",
      "Epoch:4 \n",
      "Iteration:134 \n",
      "Loss:0.043743062764406204\n",
      "Epoch:4 \n",
      "Iteration:135 \n",
      "Loss:0.057814355939626694\n",
      "Epoch:4 \n",
      "Iteration:136 \n",
      "Loss:0.020585181191563606\n",
      "Epoch:4 \n",
      "Iteration:137 \n",
      "Loss:0.11075325310230255\n",
      "Epoch:4 \n",
      "Iteration:138 \n",
      "Loss:0.02633899636566639\n",
      "Epoch:4 \n",
      "Iteration:139 \n",
      "Loss:0.01688660867512226\n",
      "Epoch:4 \n",
      "Iteration:140 \n",
      "Loss:0.03505046293139458\n",
      "Epoch:4 \n",
      "Iteration:141 \n",
      "Loss:0.06518782675266266\n",
      "Epoch:4 \n",
      "Iteration:142 \n",
      "Loss:0.02159339375793934\n",
      "Epoch:4 \n",
      "Iteration:143 \n",
      "Loss:0.03421924263238907\n",
      "Epoch:4 \n",
      "Iteration:144 \n",
      "Loss:0.020732436329126358\n",
      "Epoch:4 \n",
      "Iteration:145 \n",
      "Loss:0.04402105510234833\n",
      "Epoch:4 \n",
      "Iteration:146 \n",
      "Loss:0.06428989768028259\n",
      "Epoch:4 \n",
      "Iteration:147 \n",
      "Loss:0.03373410552740097\n",
      "Epoch:4 \n",
      "Iteration:148 \n",
      "Loss:0.013051302172243595\n",
      "Epoch:4 \n",
      "Iteration:149 \n",
      "Loss:0.057291179895401\n",
      "Epoch:4 \n",
      "Iteration:150 \n",
      "Loss:0.02513781376183033\n",
      "Epoch:4 \n",
      "Iteration:151 \n",
      "Loss:0.02907457761466503\n",
      "Epoch:4 \n",
      "Iteration:152 \n",
      "Loss:0.045794662088155746\n",
      "Epoch:4 \n",
      "Iteration:153 \n",
      "Loss:0.07223228365182877\n",
      "Epoch:4 \n",
      "Iteration:154 \n",
      "Loss:0.09580782055854797\n",
      "Epoch:4 \n",
      "Iteration:155 \n",
      "Loss:0.024940773844718933\n",
      "Epoch:4 \n",
      "Iteration:156 \n",
      "Loss:0.03560597822070122\n",
      "Epoch:4 \n",
      "Iteration:157 \n",
      "Loss:0.02950041927397251\n",
      "Epoch:4 \n",
      "Iteration:158 \n",
      "Loss:0.22930078208446503\n",
      "Epoch:4 \n",
      "Iteration:159 \n",
      "Loss:0.021253779530525208\n",
      "Epoch:4 \n",
      "Iteration:160 \n",
      "Loss:0.0204850435256958\n",
      "Epoch:4 \n",
      "Iteration:161 \n",
      "Loss:0.025805898010730743\n",
      "Epoch:4 \n",
      "Iteration:162 \n",
      "Loss:0.012012061662971973\n",
      "Epoch:4 \n",
      "Iteration:163 \n",
      "Loss:0.04875238239765167\n",
      "Epoch:4 \n",
      "Iteration:164 \n",
      "Loss:0.03282483294606209\n",
      "Epoch:4 \n",
      "Iteration:165 \n",
      "Loss:0.06108585000038147\n",
      "Epoch:4 \n",
      "Iteration:166 \n",
      "Loss:0.09782817959785461\n",
      "Epoch:4 \n",
      "Iteration:167 \n",
      "Loss:0.0010686555178835988\n",
      "Epoch:4 \n",
      "Iteration:168 \n",
      "Loss:0.012737821787595749\n",
      "Epoch:4 \n",
      "Iteration:169 \n",
      "Loss:0.053565338253974915\n",
      "Epoch:4 \n",
      "Iteration:170 \n",
      "Loss:0.0070737628266215324\n",
      "Epoch:4 \n",
      "Iteration:171 \n",
      "Loss:0.07067570090293884\n",
      "Epoch:4 \n",
      "Iteration:172 \n",
      "Loss:0.002382984384894371\n",
      "Epoch:4 \n",
      "Iteration:173 \n",
      "Loss:0.0251668319106102\n",
      "Epoch:4 \n",
      "Iteration:174 \n",
      "Loss:0.06688268482685089\n",
      "Epoch:4 \n",
      "Iteration:175 \n",
      "Loss:0.05048993602395058\n",
      "Epoch:4 \n",
      "Iteration:176 \n",
      "Loss:0.05242740735411644\n",
      "Epoch:4 \n",
      "Iteration:177 \n",
      "Loss:0.0658479705452919\n",
      "Epoch:4 \n",
      "Iteration:178 \n",
      "Loss:0.012247789651155472\n",
      "Epoch:4 \n",
      "Iteration:179 \n",
      "Loss:0.03499563783407211\n",
      "Epoch:4 \n",
      "Iteration:180 \n",
      "Loss:0.03592647984623909\n",
      "Epoch:4 \n",
      "Iteration:181 \n",
      "Loss:0.06284985691308975\n",
      "Epoch:4 \n",
      "Iteration:182 \n",
      "Loss:0.008889662101864815\n",
      "Epoch:4 \n",
      "Iteration:183 \n",
      "Loss:0.03530336171388626\n",
      "Epoch:4 \n",
      "Iteration:184 \n",
      "Loss:0.09110040962696075\n",
      "Epoch:4 \n",
      "Iteration:185 \n",
      "Loss:0.06151720881462097\n",
      "Epoch:4 \n",
      "Iteration:186 \n",
      "Loss:0.03167538717389107\n",
      "Epoch:4 \n",
      "Iteration:187 \n",
      "Loss:0.020943166688084602\n",
      "Epoch:4 \n",
      "Iteration:188 \n",
      "Loss:0.046729739755392075\n",
      "Epoch:4 \n",
      "Iteration:189 \n",
      "Loss:0.010419253259897232\n",
      "Epoch:4 \n",
      "Iteration:190 \n",
      "Loss:0.09722647070884705\n",
      "Epoch:4 \n",
      "Iteration:191 \n",
      "Loss:0.02741861157119274\n",
      "Epoch:4 \n",
      "Iteration:192 \n",
      "Loss:0.011463813483715057\n",
      "Epoch:4 \n",
      "Iteration:193 \n",
      "Loss:0.06587100774049759\n",
      "Epoch:4 \n",
      "Iteration:194 \n",
      "Loss:0.013265818357467651\n",
      "Epoch:4 \n",
      "Iteration:195 \n",
      "Loss:0.007137294393032789\n",
      "Epoch:4 \n",
      "Iteration:196 \n",
      "Loss:0.027204925194382668\n",
      "Epoch:4 \n",
      "Iteration:197 \n",
      "Loss:0.06652376055717468\n",
      "Epoch:4 \n",
      "Iteration:198 \n",
      "Loss:0.018772903829813004\n",
      "Epoch:4 \n",
      "Iteration:199 \n",
      "Loss:0.043611861765384674\n",
      "Epoch:4 \n",
      "Iteration:200 \n",
      "Loss:0.0396699495613575\n",
      "Epoch:4 \n",
      "Iteration:201 \n",
      "Loss:0.015717720612883568\n",
      "Epoch:4 \n",
      "Iteration:202 \n",
      "Loss:0.02865874022245407\n",
      "Epoch:4 \n",
      "Iteration:203 \n",
      "Loss:0.08666635304689407\n",
      "Epoch:4 \n",
      "Iteration:204 \n",
      "Loss:0.13036249577999115\n",
      "Epoch:4 \n",
      "Iteration:205 \n",
      "Loss:0.06751979887485504\n",
      "Epoch:4 \n",
      "Iteration:206 \n",
      "Loss:0.10360526293516159\n",
      "Epoch:4 \n",
      "Iteration:207 \n",
      "Loss:0.03300411254167557\n",
      "Epoch:4 \n",
      "Iteration:208 \n",
      "Loss:0.04355408996343613\n",
      "Epoch:4 \n",
      "Iteration:209 \n",
      "Loss:0.046397749334573746\n",
      "Epoch:4 \n",
      "Iteration:210 \n",
      "Loss:0.099107526242733\n",
      "Epoch:4 \n",
      "Iteration:211 \n",
      "Loss:0.005363555159419775\n",
      "Epoch:4 \n",
      "Iteration:212 \n",
      "Loss:0.016780894249677658\n",
      "Epoch:4 \n",
      "Iteration:213 \n",
      "Loss:0.034937020391225815\n",
      "Epoch:4 \n",
      "Iteration:214 \n",
      "Loss:0.032664209604263306\n",
      "Epoch:4 \n",
      "Iteration:215 \n",
      "Loss:0.021255025640130043\n",
      "Epoch:4 \n",
      "Iteration:216 \n",
      "Loss:0.0952213853597641\n",
      "Epoch:4 \n",
      "Iteration:217 \n",
      "Loss:0.026839274913072586\n",
      "Epoch:4 \n",
      "Iteration:218 \n",
      "Loss:0.02510874904692173\n",
      "Epoch:4 \n",
      "Iteration:219 \n",
      "Loss:0.0050577991642057896\n",
      "Epoch:4 \n",
      "Iteration:220 \n",
      "Loss:0.0354459248483181\n",
      "Epoch:4 \n",
      "Iteration:221 \n",
      "Loss:0.013688416220247746\n",
      "Epoch:4 \n",
      "Iteration:222 \n",
      "Loss:0.010984021238982677\n",
      "Epoch:4 \n",
      "Iteration:223 \n",
      "Loss:0.01632738672196865\n",
      "Epoch:4 \n",
      "Iteration:224 \n",
      "Loss:0.008102284744381905\n",
      "Epoch:4 \n",
      "Iteration:225 \n",
      "Loss:0.019166121259331703\n",
      "Epoch:4 \n",
      "Iteration:226 \n",
      "Loss:0.008138826116919518\n",
      "Epoch:4 \n",
      "Iteration:227 \n",
      "Loss:0.02244025655090809\n",
      "Epoch:4 \n",
      "Iteration:228 \n",
      "Loss:0.10369366407394409\n",
      "Epoch:4 \n",
      "Iteration:229 \n",
      "Loss:0.04329647496342659\n",
      "Epoch:4 \n",
      "Iteration:230 \n",
      "Loss:0.021417617797851562\n",
      "Epoch:4 \n",
      "Iteration:231 \n",
      "Loss:0.012163382954895496\n",
      "Epoch:4 \n",
      "Iteration:232 \n",
      "Loss:0.07784514129161835\n",
      "Epoch:4 \n",
      "Iteration:233 \n",
      "Loss:0.05458538979291916\n",
      "Epoch:4 \n",
      "Iteration:234 \n",
      "Loss:0.08573481440544128\n",
      "Epoch:4 \n",
      "Iteration:235 \n",
      "Loss:0.018282504752278328\n",
      "Epoch:4 \n",
      "Iteration:236 \n",
      "Loss:0.03855879232287407\n",
      "Epoch:4 \n",
      "Iteration:237 \n",
      "Loss:0.0955142080783844\n",
      "Epoch:4 \n",
      "Iteration:238 \n",
      "Loss:0.15187755227088928\n",
      "Epoch:4 \n",
      "Iteration:239 \n",
      "Loss:0.07280685752630234\n",
      "Epoch:4 \n",
      "Iteration:240 \n",
      "Loss:0.15878306329250336\n",
      "Epoch:4 \n",
      "Iteration:241 \n",
      "Loss:0.06972454488277435\n",
      "Epoch:4 \n",
      "Iteration:242 \n",
      "Loss:0.012178049422800541\n",
      "Epoch:4 \n",
      "Iteration:243 \n",
      "Loss:0.0801473930478096\n",
      "Epoch:4 \n",
      "Iteration:244 \n",
      "Loss:0.1338726282119751\n",
      "Epoch:4 \n",
      "Iteration:245 \n",
      "Loss:0.0067077274434268475\n",
      "Epoch:4 \n",
      "Iteration:246 \n",
      "Loss:0.034120168536901474\n",
      "Epoch:4 \n",
      "Iteration:247 \n",
      "Loss:0.052360281348228455\n",
      "Epoch:4 \n",
      "Iteration:248 \n",
      "Loss:0.057258643209934235\n",
      "Epoch:4 \n",
      "Iteration:249 \n",
      "Loss:0.12406441569328308\n",
      "Epoch:4 \n",
      "Iteration:250 \n",
      "Loss:0.05287754908204079\n",
      "Epoch:4 \n",
      "Iteration:251 \n",
      "Loss:0.1877429336309433\n",
      "Epoch:4 \n",
      "Iteration:252 \n",
      "Loss:0.08802897483110428\n",
      "Epoch:4 \n",
      "Iteration:253 \n",
      "Loss:0.06309767812490463\n",
      "Epoch:4 \n",
      "Iteration:254 \n",
      "Loss:0.03678484633564949\n",
      "Epoch:4 \n",
      "Iteration:255 \n",
      "Loss:0.014342695474624634\n",
      "Epoch:4 \n",
      "Iteration:256 \n",
      "Loss:0.044328488409519196\n",
      "Epoch:4 \n",
      "Iteration:257 \n",
      "Loss:0.053095147013664246\n",
      "Epoch:4 \n",
      "Iteration:258 \n",
      "Loss:0.04106579348444939\n",
      "Epoch:4 \n",
      "Iteration:259 \n",
      "Loss:0.04265962541103363\n",
      "Epoch:4 \n",
      "Iteration:260 \n",
      "Loss:0.03779234364628792\n",
      "Epoch:4 \n",
      "Iteration:261 \n",
      "Loss:0.048096828162670135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4 \n",
      "Iteration:262 \n",
      "Loss:0.05857823044061661\n",
      "Epoch:4 \n",
      "Iteration:263 \n",
      "Loss:0.14568962156772614\n",
      "Epoch:4 \n",
      "Iteration:264 \n",
      "Loss:0.010872261598706245\n",
      "Epoch:4 \n",
      "Iteration:265 \n",
      "Loss:0.030020492151379585\n",
      "Epoch:4 \n",
      "Iteration:266 \n",
      "Loss:0.03108099475502968\n",
      "Epoch:4 \n",
      "Iteration:267 \n",
      "Loss:0.060746122151613235\n",
      "Epoch:4 \n",
      "Iteration:268 \n",
      "Loss:0.010861274786293507\n",
      "Epoch:4 \n",
      "Iteration:269 \n",
      "Loss:0.041624534875154495\n",
      "Epoch:4 \n",
      "Iteration:270 \n",
      "Loss:0.04964950680732727\n",
      "Epoch:4 \n",
      "Iteration:271 \n",
      "Loss:0.02079024724662304\n",
      "Epoch:4 \n",
      "Iteration:272 \n",
      "Loss:0.039441172033548355\n",
      "Epoch:4 \n",
      "Iteration:273 \n",
      "Loss:0.03411968797445297\n",
      "Epoch:4 \n",
      "Iteration:274 \n",
      "Loss:0.026641465723514557\n",
      "Epoch:4 \n",
      "Iteration:275 \n",
      "Loss:0.02539953961968422\n",
      "Epoch:4 \n",
      "Iteration:276 \n",
      "Loss:0.047852907329797745\n",
      "Epoch:4 \n",
      "Iteration:277 \n",
      "Loss:0.04497009143233299\n",
      "Epoch:4 \n",
      "Iteration:278 \n",
      "Loss:0.01229097880423069\n",
      "Epoch:4 \n",
      "Iteration:279 \n",
      "Loss:0.0556337833404541\n",
      "Epoch:4 \n",
      "Iteration:280 \n",
      "Loss:0.058529749512672424\n",
      "Epoch:4 \n",
      "Iteration:281 \n",
      "Loss:0.09918298572301865\n",
      "Epoch:4 \n",
      "Iteration:282 \n",
      "Loss:0.030847666785120964\n",
      "Epoch:4 \n",
      "Iteration:283 \n",
      "Loss:0.053102895617485046\n",
      "Epoch:4 \n",
      "Iteration:284 \n",
      "Loss:0.04342872276902199\n",
      "Epoch:4 \n",
      "Iteration:285 \n",
      "Loss:0.029163140803575516\n",
      "Epoch:4 \n",
      "Iteration:286 \n",
      "Loss:0.1216549426317215\n",
      "Epoch:4 \n",
      "Iteration:287 \n",
      "Loss:0.049458179622888565\n",
      "Epoch:4 \n",
      "Iteration:288 \n",
      "Loss:0.019466133788228035\n",
      "Epoch:4 \n",
      "Iteration:289 \n",
      "Loss:0.015868451446294785\n",
      "Epoch:4 \n",
      "Iteration:290 \n",
      "Loss:0.026472298428416252\n",
      "Epoch:4 \n",
      "Iteration:291 \n",
      "Loss:0.008392277173697948\n",
      "Epoch:4 \n",
      "Iteration:292 \n",
      "Loss:0.1151043251156807\n",
      "Epoch:4 \n",
      "Iteration:293 \n",
      "Loss:0.03363701328635216\n",
      "Epoch:4 \n",
      "Iteration:294 \n",
      "Loss:0.0700664296746254\n",
      "Epoch:4 \n",
      "Iteration:295 \n",
      "Loss:0.03385183960199356\n",
      "Epoch:4 \n",
      "Iteration:296 \n",
      "Loss:0.0621645413339138\n",
      "Epoch:4 \n",
      "Iteration:297 \n",
      "Loss:0.030149735510349274\n",
      "Epoch:4 \n",
      "Iteration:298 \n",
      "Loss:0.017900250852108\n",
      "Epoch:4 \n",
      "Iteration:299 \n",
      "Loss:0.018759354948997498\n",
      "Epoch:4 \n",
      "Iteration:300 \n",
      "Loss:0.01161237433552742\n",
      "Epoch:4 \n",
      "Iteration:301 \n",
      "Loss:0.05781356990337372\n",
      "Epoch:4 \n",
      "Iteration:302 \n",
      "Loss:0.13541348278522491\n",
      "Epoch:4 \n",
      "Iteration:303 \n",
      "Loss:0.03125126659870148\n",
      "Epoch:4 \n",
      "Iteration:304 \n",
      "Loss:0.04410817101597786\n",
      "Epoch:4 \n",
      "Iteration:305 \n",
      "Loss:0.01324605941772461\n",
      "Epoch:4 \n",
      "Iteration:306 \n",
      "Loss:0.046619124710559845\n",
      "Epoch:4 \n",
      "Iteration:307 \n",
      "Loss:0.01618296466767788\n",
      "Epoch:4 \n",
      "Iteration:308 \n",
      "Loss:0.05036887153983116\n",
      "Epoch:4 \n",
      "Iteration:309 \n",
      "Loss:0.046777963638305664\n",
      "Epoch:4 \n",
      "Iteration:310 \n",
      "Loss:0.05538192763924599\n",
      "Epoch:4 \n",
      "Iteration:311 \n",
      "Loss:0.009595093317329884\n",
      "Epoch:4 \n",
      "Iteration:312 \n",
      "Loss:0.021324045956134796\n",
      "Epoch:4 \n",
      "Iteration:313 \n",
      "Loss:0.057597726583480835\n",
      "Epoch:4 \n",
      "Iteration:314 \n",
      "Loss:0.007440105080604553\n",
      "Epoch:4 \n",
      "Iteration:315 \n",
      "Loss:0.032388605177402496\n",
      "Epoch:4 \n",
      "Iteration:316 \n",
      "Loss:0.029006514698266983\n",
      "Epoch:4 \n",
      "Iteration:317 \n",
      "Loss:0.059292688965797424\n",
      "Epoch:4 \n",
      "Iteration:318 \n",
      "Loss:0.0158858485519886\n",
      "Epoch:4 \n",
      "Iteration:319 \n",
      "Loss:0.009078847244381905\n",
      "Epoch:4 \n",
      "Iteration:320 \n",
      "Loss:0.12016525119543076\n",
      "Epoch:4 \n",
      "Iteration:321 \n",
      "Loss:0.09501208364963531\n",
      "Epoch:4 \n",
      "Iteration:322 \n",
      "Loss:0.16298708319664001\n",
      "Epoch:4 \n",
      "Iteration:323 \n",
      "Loss:0.013584684580564499\n",
      "Epoch:4 \n",
      "Iteration:324 \n",
      "Loss:0.034604012966156006\n",
      "Epoch:4 \n",
      "Iteration:325 \n",
      "Loss:0.026373988017439842\n",
      "Epoch:4 \n",
      "Iteration:326 \n",
      "Loss:0.025165721774101257\n",
      "Epoch:4 \n",
      "Iteration:327 \n",
      "Loss:0.014634179882705212\n",
      "Epoch:4 \n",
      "Iteration:328 \n",
      "Loss:0.05756602808833122\n",
      "Epoch:4 \n",
      "Iteration:329 \n",
      "Loss:0.031187834218144417\n",
      "Epoch:4 \n",
      "Iteration:330 \n",
      "Loss:0.11075112223625183\n",
      "Epoch:4 \n",
      "Iteration:331 \n",
      "Loss:0.1315167397260666\n",
      "Epoch:4 \n",
      "Iteration:332 \n",
      "Loss:0.008925540372729301\n",
      "Epoch:4 \n",
      "Iteration:333 \n",
      "Loss:0.034147828817367554\n",
      "Epoch:4 \n",
      "Iteration:334 \n",
      "Loss:0.038945067673921585\n",
      "Epoch:4 \n",
      "Iteration:335 \n",
      "Loss:0.08711092919111252\n",
      "Epoch:4 \n",
      "Iteration:336 \n",
      "Loss:0.05929270386695862\n",
      "Epoch:4 \n",
      "Iteration:337 \n",
      "Loss:0.05204319581389427\n",
      "Epoch:4 \n",
      "Iteration:338 \n",
      "Loss:0.03917538374662399\n",
      "Epoch:4 \n",
      "Iteration:339 \n",
      "Loss:0.039570700377225876\n",
      "Epoch:4 \n",
      "Iteration:340 \n",
      "Loss:0.06836003065109253\n",
      "Epoch:4 \n",
      "Iteration:341 \n",
      "Loss:0.08269675076007843\n",
      "Epoch:4 \n",
      "Iteration:342 \n",
      "Loss:0.010118076577782631\n",
      "Epoch:4 \n",
      "Iteration:343 \n",
      "Loss:0.07666605710983276\n",
      "Epoch:4 \n",
      "Iteration:344 \n",
      "Loss:0.008852044120430946\n",
      "Epoch:4 \n",
      "Iteration:345 \n",
      "Loss:0.07971706986427307\n",
      "Epoch:4 \n",
      "Iteration:346 \n",
      "Loss:0.07816839963197708\n",
      "Epoch:4 \n",
      "Iteration:347 \n",
      "Loss:0.054274026304483414\n",
      "Epoch:4 \n",
      "Iteration:348 \n",
      "Loss:0.034354422241449356\n",
      "Epoch:4 \n",
      "Iteration:349 \n",
      "Loss:0.049411311745643616\n",
      "Epoch:4 \n",
      "Iteration:350 \n",
      "Loss:0.09869148582220078\n",
      "Epoch:4 \n",
      "Iteration:351 \n",
      "Loss:0.05709552764892578\n",
      "Epoch:4 \n",
      "Iteration:352 \n",
      "Loss:0.08586981892585754\n",
      "Epoch:4 \n",
      "Iteration:353 \n",
      "Loss:0.028442520648241043\n",
      "Epoch:4 \n",
      "Iteration:354 \n",
      "Loss:0.019120456650853157\n",
      "Epoch:4 \n",
      "Iteration:355 \n",
      "Loss:0.07481022924184799\n",
      "Epoch:4 \n",
      "Iteration:356 \n",
      "Loss:0.10527177155017853\n",
      "Epoch:4 \n",
      "Iteration:357 \n",
      "Loss:0.0619005523622036\n",
      "Epoch:4 \n",
      "Iteration:358 \n",
      "Loss:0.01030681747943163\n",
      "Epoch:4 \n",
      "Iteration:359 \n",
      "Loss:0.03534584864974022\n",
      "Epoch:4 \n",
      "Iteration:360 \n",
      "Loss:0.028305325657129288\n",
      "Epoch:4 \n",
      "Iteration:361 \n",
      "Loss:0.021814441308379173\n",
      "Epoch:4 \n",
      "Iteration:362 \n",
      "Loss:0.0297683198004961\n",
      "Epoch:4 \n",
      "Iteration:363 \n",
      "Loss:0.08374860137701035\n",
      "Epoch:4 \n",
      "Iteration:364 \n",
      "Loss:0.048098813742399216\n",
      "Epoch:4 \n",
      "Iteration:365 \n",
      "Loss:0.023173758760094643\n",
      "Epoch:4 \n",
      "Iteration:366 \n",
      "Loss:0.12156688421964645\n",
      "Epoch:4 \n",
      "Iteration:367 \n",
      "Loss:0.04499993845820427\n",
      "Epoch:4 \n",
      "Iteration:368 \n",
      "Loss:0.030780615285038948\n",
      "Epoch:4 \n",
      "Iteration:369 \n",
      "Loss:0.07484473288059235\n",
      "Epoch:4 \n",
      "Iteration:370 \n",
      "Loss:0.10344385355710983\n",
      "Epoch:4 \n",
      "Iteration:371 \n",
      "Loss:0.0921032726764679\n",
      "Epoch:4 \n",
      "Iteration:372 \n",
      "Loss:0.0654354840517044\n",
      "Epoch:4 \n",
      "Iteration:373 \n",
      "Loss:0.05005211383104324\n",
      "Epoch:4 \n",
      "Iteration:374 \n",
      "Loss:0.02038717269897461\n",
      "Epoch:4 \n",
      "Iteration:375 \n",
      "Loss:0.04223956540226936\n",
      "Epoch:4 \n",
      "Iteration:376 \n",
      "Loss:0.08283750712871552\n",
      "Epoch:4 \n",
      "Iteration:377 \n",
      "Loss:0.0768808051943779\n",
      "Epoch:4 \n",
      "Iteration:378 \n",
      "Loss:0.0410267636179924\n",
      "Epoch:4 \n",
      "Iteration:379 \n",
      "Loss:0.04930896311998367\n",
      "Epoch:4 \n",
      "Iteration:380 \n",
      "Loss:0.02938346192240715\n",
      "Epoch:4 \n",
      "Iteration:381 \n",
      "Loss:0.07919315993785858\n",
      "Epoch:4 \n",
      "Iteration:382 \n",
      "Loss:0.04901115596294403\n",
      "Epoch:4 \n",
      "Iteration:383 \n",
      "Loss:0.06069306284189224\n",
      "Epoch:4 \n",
      "Iteration:384 \n",
      "Loss:0.0152069590985775\n",
      "Epoch:4 \n",
      "Iteration:385 \n",
      "Loss:0.027033641934394836\n",
      "Epoch:4 \n",
      "Iteration:386 \n",
      "Loss:0.014176848344504833\n",
      "Epoch:4 \n",
      "Iteration:387 \n",
      "Loss:0.031892016530036926\n",
      "Epoch:4 \n",
      "Iteration:388 \n",
      "Loss:0.02979177050292492\n",
      "Epoch:4 \n",
      "Iteration:389 \n",
      "Loss:0.027069278061389923\n",
      "Epoch:4 \n",
      "Iteration:390 \n",
      "Loss:0.016626836732029915\n",
      "Epoch:4 \n",
      "Iteration:391 \n",
      "Loss:0.09286576509475708\n",
      "Epoch:4 \n",
      "Iteration:392 \n",
      "Loss:0.13967980444431305\n",
      "Epoch:4 \n",
      "Iteration:393 \n",
      "Loss:0.036345526576042175\n",
      "Epoch:4 \n",
      "Iteration:394 \n",
      "Loss:0.011369862593710423\n",
      "Epoch:4 \n",
      "Iteration:395 \n",
      "Loss:0.05397408455610275\n",
      "Epoch:4 \n",
      "Iteration:396 \n",
      "Loss:0.016800187528133392\n",
      "Epoch:4 \n",
      "Iteration:397 \n",
      "Loss:0.019090062007308006\n",
      "Epoch:4 \n",
      "Iteration:398 \n",
      "Loss:0.061925627291202545\n",
      "Epoch:4 \n",
      "Iteration:399 \n",
      "Loss:0.02510826848447323\n",
      "Epoch:4 \n",
      "Iteration:400 \n",
      "Loss:0.1153946965932846\n",
      "Epoch:4 \n",
      "Iteration:401 \n",
      "Loss:0.030572600662708282\n",
      "Epoch:4 \n",
      "Iteration:402 \n",
      "Loss:0.004262290894985199\n",
      "Epoch:4 \n",
      "Iteration:403 \n",
      "Loss:0.00553370825946331\n",
      "Epoch:4 \n",
      "Iteration:404 \n",
      "Loss:0.018416978418827057\n",
      "Epoch:4 \n",
      "Iteration:405 \n",
      "Loss:0.03513094782829285\n",
      "Epoch:4 \n",
      "Iteration:406 \n",
      "Loss:0.03506619110703468\n",
      "Epoch:4 \n",
      "Iteration:407 \n",
      "Loss:0.012176946736872196\n",
      "Epoch:4 \n",
      "Iteration:408 \n",
      "Loss:0.022709794342517853\n",
      "Epoch:4 \n",
      "Iteration:409 \n",
      "Loss:0.08302965760231018\n",
      "Epoch:4 \n",
      "Iteration:410 \n",
      "Loss:0.04280880093574524\n",
      "Epoch:4 \n",
      "Iteration:411 \n",
      "Loss:0.0362771637737751\n",
      "Epoch:4 \n",
      "Iteration:412 \n",
      "Loss:0.08058687299489975\n",
      "Epoch:4 \n",
      "Iteration:413 \n",
      "Loss:0.03942721709609032\n",
      "Epoch:4 \n",
      "Iteration:414 \n",
      "Loss:0.013724220916628838\n",
      "Epoch:4 \n",
      "Iteration:415 \n",
      "Loss:0.006410585716366768\n",
      "Epoch:4 \n",
      "Iteration:416 \n",
      "Loss:0.012704133987426758\n",
      "Epoch:4 \n",
      "Iteration:417 \n",
      "Loss:0.0651843249797821\n",
      "Epoch:4 \n",
      "Iteration:418 \n",
      "Loss:0.0464077852666378\n",
      "Epoch:4 \n",
      "Iteration:419 \n",
      "Loss:0.023719336837530136\n",
      "Epoch:4 \n",
      "Iteration:420 \n",
      "Loss:0.06330014020204544\n",
      "Epoch:4 \n",
      "Iteration:421 \n",
      "Loss:0.025844551622867584\n",
      "Epoch:4 \n",
      "Iteration:422 \n",
      "Loss:0.05381614714860916\n",
      "Epoch:4 \n",
      "Iteration:423 \n",
      "Loss:0.007979905232787132\n",
      "Epoch:4 \n",
      "Iteration:424 \n",
      "Loss:0.0760030746459961\n",
      "Epoch:4 \n",
      "Iteration:425 \n",
      "Loss:0.04318208247423172\n",
      "Epoch:4 \n",
      "Iteration:426 \n",
      "Loss:0.05404441058635712\n",
      "Epoch:4 \n",
      "Iteration:427 \n",
      "Loss:0.04757403954863548\n",
      "Epoch:4 \n",
      "Iteration:428 \n",
      "Loss:0.06503885984420776\n",
      "Epoch:4 \n",
      "Iteration:429 \n",
      "Loss:0.06557431071996689\n",
      "Epoch:4 \n",
      "Iteration:430 \n",
      "Loss:0.04281027242541313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4 \n",
      "Iteration:431 \n",
      "Loss:0.14199618995189667\n",
      "Epoch:4 \n",
      "Iteration:432 \n",
      "Loss:0.06995493173599243\n",
      "Epoch:4 \n",
      "Iteration:433 \n",
      "Loss:0.011810476891696453\n",
      "Epoch:4 \n",
      "Iteration:434 \n",
      "Loss:0.009954454377293587\n",
      "Epoch:4 \n",
      "Iteration:435 \n",
      "Loss:0.08230546861886978\n",
      "Epoch:4 \n",
      "Iteration:436 \n",
      "Loss:0.07516516745090485\n",
      "Epoch:4 \n",
      "Iteration:437 \n",
      "Loss:0.04371054098010063\n",
      "Epoch:4 \n",
      "Iteration:438 \n",
      "Loss:0.053136616945266724\n",
      "Epoch:4 \n",
      "Iteration:439 \n",
      "Loss:0.0999719649553299\n",
      "Epoch:4 \n",
      "Iteration:440 \n",
      "Loss:0.039290495216846466\n",
      "Epoch:4 \n",
      "Iteration:441 \n",
      "Loss:0.08188962936401367\n",
      "Epoch:4 \n",
      "Iteration:442 \n",
      "Loss:0.02714819833636284\n",
      "Epoch:4 \n",
      "Iteration:443 \n",
      "Loss:0.07733628898859024\n",
      "Epoch:4 \n",
      "Iteration:444 \n",
      "Loss:0.1293228417634964\n",
      "Epoch:4 \n",
      "Iteration:445 \n",
      "Loss:0.03363217040896416\n",
      "Epoch:4 \n",
      "Iteration:446 \n",
      "Loss:0.0377047173678875\n",
      "Epoch:4 \n",
      "Iteration:447 \n",
      "Loss:0.039709605276584625\n",
      "Epoch:4 \n",
      "Iteration:448 \n",
      "Loss:0.01902863010764122\n",
      "Epoch:4 \n",
      "Iteration:449 \n",
      "Loss:0.09580697864294052\n",
      "Epoch:4 \n",
      "Iteration:450 \n",
      "Loss:0.15124060213565826\n",
      "Epoch:4 \n",
      "Iteration:451 \n",
      "Loss:0.005545345135033131\n",
      "Epoch:4 \n",
      "Iteration:452 \n",
      "Loss:0.11923860758543015\n",
      "Epoch:4 \n",
      "Iteration:453 \n",
      "Loss:0.006934755481779575\n",
      "Epoch:4 \n",
      "Iteration:454 \n",
      "Loss:0.07091594487428665\n",
      "Epoch:4 \n",
      "Iteration:455 \n",
      "Loss:0.06721224635839462\n",
      "Epoch:4 \n",
      "Iteration:456 \n",
      "Loss:0.052284542471170425\n",
      "Epoch:4 \n",
      "Iteration:457 \n",
      "Loss:0.032030023634433746\n",
      "Epoch:4 \n",
      "Iteration:458 \n",
      "Loss:0.03915654122829437\n",
      "Epoch:4 \n",
      "Iteration:459 \n",
      "Loss:0.05819541960954666\n",
      "Epoch:4 \n",
      "Iteration:460 \n",
      "Loss:0.12053148448467255\n",
      "Epoch:4 \n",
      "Iteration:461 \n",
      "Loss:0.04694046452641487\n",
      "Epoch:4 \n",
      "Iteration:462 \n",
      "Loss:0.05128633975982666\n",
      "Epoch:4 \n",
      "Iteration:463 \n",
      "Loss:0.042960718274116516\n",
      "Epoch:4 \n",
      "Iteration:464 \n",
      "Loss:0.08014263212680817\n",
      "Epoch:4 \n",
      "Iteration:465 \n",
      "Loss:0.1014503762125969\n",
      "Epoch:4 \n",
      "Iteration:466 \n",
      "Loss:0.04885342717170715\n",
      "Epoch:4 \n",
      "Iteration:467 \n",
      "Loss:0.044103365391492844\n",
      "Epoch:4 \n",
      "Iteration:468 \n",
      "Loss:0.00812288373708725\n",
      "Epoch:4 \n",
      "Iteration:469 \n",
      "Loss:0.01095378864556551\n",
      "Epoch:4 \n",
      "Iteration:470 \n",
      "Loss:0.018764303997159004\n",
      "Epoch:4 \n",
      "Iteration:471 \n",
      "Loss:0.12191998213529587\n",
      "Epoch:4 \n",
      "Iteration:472 \n",
      "Loss:0.017470842227339745\n",
      "Epoch:4 \n",
      "Iteration:473 \n",
      "Loss:0.05841919034719467\n",
      "Epoch:4 \n",
      "Iteration:474 \n",
      "Loss:0.07395163178443909\n",
      "Epoch:4 \n",
      "Iteration:475 \n",
      "Loss:0.03661569952964783\n",
      "Epoch:4 \n",
      "Iteration:476 \n",
      "Loss:0.1069280207157135\n",
      "Epoch:4 \n",
      "Iteration:477 \n",
      "Loss:0.020640652626752853\n",
      "Epoch:4 \n",
      "Iteration:478 \n",
      "Loss:0.051563773304224014\n",
      "Epoch:4 \n",
      "Iteration:479 \n",
      "Loss:0.03260396420955658\n",
      "Epoch:4 \n",
      "Iteration:480 \n",
      "Loss:0.04502737149596214\n",
      "Epoch:4 \n",
      "Iteration:481 \n",
      "Loss:0.01069057360291481\n",
      "Epoch:4 \n",
      "Iteration:482 \n",
      "Loss:0.04583257809281349\n",
      "Epoch:4 \n",
      "Iteration:483 \n",
      "Loss:0.03359924629330635\n",
      "Epoch:4 \n",
      "Iteration:484 \n",
      "Loss:0.07638128101825714\n",
      "Epoch:4 \n",
      "Iteration:485 \n",
      "Loss:0.11755654215812683\n",
      "Epoch:4 \n",
      "Iteration:486 \n",
      "Loss:0.014886762946844101\n",
      "Epoch:4 \n",
      "Iteration:487 \n",
      "Loss:0.02165236882865429\n",
      "Epoch:4 \n",
      "Iteration:488 \n",
      "Loss:0.07066107541322708\n",
      "Epoch:4 \n",
      "Iteration:489 \n",
      "Loss:0.01928560808300972\n",
      "Epoch:4 \n",
      "Iteration:490 \n",
      "Loss:0.07626079767942429\n",
      "Epoch:4 \n",
      "Iteration:491 \n",
      "Loss:0.0028982048388570547\n",
      "Epoch:4 \n",
      "Iteration:492 \n",
      "Loss:0.031610194593667984\n",
      "Epoch:4 \n",
      "Iteration:493 \n",
      "Loss:0.02797190472483635\n",
      "Epoch:4 \n",
      "Iteration:494 \n",
      "Loss:0.044019054621458054\n",
      "Epoch:4 \n",
      "Iteration:495 \n",
      "Loss:0.07471911609172821\n",
      "Epoch:4 \n",
      "Iteration:496 \n",
      "Loss:0.1199987605214119\n",
      "Epoch:4 \n",
      "Iteration:497 \n",
      "Loss:0.05997126176953316\n",
      "Epoch:4 \n",
      "Iteration:498 \n",
      "Loss:0.11776549369096756\n",
      "Epoch:4 \n",
      "Iteration:499 \n",
      "Loss:0.015714412555098534\n",
      "Epoch:4 \n",
      "Iteration:500 \n",
      "Loss:0.01498736534267664\n",
      "Epoch:4 \n",
      "Iteration:501 \n",
      "Loss:0.005744211841374636\n",
      "Epoch:4 \n",
      "Iteration:502 \n",
      "Loss:0.10182604193687439\n",
      "Epoch:4 \n",
      "Iteration:503 \n",
      "Loss:0.011080545373260975\n",
      "Epoch:4 \n",
      "Iteration:504 \n",
      "Loss:0.032462649047374725\n",
      "Epoch:4 \n",
      "Iteration:505 \n",
      "Loss:0.03780407831072807\n",
      "Epoch:4 \n",
      "Iteration:506 \n",
      "Loss:0.015614187344908714\n",
      "Epoch:4 \n",
      "Iteration:507 \n",
      "Loss:0.06760770082473755\n",
      "Epoch:4 \n",
      "Iteration:508 \n",
      "Loss:0.016738038510084152\n",
      "Epoch:4 \n",
      "Iteration:509 \n",
      "Loss:0.01658971793949604\n",
      "Epoch:4 \n",
      "Iteration:510 \n",
      "Loss:0.024305012077093124\n",
      "Epoch:4 \n",
      "Iteration:511 \n",
      "Loss:0.04759565740823746\n",
      "Epoch:4 \n",
      "Iteration:512 \n",
      "Loss:0.06693946570158005\n",
      "Epoch:4 \n",
      "Iteration:513 \n",
      "Loss:0.022829845547676086\n",
      "Epoch:4 \n",
      "Iteration:514 \n",
      "Loss:0.06104227527976036\n",
      "Epoch:4 \n",
      "Iteration:515 \n",
      "Loss:0.0050639077089726925\n",
      "Epoch:4 \n",
      "Iteration:516 \n",
      "Loss:0.024346036836504936\n",
      "Epoch:4 \n",
      "Iteration:517 \n",
      "Loss:0.10049983859062195\n",
      "Epoch:4 \n",
      "Iteration:518 \n",
      "Loss:0.09313666075468063\n",
      "Epoch:4 \n",
      "Iteration:519 \n",
      "Loss:0.030021915212273598\n",
      "Epoch:4 \n",
      "Iteration:520 \n",
      "Loss:0.04179150238633156\n",
      "Epoch:4 \n",
      "Iteration:521 \n",
      "Loss:0.02456755004823208\n",
      "Epoch:4 \n",
      "Iteration:522 \n",
      "Loss:0.03723936900496483\n",
      "Epoch:4 \n",
      "Iteration:523 \n",
      "Loss:0.07391918450593948\n",
      "Epoch:4 \n",
      "Iteration:524 \n",
      "Loss:0.020798394456505775\n",
      "Epoch:4 \n",
      "Iteration:525 \n",
      "Loss:0.05160648748278618\n",
      "Epoch:4 \n",
      "Iteration:526 \n",
      "Loss:0.03905922919511795\n",
      "Epoch:4 \n",
      "Iteration:527 \n",
      "Loss:0.06144167110323906\n",
      "Epoch:4 \n",
      "Iteration:528 \n",
      "Loss:0.09771379828453064\n",
      "Epoch:4 \n",
      "Iteration:529 \n",
      "Loss:0.027142181992530823\n",
      "Epoch:4 \n",
      "Iteration:530 \n",
      "Loss:0.13097403943538666\n",
      "Epoch:4 \n",
      "Iteration:531 \n",
      "Loss:0.010433226823806763\n",
      "Epoch:4 \n",
      "Iteration:532 \n",
      "Loss:0.016229771077632904\n",
      "Epoch:4 \n",
      "Iteration:533 \n",
      "Loss:0.03643365204334259\n",
      "Epoch:4 \n",
      "Iteration:534 \n",
      "Loss:0.023893823847174644\n",
      "Epoch:4 \n",
      "Iteration:535 \n",
      "Loss:0.006459407042711973\n",
      "Epoch:4 \n",
      "Iteration:536 \n",
      "Loss:0.05996400862932205\n",
      "Epoch:4 \n",
      "Iteration:537 \n",
      "Loss:0.05098329856991768\n",
      "Epoch:4 \n",
      "Iteration:538 \n",
      "Loss:0.06003999710083008\n",
      "Epoch:4 \n",
      "Iteration:539 \n",
      "Loss:0.06455986201763153\n",
      "Epoch:4 \n",
      "Iteration:540 \n",
      "Loss:0.0018409814219921827\n",
      "Epoch:4 \n",
      "Iteration:541 \n",
      "Loss:0.03902461379766464\n",
      "Epoch:4 \n",
      "Iteration:542 \n",
      "Loss:0.04378477483987808\n",
      "Epoch:4 \n",
      "Iteration:543 \n",
      "Loss:0.0061819530092179775\n",
      "Epoch:4 \n",
      "Iteration:544 \n",
      "Loss:0.047182999551296234\n",
      "Epoch:4 \n",
      "Iteration:545 \n",
      "Loss:0.05746840313076973\n",
      "Epoch:4 \n",
      "Iteration:546 \n",
      "Loss:0.01906627230346203\n",
      "Epoch:4 \n",
      "Iteration:547 \n",
      "Loss:0.20936216413974762\n",
      "Epoch:4 \n",
      "Iteration:548 \n",
      "Loss:0.0839746966958046\n",
      "Epoch:4 \n",
      "Iteration:549 \n",
      "Loss:0.024327969178557396\n",
      "Epoch:4 \n",
      "Iteration:550 \n",
      "Loss:0.042461663484573364\n",
      "Epoch:4 \n",
      "Iteration:551 \n",
      "Loss:0.01974553056061268\n",
      "Epoch:4 \n",
      "Iteration:552 \n",
      "Loss:0.05275796353816986\n",
      "Epoch:4 \n",
      "Iteration:553 \n",
      "Loss:0.11748737096786499\n",
      "Epoch:4 \n",
      "Iteration:554 \n",
      "Loss:0.17028820514678955\n",
      "Epoch:4 \n",
      "Iteration:555 \n",
      "Loss:0.012800345197319984\n",
      "Epoch:4 \n",
      "Iteration:556 \n",
      "Loss:0.17666266858577728\n",
      "Epoch:4 \n",
      "Iteration:557 \n",
      "Loss:0.028835639357566833\n",
      "Epoch:4 \n",
      "Iteration:558 \n",
      "Loss:0.04012872651219368\n",
      "Epoch:4 \n",
      "Iteration:559 \n",
      "Loss:0.04176301136612892\n",
      "Epoch:4 \n",
      "Iteration:560 \n",
      "Loss:0.07548405975103378\n",
      "Epoch:4 \n",
      "Iteration:561 \n",
      "Loss:0.0319393165409565\n",
      "Epoch:4 \n",
      "Iteration:562 \n",
      "Loss:0.058027878403663635\n",
      "Epoch:4 \n",
      "Iteration:563 \n",
      "Loss:0.08902982622385025\n",
      "Epoch:4 \n",
      "Iteration:564 \n",
      "Loss:0.04373681917786598\n",
      "Epoch:4 \n",
      "Iteration:565 \n",
      "Loss:0.03629143163561821\n",
      "Epoch:4 \n",
      "Iteration:566 \n",
      "Loss:0.035276684910058975\n",
      "Epoch:4 \n",
      "Iteration:567 \n",
      "Loss:0.07741788774728775\n",
      "Epoch:4 \n",
      "Iteration:568 \n",
      "Loss:0.03359391540288925\n",
      "Epoch:4 \n",
      "Iteration:569 \n",
      "Loss:0.1394750475883484\n",
      "Epoch:4 \n",
      "Iteration:570 \n",
      "Loss:0.10001648217439651\n",
      "Epoch:4 \n",
      "Iteration:571 \n",
      "Loss:0.01608988642692566\n",
      "Epoch:4 \n",
      "Iteration:572 \n",
      "Loss:0.05794857069849968\n",
      "Epoch:4 \n",
      "Iteration:573 \n",
      "Loss:0.01524822972714901\n",
      "Epoch:4 \n",
      "Iteration:574 \n",
      "Loss:0.1591728925704956\n",
      "Epoch:4 \n",
      "Iteration:575 \n",
      "Loss:0.0415838398039341\n",
      "Epoch:4 \n",
      "Iteration:576 \n",
      "Loss:0.05721773952245712\n",
      "Epoch:4 \n",
      "Iteration:577 \n",
      "Loss:0.1552903801202774\n",
      "Epoch:4 \n",
      "Iteration:578 \n",
      "Loss:0.007739701773971319\n",
      "Epoch:4 \n",
      "Iteration:579 \n",
      "Loss:0.029678814113140106\n",
      "Epoch:4 \n",
      "Iteration:580 \n",
      "Loss:0.0794377475976944\n",
      "Epoch:4 \n",
      "Iteration:581 \n",
      "Loss:0.020843911916017532\n",
      "Epoch:4 \n",
      "Iteration:582 \n",
      "Loss:0.07401937991380692\n",
      "Epoch:4 \n",
      "Iteration:583 \n",
      "Loss:0.06163157522678375\n",
      "Epoch:4 \n",
      "Iteration:584 \n",
      "Loss:0.07000530511140823\n",
      "Epoch:4 \n",
      "Iteration:585 \n",
      "Loss:0.030937686562538147\n",
      "Epoch:4 \n",
      "Iteration:586 \n",
      "Loss:0.0781107246875763\n",
      "Epoch:4 \n",
      "Iteration:587 \n",
      "Loss:0.03676345571875572\n",
      "Epoch:4 \n",
      "Iteration:588 \n",
      "Loss:0.008249121718108654\n",
      "Epoch:4 \n",
      "Iteration:589 \n",
      "Loss:0.028176402673125267\n",
      "Epoch:4 \n",
      "Iteration:590 \n",
      "Loss:0.007778591010719538\n",
      "Epoch:4 \n",
      "Iteration:591 \n",
      "Loss:0.09243366122245789\n",
      "Epoch:4 \n",
      "Iteration:592 \n",
      "Loss:0.06417675316333771\n",
      "Epoch:4 \n",
      "Iteration:593 \n",
      "Loss:0.0426030196249485\n",
      "Epoch:4 \n",
      "Iteration:594 \n",
      "Loss:0.12897726893424988\n",
      "Epoch:4 \n",
      "Iteration:595 \n",
      "Loss:0.03154204413294792\n",
      "Epoch:4 \n",
      "Iteration:596 \n",
      "Loss:0.07647789269685745\n",
      "Epoch:4 \n",
      "Iteration:597 \n",
      "Loss:0.006134346127510071\n",
      "Epoch:4 \n",
      "Iteration:598 \n",
      "Loss:0.028373437002301216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4 \n",
      "Iteration:599 \n",
      "Loss:0.028972206637263298\n",
      "Epoch:4 \n",
      "Iteration:600 \n",
      "Loss:0.17726197838783264\n",
      "\n",
      "Accuracy of network in epoch 4: 98.48833333333333\n",
      "Epoch:5 \n",
      "Iteration:1 \n",
      "Loss:0.008354908786714077\n",
      "Epoch:5 \n",
      "Iteration:2 \n",
      "Loss:0.0021204808726906776\n",
      "Epoch:5 \n",
      "Iteration:3 \n",
      "Loss:0.009838227182626724\n",
      "Epoch:5 \n",
      "Iteration:4 \n",
      "Loss:0.04806382209062576\n",
      "Epoch:5 \n",
      "Iteration:5 \n",
      "Loss:0.016117582097649574\n",
      "Epoch:5 \n",
      "Iteration:6 \n",
      "Loss:0.014580371789634228\n",
      "Epoch:5 \n",
      "Iteration:7 \n",
      "Loss:0.014269322156906128\n",
      "Epoch:5 \n",
      "Iteration:8 \n",
      "Loss:0.019681314006447792\n",
      "Epoch:5 \n",
      "Iteration:9 \n",
      "Loss:0.027688927948474884\n",
      "Epoch:5 \n",
      "Iteration:10 \n",
      "Loss:0.060438722372055054\n",
      "Epoch:5 \n",
      "Iteration:11 \n",
      "Loss:0.0070455027744174\n",
      "Epoch:5 \n",
      "Iteration:12 \n",
      "Loss:0.003211061004549265\n",
      "Epoch:5 \n",
      "Iteration:13 \n",
      "Loss:0.009114285930991173\n",
      "Epoch:5 \n",
      "Iteration:14 \n",
      "Loss:0.17354173958301544\n",
      "Epoch:5 \n",
      "Iteration:15 \n",
      "Loss:0.016876043751835823\n",
      "Epoch:5 \n",
      "Iteration:16 \n",
      "Loss:0.013226073235273361\n",
      "Epoch:5 \n",
      "Iteration:17 \n",
      "Loss:0.0037711127661168575\n",
      "Epoch:5 \n",
      "Iteration:18 \n",
      "Loss:0.02288839779794216\n",
      "Epoch:5 \n",
      "Iteration:19 \n",
      "Loss:0.0270115714520216\n",
      "Epoch:5 \n",
      "Iteration:20 \n",
      "Loss:0.09235724806785583\n",
      "Epoch:5 \n",
      "Iteration:21 \n",
      "Loss:0.013838011771440506\n",
      "Epoch:5 \n",
      "Iteration:22 \n",
      "Loss:0.008335795253515244\n",
      "Epoch:5 \n",
      "Iteration:23 \n",
      "Loss:0.021612191572785378\n",
      "Epoch:5 \n",
      "Iteration:24 \n",
      "Loss:0.029852796345949173\n",
      "Epoch:5 \n",
      "Iteration:25 \n",
      "Loss:0.008187114261090755\n",
      "Epoch:5 \n",
      "Iteration:26 \n",
      "Loss:0.059249456971883774\n",
      "Epoch:5 \n",
      "Iteration:27 \n",
      "Loss:0.04806073009967804\n",
      "Epoch:5 \n",
      "Iteration:28 \n",
      "Loss:0.0015549063682556152\n",
      "Epoch:5 \n",
      "Iteration:29 \n",
      "Loss:0.03204211965203285\n",
      "Epoch:5 \n",
      "Iteration:30 \n",
      "Loss:0.00381019851192832\n",
      "Epoch:5 \n",
      "Iteration:31 \n",
      "Loss:0.024857446551322937\n",
      "Epoch:5 \n",
      "Iteration:32 \n",
      "Loss:0.09588378667831421\n",
      "Epoch:5 \n",
      "Iteration:33 \n",
      "Loss:0.023676997050642967\n",
      "Epoch:5 \n",
      "Iteration:34 \n",
      "Loss:0.005142894573509693\n",
      "Epoch:5 \n",
      "Iteration:35 \n",
      "Loss:0.02087865024805069\n",
      "Epoch:5 \n",
      "Iteration:36 \n",
      "Loss:0.0809541866183281\n",
      "Epoch:5 \n",
      "Iteration:37 \n",
      "Loss:0.005338664632290602\n",
      "Epoch:5 \n",
      "Iteration:38 \n",
      "Loss:0.06231238320469856\n",
      "Epoch:5 \n",
      "Iteration:39 \n",
      "Loss:0.004364416468888521\n",
      "Epoch:5 \n",
      "Iteration:40 \n",
      "Loss:0.006477198097854853\n",
      "Epoch:5 \n",
      "Iteration:41 \n",
      "Loss:0.023357994854450226\n",
      "Epoch:5 \n",
      "Iteration:42 \n",
      "Loss:0.020256878808140755\n",
      "Epoch:5 \n",
      "Iteration:43 \n",
      "Loss:0.021849319338798523\n",
      "Epoch:5 \n",
      "Iteration:44 \n",
      "Loss:0.03112613968551159\n",
      "Epoch:5 \n",
      "Iteration:45 \n",
      "Loss:0.0739545151591301\n",
      "Epoch:5 \n",
      "Iteration:46 \n",
      "Loss:0.04631819203495979\n",
      "Epoch:5 \n",
      "Iteration:47 \n",
      "Loss:0.0501219742000103\n",
      "Epoch:5 \n",
      "Iteration:48 \n",
      "Loss:0.036879584193229675\n",
      "Epoch:5 \n",
      "Iteration:49 \n",
      "Loss:0.041280701756477356\n",
      "Epoch:5 \n",
      "Iteration:50 \n",
      "Loss:0.0423673540353775\n",
      "Epoch:5 \n",
      "Iteration:51 \n",
      "Loss:0.09156283736228943\n",
      "Epoch:5 \n",
      "Iteration:52 \n",
      "Loss:0.021895036101341248\n",
      "Epoch:5 \n",
      "Iteration:53 \n",
      "Loss:0.0639810636639595\n",
      "Epoch:5 \n",
      "Iteration:54 \n",
      "Loss:0.05017377808690071\n",
      "Epoch:5 \n",
      "Iteration:55 \n",
      "Loss:0.027040056884288788\n",
      "Epoch:5 \n",
      "Iteration:56 \n",
      "Loss:0.05508287996053696\n",
      "Epoch:5 \n",
      "Iteration:57 \n",
      "Loss:0.02778727561235428\n",
      "Epoch:5 \n",
      "Iteration:58 \n",
      "Loss:0.03794340789318085\n",
      "Epoch:5 \n",
      "Iteration:59 \n",
      "Loss:0.024515200406312943\n",
      "Epoch:5 \n",
      "Iteration:60 \n",
      "Loss:0.01581365242600441\n",
      "Epoch:5 \n",
      "Iteration:61 \n",
      "Loss:0.04929263889789581\n",
      "Epoch:5 \n",
      "Iteration:62 \n",
      "Loss:0.018748585134744644\n",
      "Epoch:5 \n",
      "Iteration:63 \n",
      "Loss:0.017937609925866127\n",
      "Epoch:5 \n",
      "Iteration:64 \n",
      "Loss:0.044574931263923645\n",
      "Epoch:5 \n",
      "Iteration:65 \n",
      "Loss:0.05748237669467926\n",
      "Epoch:5 \n",
      "Iteration:66 \n",
      "Loss:0.022236211225390434\n",
      "Epoch:5 \n",
      "Iteration:67 \n",
      "Loss:0.006080453284084797\n",
      "Epoch:5 \n",
      "Iteration:68 \n",
      "Loss:0.025870464742183685\n",
      "Epoch:5 \n",
      "Iteration:69 \n",
      "Loss:0.003083490766584873\n",
      "Epoch:5 \n",
      "Iteration:70 \n",
      "Loss:0.07590921223163605\n",
      "Epoch:5 \n",
      "Iteration:71 \n",
      "Loss:0.033865828067064285\n",
      "Epoch:5 \n",
      "Iteration:72 \n",
      "Loss:0.015424821525812149\n",
      "Epoch:5 \n",
      "Iteration:73 \n",
      "Loss:0.014791633002460003\n",
      "Epoch:5 \n",
      "Iteration:74 \n",
      "Loss:0.0077199786901474\n",
      "Epoch:5 \n",
      "Iteration:75 \n",
      "Loss:0.012633204460144043\n",
      "Epoch:5 \n",
      "Iteration:76 \n",
      "Loss:0.022363949567079544\n",
      "Epoch:5 \n",
      "Iteration:77 \n",
      "Loss:0.020985035225749016\n",
      "Epoch:5 \n",
      "Iteration:78 \n",
      "Loss:0.0016782167367637157\n",
      "Epoch:5 \n",
      "Iteration:79 \n",
      "Loss:0.08685922622680664\n",
      "Epoch:5 \n",
      "Iteration:80 \n",
      "Loss:0.07231438905000687\n",
      "Epoch:5 \n",
      "Iteration:81 \n",
      "Loss:0.041871003806591034\n",
      "Epoch:5 \n",
      "Iteration:82 \n",
      "Loss:0.00772493053227663\n",
      "Epoch:5 \n",
      "Iteration:83 \n",
      "Loss:0.056029416620731354\n",
      "Epoch:5 \n",
      "Iteration:84 \n",
      "Loss:0.005039834417402744\n",
      "Epoch:5 \n",
      "Iteration:85 \n",
      "Loss:0.0009520672028884292\n",
      "Epoch:5 \n",
      "Iteration:86 \n",
      "Loss:0.0017091198824346066\n",
      "Epoch:5 \n",
      "Iteration:87 \n",
      "Loss:0.02866588905453682\n",
      "Epoch:5 \n",
      "Iteration:88 \n",
      "Loss:0.0071078138425946236\n",
      "Epoch:5 \n",
      "Iteration:89 \n",
      "Loss:0.06789544224739075\n",
      "Epoch:5 \n",
      "Iteration:90 \n",
      "Loss:0.0069314269348979\n",
      "Epoch:5 \n",
      "Iteration:91 \n",
      "Loss:0.017479751259088516\n",
      "Epoch:5 \n",
      "Iteration:92 \n",
      "Loss:0.011844546534121037\n",
      "Epoch:5 \n",
      "Iteration:93 \n",
      "Loss:0.11414621025323868\n",
      "Epoch:5 \n",
      "Iteration:94 \n",
      "Loss:0.11900026351213455\n",
      "Epoch:5 \n",
      "Iteration:95 \n",
      "Loss:0.0513598769903183\n",
      "Epoch:5 \n",
      "Iteration:96 \n",
      "Loss:0.02156989835202694\n",
      "Epoch:5 \n",
      "Iteration:97 \n",
      "Loss:0.10540921241044998\n",
      "Epoch:5 \n",
      "Iteration:98 \n",
      "Loss:0.004759255796670914\n",
      "Epoch:5 \n",
      "Iteration:99 \n",
      "Loss:0.0017172759398818016\n",
      "Epoch:5 \n",
      "Iteration:100 \n",
      "Loss:0.018207835033535957\n",
      "Epoch:5 \n",
      "Iteration:101 \n",
      "Loss:0.05174718797206879\n",
      "Epoch:5 \n",
      "Iteration:102 \n",
      "Loss:0.0614677295088768\n",
      "Epoch:5 \n",
      "Iteration:103 \n",
      "Loss:0.11644528061151505\n",
      "Epoch:5 \n",
      "Iteration:104 \n",
      "Loss:0.062433015555143356\n",
      "Epoch:5 \n",
      "Iteration:105 \n",
      "Loss:0.008429351262748241\n",
      "Epoch:5 \n",
      "Iteration:106 \n",
      "Loss:0.01928906887769699\n",
      "Epoch:5 \n",
      "Iteration:107 \n",
      "Loss:0.015428189188241959\n",
      "Epoch:5 \n",
      "Iteration:108 \n",
      "Loss:0.029354562982916832\n",
      "Epoch:5 \n",
      "Iteration:109 \n",
      "Loss:0.025937790051102638\n",
      "Epoch:5 \n",
      "Iteration:110 \n",
      "Loss:0.056165728718042374\n",
      "Epoch:5 \n",
      "Iteration:111 \n",
      "Loss:0.05902191251516342\n",
      "Epoch:5 \n",
      "Iteration:112 \n",
      "Loss:0.008383464068174362\n",
      "Epoch:5 \n",
      "Iteration:113 \n",
      "Loss:0.007031195797026157\n",
      "Epoch:5 \n",
      "Iteration:114 \n",
      "Loss:0.029090743511915207\n",
      "Epoch:5 \n",
      "Iteration:115 \n",
      "Loss:0.09299366176128387\n",
      "Epoch:5 \n",
      "Iteration:116 \n",
      "Loss:0.021445002406835556\n",
      "Epoch:5 \n",
      "Iteration:117 \n",
      "Loss:0.011151851154863834\n",
      "Epoch:5 \n",
      "Iteration:118 \n",
      "Loss:0.049888186156749725\n",
      "Epoch:5 \n",
      "Iteration:119 \n",
      "Loss:0.05277197062969208\n",
      "Epoch:5 \n",
      "Iteration:120 \n",
      "Loss:0.012252547778189182\n",
      "Epoch:5 \n",
      "Iteration:121 \n",
      "Loss:0.05729968845844269\n",
      "Epoch:5 \n",
      "Iteration:122 \n",
      "Loss:0.013007535599172115\n",
      "Epoch:5 \n",
      "Iteration:123 \n",
      "Loss:0.030860135331749916\n",
      "Epoch:5 \n",
      "Iteration:124 \n",
      "Loss:0.010219590738415718\n",
      "Epoch:5 \n",
      "Iteration:125 \n",
      "Loss:0.01604657992720604\n",
      "Epoch:5 \n",
      "Iteration:126 \n",
      "Loss:0.014644069597125053\n",
      "Epoch:5 \n",
      "Iteration:127 \n",
      "Loss:0.03249460831284523\n",
      "Epoch:5 \n",
      "Iteration:128 \n",
      "Loss:0.02780020423233509\n",
      "Epoch:5 \n",
      "Iteration:129 \n",
      "Loss:0.013386514969170094\n",
      "Epoch:5 \n",
      "Iteration:130 \n",
      "Loss:0.05896317958831787\n",
      "Epoch:5 \n",
      "Iteration:131 \n",
      "Loss:0.007848761044442654\n",
      "Epoch:5 \n",
      "Iteration:132 \n",
      "Loss:0.00525230448693037\n",
      "Epoch:5 \n",
      "Iteration:133 \n",
      "Loss:0.025292033329606056\n",
      "Epoch:5 \n",
      "Iteration:134 \n",
      "Loss:0.09321035444736481\n",
      "Epoch:5 \n",
      "Iteration:135 \n",
      "Loss:0.014823350124061108\n",
      "Epoch:5 \n",
      "Iteration:136 \n",
      "Loss:0.03575638681650162\n",
      "Epoch:5 \n",
      "Iteration:137 \n",
      "Loss:0.09268287569284439\n",
      "Epoch:5 \n",
      "Iteration:138 \n",
      "Loss:0.00418532220646739\n",
      "Epoch:5 \n",
      "Iteration:139 \n",
      "Loss:0.005711697973310947\n",
      "Epoch:5 \n",
      "Iteration:140 \n",
      "Loss:0.017858222126960754\n",
      "Epoch:5 \n",
      "Iteration:141 \n",
      "Loss:0.008881667628884315\n",
      "Epoch:5 \n",
      "Iteration:142 \n",
      "Loss:0.023469991981983185\n",
      "Epoch:5 \n",
      "Iteration:143 \n",
      "Loss:0.031435318291187286\n",
      "Epoch:5 \n",
      "Iteration:144 \n",
      "Loss:0.047724198549985886\n",
      "Epoch:5 \n",
      "Iteration:145 \n",
      "Loss:0.0122687928378582\n",
      "Epoch:5 \n",
      "Iteration:146 \n",
      "Loss:0.01102321594953537\n",
      "Epoch:5 \n",
      "Iteration:147 \n",
      "Loss:0.0547788143157959\n",
      "Epoch:5 \n",
      "Iteration:148 \n",
      "Loss:0.019800670444965363\n",
      "Epoch:5 \n",
      "Iteration:149 \n",
      "Loss:0.007703104056417942\n",
      "Epoch:5 \n",
      "Iteration:150 \n",
      "Loss:0.03032558225095272\n",
      "Epoch:5 \n",
      "Iteration:151 \n",
      "Loss:0.007082965224981308\n",
      "Epoch:5 \n",
      "Iteration:152 \n",
      "Loss:0.08963114768266678\n",
      "Epoch:5 \n",
      "Iteration:153 \n",
      "Loss:0.011886557564139366\n",
      "Epoch:5 \n",
      "Iteration:154 \n",
      "Loss:0.005273689050227404\n",
      "Epoch:5 \n",
      "Iteration:155 \n",
      "Loss:0.0060187410563230515\n",
      "Epoch:5 \n",
      "Iteration:156 \n",
      "Loss:0.0047499011270701885\n",
      "Epoch:5 \n",
      "Iteration:157 \n",
      "Loss:0.001847783220000565\n",
      "Epoch:5 \n",
      "Iteration:158 \n",
      "Loss:0.006017403211444616\n",
      "Epoch:5 \n",
      "Iteration:159 \n",
      "Loss:0.03088475950062275\n",
      "Epoch:5 \n",
      "Iteration:160 \n",
      "Loss:0.022606918588280678\n",
      "Epoch:5 \n",
      "Iteration:161 \n",
      "Loss:0.04354383051395416\n",
      "Epoch:5 \n",
      "Iteration:162 \n",
      "Loss:0.053142450749874115\n",
      "Epoch:5 \n",
      "Iteration:163 \n",
      "Loss:0.0013626903528347611\n",
      "Epoch:5 \n",
      "Iteration:164 \n",
      "Loss:0.0022888618987053633\n",
      "Epoch:5 \n",
      "Iteration:165 \n",
      "Loss:0.10601752996444702\n",
      "Epoch:5 \n",
      "Iteration:166 \n",
      "Loss:0.027201468124985695\n",
      "Epoch:5 \n",
      "Iteration:167 \n",
      "Loss:0.029203757643699646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5 \n",
      "Iteration:168 \n",
      "Loss:0.003371194237843156\n",
      "Epoch:5 \n",
      "Iteration:169 \n",
      "Loss:0.1367059350013733\n",
      "Epoch:5 \n",
      "Iteration:170 \n",
      "Loss:0.0333084836602211\n",
      "Epoch:5 \n",
      "Iteration:171 \n",
      "Loss:0.0029286888893693686\n",
      "Epoch:5 \n",
      "Iteration:172 \n",
      "Loss:0.043764401227235794\n",
      "Epoch:5 \n",
      "Iteration:173 \n",
      "Loss:0.022074062377214432\n",
      "Epoch:5 \n",
      "Iteration:174 \n",
      "Loss:0.03626113757491112\n",
      "Epoch:5 \n",
      "Iteration:175 \n",
      "Loss:0.027319753542542458\n",
      "Epoch:5 \n",
      "Iteration:176 \n",
      "Loss:0.10259804874658585\n",
      "Epoch:5 \n",
      "Iteration:177 \n",
      "Loss:0.07937386631965637\n",
      "Epoch:5 \n",
      "Iteration:178 \n",
      "Loss:0.0366901233792305\n",
      "Epoch:5 \n",
      "Iteration:179 \n",
      "Loss:0.01942426711320877\n",
      "Epoch:5 \n",
      "Iteration:180 \n",
      "Loss:0.021640989929437637\n",
      "Epoch:5 \n",
      "Iteration:181 \n",
      "Loss:0.021282339468598366\n",
      "Epoch:5 \n",
      "Iteration:182 \n",
      "Loss:0.12226741760969162\n",
      "Epoch:5 \n",
      "Iteration:183 \n",
      "Loss:0.0194413885474205\n",
      "Epoch:5 \n",
      "Iteration:184 \n",
      "Loss:0.022255633026361465\n",
      "Epoch:5 \n",
      "Iteration:185 \n",
      "Loss:0.04461973160505295\n",
      "Epoch:5 \n",
      "Iteration:186 \n",
      "Loss:0.05878574401140213\n",
      "Epoch:5 \n",
      "Iteration:187 \n",
      "Loss:0.10168582946062088\n",
      "Epoch:5 \n",
      "Iteration:188 \n",
      "Loss:0.15556085109710693\n",
      "Epoch:5 \n",
      "Iteration:189 \n",
      "Loss:0.04161043092608452\n",
      "Epoch:5 \n",
      "Iteration:190 \n",
      "Loss:0.02827397733926773\n",
      "Epoch:5 \n",
      "Iteration:191 \n",
      "Loss:0.006266253534704447\n",
      "Epoch:5 \n",
      "Iteration:192 \n",
      "Loss:0.03830084949731827\n",
      "Epoch:5 \n",
      "Iteration:193 \n",
      "Loss:0.01181009691208601\n",
      "Epoch:5 \n",
      "Iteration:194 \n",
      "Loss:0.00519329309463501\n",
      "Epoch:5 \n",
      "Iteration:195 \n",
      "Loss:0.09154728055000305\n",
      "Epoch:5 \n",
      "Iteration:196 \n",
      "Loss:0.011460389941930771\n",
      "Epoch:5 \n",
      "Iteration:197 \n",
      "Loss:0.0032536129001528025\n",
      "Epoch:5 \n",
      "Iteration:198 \n",
      "Loss:0.04917233809828758\n",
      "Epoch:5 \n",
      "Iteration:199 \n",
      "Loss:0.06639426201581955\n",
      "Epoch:5 \n",
      "Iteration:200 \n",
      "Loss:0.03480757400393486\n",
      "Epoch:5 \n",
      "Iteration:201 \n",
      "Loss:0.0013720226706936955\n",
      "Epoch:5 \n",
      "Iteration:202 \n",
      "Loss:0.0681237056851387\n",
      "Epoch:5 \n",
      "Iteration:203 \n",
      "Loss:0.01267420407384634\n",
      "Epoch:5 \n",
      "Iteration:204 \n",
      "Loss:0.05429048836231232\n",
      "Epoch:5 \n",
      "Iteration:205 \n",
      "Loss:0.1858130395412445\n",
      "Epoch:5 \n",
      "Iteration:206 \n",
      "Loss:0.07387594878673553\n",
      "Epoch:5 \n",
      "Iteration:207 \n",
      "Loss:0.0027198835741728544\n",
      "Epoch:5 \n",
      "Iteration:208 \n",
      "Loss:0.026393337175250053\n",
      "Epoch:5 \n",
      "Iteration:209 \n",
      "Loss:0.007987418211996555\n",
      "Epoch:5 \n",
      "Iteration:210 \n",
      "Loss:0.06281453371047974\n",
      "Epoch:5 \n",
      "Iteration:211 \n",
      "Loss:0.04198765382170677\n",
      "Epoch:5 \n",
      "Iteration:212 \n",
      "Loss:0.015796007588505745\n",
      "Epoch:5 \n",
      "Iteration:213 \n",
      "Loss:0.025341691449284554\n",
      "Epoch:5 \n",
      "Iteration:214 \n",
      "Loss:0.04837237298488617\n",
      "Epoch:5 \n",
      "Iteration:215 \n",
      "Loss:0.05099866911768913\n",
      "Epoch:5 \n",
      "Iteration:216 \n",
      "Loss:0.10531184077262878\n",
      "Epoch:5 \n",
      "Iteration:217 \n",
      "Loss:0.020571578294038773\n",
      "Epoch:5 \n",
      "Iteration:218 \n",
      "Loss:0.0203409343957901\n",
      "Epoch:5 \n",
      "Iteration:219 \n",
      "Loss:0.002193605527281761\n",
      "Epoch:5 \n",
      "Iteration:220 \n",
      "Loss:0.01738307811319828\n",
      "Epoch:5 \n",
      "Iteration:221 \n",
      "Loss:0.02719496563076973\n",
      "Epoch:5 \n",
      "Iteration:222 \n",
      "Loss:0.011695698834955692\n",
      "Epoch:5 \n",
      "Iteration:223 \n",
      "Loss:0.045908451080322266\n",
      "Epoch:5 \n",
      "Iteration:224 \n",
      "Loss:0.056271396577358246\n",
      "Epoch:5 \n",
      "Iteration:225 \n",
      "Loss:0.0629812702536583\n",
      "Epoch:5 \n",
      "Iteration:226 \n",
      "Loss:0.005832304246723652\n",
      "Epoch:5 \n",
      "Iteration:227 \n",
      "Loss:0.01732846349477768\n",
      "Epoch:5 \n",
      "Iteration:228 \n",
      "Loss:0.04719158634543419\n",
      "Epoch:5 \n",
      "Iteration:229 \n",
      "Loss:0.02598273567855358\n",
      "Epoch:5 \n",
      "Iteration:230 \n",
      "Loss:0.020170358940958977\n",
      "Epoch:5 \n",
      "Iteration:231 \n",
      "Loss:0.009102598764002323\n",
      "Epoch:5 \n",
      "Iteration:232 \n",
      "Loss:0.004015134647488594\n",
      "Epoch:5 \n",
      "Iteration:233 \n",
      "Loss:0.05322112888097763\n",
      "Epoch:5 \n",
      "Iteration:234 \n",
      "Loss:0.0249424297362566\n",
      "Epoch:5 \n",
      "Iteration:235 \n",
      "Loss:0.008256280794739723\n",
      "Epoch:5 \n",
      "Iteration:236 \n",
      "Loss:0.11711681634187698\n",
      "Epoch:5 \n",
      "Iteration:237 \n",
      "Loss:0.012890856713056564\n",
      "Epoch:5 \n",
      "Iteration:238 \n",
      "Loss:0.021939408034086227\n",
      "Epoch:5 \n",
      "Iteration:239 \n",
      "Loss:0.0330989845097065\n",
      "Epoch:5 \n",
      "Iteration:240 \n",
      "Loss:0.014909918420016766\n",
      "Epoch:5 \n",
      "Iteration:241 \n",
      "Loss:0.042488984763622284\n",
      "Epoch:5 \n",
      "Iteration:242 \n",
      "Loss:0.0034189862199127674\n",
      "Epoch:5 \n",
      "Iteration:243 \n",
      "Loss:0.027727391570806503\n",
      "Epoch:5 \n",
      "Iteration:244 \n",
      "Loss:0.005130380392074585\n",
      "Epoch:5 \n",
      "Iteration:245 \n",
      "Loss:0.025456123054027557\n",
      "Epoch:5 \n",
      "Iteration:246 \n",
      "Loss:0.03983313962817192\n",
      "Epoch:5 \n",
      "Iteration:247 \n",
      "Loss:0.04004927724599838\n",
      "Epoch:5 \n",
      "Iteration:248 \n",
      "Loss:0.10825648158788681\n",
      "Epoch:5 \n",
      "Iteration:249 \n",
      "Loss:0.018194502219557762\n",
      "Epoch:5 \n",
      "Iteration:250 \n",
      "Loss:0.019829368218779564\n",
      "Epoch:5 \n",
      "Iteration:251 \n",
      "Loss:0.15166132152080536\n",
      "Epoch:5 \n",
      "Iteration:252 \n",
      "Loss:0.007216374855488539\n",
      "Epoch:5 \n",
      "Iteration:253 \n",
      "Loss:0.021638037636876106\n",
      "Epoch:5 \n",
      "Iteration:254 \n",
      "Loss:0.0780341699719429\n",
      "Epoch:5 \n",
      "Iteration:255 \n",
      "Loss:0.02714572474360466\n",
      "Epoch:5 \n",
      "Iteration:256 \n",
      "Loss:0.010995647870004177\n",
      "Epoch:5 \n",
      "Iteration:257 \n",
      "Loss:0.016640473157167435\n",
      "Epoch:5 \n",
      "Iteration:258 \n",
      "Loss:0.024537472054362297\n",
      "Epoch:5 \n",
      "Iteration:259 \n",
      "Loss:0.056220997124910355\n",
      "Epoch:5 \n",
      "Iteration:260 \n",
      "Loss:0.014782808721065521\n",
      "Epoch:5 \n",
      "Iteration:261 \n",
      "Loss:0.04132203012704849\n",
      "Epoch:5 \n",
      "Iteration:262 \n",
      "Loss:0.05179990828037262\n",
      "Epoch:5 \n",
      "Iteration:263 \n",
      "Loss:0.04839722067117691\n",
      "Epoch:5 \n",
      "Iteration:264 \n",
      "Loss:0.08021727204322815\n",
      "Epoch:5 \n",
      "Iteration:265 \n",
      "Loss:0.22404035925865173\n",
      "Epoch:5 \n",
      "Iteration:266 \n",
      "Loss:0.004304721485823393\n",
      "Epoch:5 \n",
      "Iteration:267 \n",
      "Loss:0.009333536028862\n",
      "Epoch:5 \n",
      "Iteration:268 \n",
      "Loss:0.04572006314992905\n",
      "Epoch:5 \n",
      "Iteration:269 \n",
      "Loss:0.0029876308981329203\n",
      "Epoch:5 \n",
      "Iteration:270 \n",
      "Loss:0.06083708256483078\n",
      "Epoch:5 \n",
      "Iteration:271 \n",
      "Loss:0.06141042709350586\n",
      "Epoch:5 \n",
      "Iteration:272 \n",
      "Loss:0.09740705788135529\n",
      "Epoch:5 \n",
      "Iteration:273 \n",
      "Loss:0.04087788984179497\n",
      "Epoch:5 \n",
      "Iteration:274 \n",
      "Loss:0.04284539818763733\n",
      "Epoch:5 \n",
      "Iteration:275 \n",
      "Loss:0.08911549299955368\n",
      "Epoch:5 \n",
      "Iteration:276 \n",
      "Loss:0.054368551820516586\n",
      "Epoch:5 \n",
      "Iteration:277 \n",
      "Loss:0.051383983343839645\n",
      "Epoch:5 \n",
      "Iteration:278 \n",
      "Loss:0.043430395424366\n",
      "Epoch:5 \n",
      "Iteration:279 \n",
      "Loss:0.021310005336999893\n",
      "Epoch:5 \n",
      "Iteration:280 \n",
      "Loss:0.023348279297351837\n",
      "Epoch:5 \n",
      "Iteration:281 \n",
      "Loss:0.04596295952796936\n",
      "Epoch:5 \n",
      "Iteration:282 \n",
      "Loss:0.006715267896652222\n",
      "Epoch:5 \n",
      "Iteration:283 \n",
      "Loss:0.004026536364108324\n",
      "Epoch:5 \n",
      "Iteration:284 \n",
      "Loss:0.028221596032381058\n",
      "Epoch:5 \n",
      "Iteration:285 \n",
      "Loss:0.080050989985466\n",
      "Epoch:5 \n",
      "Iteration:286 \n",
      "Loss:0.0425339974462986\n",
      "Epoch:5 \n",
      "Iteration:287 \n",
      "Loss:0.006741777062416077\n",
      "Epoch:5 \n",
      "Iteration:288 \n",
      "Loss:0.023670291528105736\n",
      "Epoch:5 \n",
      "Iteration:289 \n",
      "Loss:0.022853979840874672\n",
      "Epoch:5 \n",
      "Iteration:290 \n",
      "Loss:0.0547621063888073\n",
      "Epoch:5 \n",
      "Iteration:291 \n",
      "Loss:0.03657141700387001\n",
      "Epoch:5 \n",
      "Iteration:292 \n",
      "Loss:0.028854265809059143\n",
      "Epoch:5 \n",
      "Iteration:293 \n",
      "Loss:0.014157739467918873\n",
      "Epoch:5 \n",
      "Iteration:294 \n",
      "Loss:0.048996973782777786\n",
      "Epoch:5 \n",
      "Iteration:295 \n",
      "Loss:0.07205921411514282\n",
      "Epoch:5 \n",
      "Iteration:296 \n",
      "Loss:0.07057604938745499\n",
      "Epoch:5 \n",
      "Iteration:297 \n",
      "Loss:0.03098134696483612\n",
      "Epoch:5 \n",
      "Iteration:298 \n",
      "Loss:0.014943258836865425\n",
      "Epoch:5 \n",
      "Iteration:299 \n",
      "Loss:0.005203001666814089\n",
      "Epoch:5 \n",
      "Iteration:300 \n",
      "Loss:0.09293577075004578\n",
      "Epoch:5 \n",
      "Iteration:301 \n",
      "Loss:0.179232656955719\n",
      "Epoch:5 \n",
      "Iteration:302 \n",
      "Loss:0.0018876743270084262\n",
      "Epoch:5 \n",
      "Iteration:303 \n",
      "Loss:0.010010268539190292\n",
      "Epoch:5 \n",
      "Iteration:304 \n",
      "Loss:0.014469531364738941\n",
      "Epoch:5 \n",
      "Iteration:305 \n",
      "Loss:0.025282375514507294\n",
      "Epoch:5 \n",
      "Iteration:306 \n",
      "Loss:0.015078680589795113\n",
      "Epoch:5 \n",
      "Iteration:307 \n",
      "Loss:0.019649401307106018\n",
      "Epoch:5 \n",
      "Iteration:308 \n",
      "Loss:0.007615830283612013\n",
      "Epoch:5 \n",
      "Iteration:309 \n",
      "Loss:0.003454160410910845\n",
      "Epoch:5 \n",
      "Iteration:310 \n",
      "Loss:0.048382796347141266\n",
      "Epoch:5 \n",
      "Iteration:311 \n",
      "Loss:0.04584045708179474\n",
      "Epoch:5 \n",
      "Iteration:312 \n",
      "Loss:0.043960265815258026\n",
      "Epoch:5 \n",
      "Iteration:313 \n",
      "Loss:0.05136370658874512\n",
      "Epoch:5 \n",
      "Iteration:314 \n",
      "Loss:0.014082192443311214\n",
      "Epoch:5 \n",
      "Iteration:315 \n",
      "Loss:0.047573547810316086\n",
      "Epoch:5 \n",
      "Iteration:316 \n",
      "Loss:0.012408256530761719\n",
      "Epoch:5 \n",
      "Iteration:317 \n",
      "Loss:0.06963067501783371\n",
      "Epoch:5 \n",
      "Iteration:318 \n",
      "Loss:0.02974516898393631\n",
      "Epoch:5 \n",
      "Iteration:319 \n",
      "Loss:0.05822201818227768\n",
      "Epoch:5 \n",
      "Iteration:320 \n",
      "Loss:0.08304069191217422\n",
      "Epoch:5 \n",
      "Iteration:321 \n",
      "Loss:0.030409004539251328\n",
      "Epoch:5 \n",
      "Iteration:322 \n",
      "Loss:0.07946910709142685\n",
      "Epoch:5 \n",
      "Iteration:323 \n",
      "Loss:0.04857013374567032\n",
      "Epoch:5 \n",
      "Iteration:324 \n",
      "Loss:0.10957178473472595\n",
      "Epoch:5 \n",
      "Iteration:325 \n",
      "Loss:0.009823182597756386\n",
      "Epoch:5 \n",
      "Iteration:326 \n",
      "Loss:0.005253791343420744\n",
      "Epoch:5 \n",
      "Iteration:327 \n",
      "Loss:0.10247427970170975\n",
      "Epoch:5 \n",
      "Iteration:328 \n",
      "Loss:0.05277135223150253\n",
      "Epoch:5 \n",
      "Iteration:329 \n",
      "Loss:0.007127128075808287\n",
      "Epoch:5 \n",
      "Iteration:330 \n",
      "Loss:0.04397985339164734\n",
      "Epoch:5 \n",
      "Iteration:331 \n",
      "Loss:0.016715778037905693\n",
      "Epoch:5 \n",
      "Iteration:332 \n",
      "Loss:0.04088862985372543\n",
      "Epoch:5 \n",
      "Iteration:333 \n",
      "Loss:0.17076756060123444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5 \n",
      "Iteration:334 \n",
      "Loss:0.010469253174960613\n",
      "Epoch:5 \n",
      "Iteration:335 \n",
      "Loss:0.00333763868547976\n",
      "Epoch:5 \n",
      "Iteration:336 \n",
      "Loss:0.05751198157668114\n",
      "Epoch:5 \n",
      "Iteration:337 \n",
      "Loss:0.0816745012998581\n",
      "Epoch:5 \n",
      "Iteration:338 \n",
      "Loss:0.0029237940907478333\n",
      "Epoch:5 \n",
      "Iteration:339 \n",
      "Loss:0.05111284554004669\n",
      "Epoch:5 \n",
      "Iteration:340 \n",
      "Loss:0.09387700259685516\n",
      "Epoch:5 \n",
      "Iteration:341 \n",
      "Loss:0.03929232060909271\n",
      "Epoch:5 \n",
      "Iteration:342 \n",
      "Loss:0.00708002271130681\n",
      "Epoch:5 \n",
      "Iteration:343 \n",
      "Loss:0.062146686017513275\n",
      "Epoch:5 \n",
      "Iteration:344 \n",
      "Loss:0.013858905993402004\n",
      "Epoch:5 \n",
      "Iteration:345 \n",
      "Loss:0.09932363778352737\n",
      "Epoch:5 \n",
      "Iteration:346 \n",
      "Loss:0.03540554642677307\n",
      "Epoch:5 \n",
      "Iteration:347 \n",
      "Loss:0.013138684444129467\n",
      "Epoch:5 \n",
      "Iteration:348 \n",
      "Loss:0.12957780063152313\n",
      "Epoch:5 \n",
      "Iteration:349 \n",
      "Loss:0.015373778529465199\n",
      "Epoch:5 \n",
      "Iteration:350 \n",
      "Loss:0.05264896899461746\n",
      "Epoch:5 \n",
      "Iteration:351 \n",
      "Loss:0.08456496149301529\n",
      "Epoch:5 \n",
      "Iteration:352 \n",
      "Loss:0.006332049146294594\n",
      "Epoch:5 \n",
      "Iteration:353 \n",
      "Loss:0.046433161944150925\n",
      "Epoch:5 \n",
      "Iteration:354 \n",
      "Loss:0.043002448976039886\n",
      "Epoch:5 \n",
      "Iteration:355 \n",
      "Loss:0.028477050364017487\n",
      "Epoch:5 \n",
      "Iteration:356 \n",
      "Loss:0.12587451934814453\n",
      "Epoch:5 \n",
      "Iteration:357 \n",
      "Loss:0.10533547401428223\n",
      "Epoch:5 \n",
      "Iteration:358 \n",
      "Loss:0.06571564823389053\n",
      "Epoch:5 \n",
      "Iteration:359 \n",
      "Loss:0.019195809960365295\n",
      "Epoch:5 \n",
      "Iteration:360 \n",
      "Loss:0.045375071465969086\n",
      "Epoch:5 \n",
      "Iteration:361 \n",
      "Loss:0.10289410501718521\n",
      "Epoch:5 \n",
      "Iteration:362 \n",
      "Loss:0.028914734721183777\n",
      "Epoch:5 \n",
      "Iteration:363 \n",
      "Loss:0.008770766668021679\n",
      "Epoch:5 \n",
      "Iteration:364 \n",
      "Loss:0.1241755411028862\n",
      "Epoch:5 \n",
      "Iteration:365 \n",
      "Loss:0.00492456741631031\n",
      "Epoch:5 \n",
      "Iteration:366 \n",
      "Loss:0.06998497992753983\n",
      "Epoch:5 \n",
      "Iteration:367 \n",
      "Loss:0.025064587593078613\n",
      "Epoch:5 \n",
      "Iteration:368 \n",
      "Loss:0.10906746983528137\n",
      "Epoch:5 \n",
      "Iteration:369 \n",
      "Loss:0.03760945424437523\n",
      "Epoch:5 \n",
      "Iteration:370 \n",
      "Loss:0.008554023690521717\n",
      "Epoch:5 \n",
      "Iteration:371 \n",
      "Loss:0.1067260280251503\n",
      "Epoch:5 \n",
      "Iteration:372 \n",
      "Loss:0.02043270133435726\n",
      "Epoch:5 \n",
      "Iteration:373 \n",
      "Loss:0.042824264615774155\n",
      "Epoch:5 \n",
      "Iteration:374 \n",
      "Loss:0.07117931544780731\n",
      "Epoch:5 \n",
      "Iteration:375 \n",
      "Loss:0.023089684545993805\n",
      "Epoch:5 \n",
      "Iteration:376 \n",
      "Loss:0.028763189911842346\n",
      "Epoch:5 \n",
      "Iteration:377 \n",
      "Loss:0.024829117581248283\n",
      "Epoch:5 \n",
      "Iteration:378 \n",
      "Loss:0.0016516378382220864\n",
      "Epoch:5 \n",
      "Iteration:379 \n",
      "Loss:0.02502274513244629\n",
      "Epoch:5 \n",
      "Iteration:380 \n",
      "Loss:0.17466101050376892\n",
      "Epoch:5 \n",
      "Iteration:381 \n",
      "Loss:0.023437142372131348\n",
      "Epoch:5 \n",
      "Iteration:382 \n",
      "Loss:0.03697950392961502\n",
      "Epoch:5 \n",
      "Iteration:383 \n",
      "Loss:0.1192232295870781\n",
      "Epoch:5 \n",
      "Iteration:384 \n",
      "Loss:0.10674715042114258\n",
      "Epoch:5 \n",
      "Iteration:385 \n",
      "Loss:0.06372912228107452\n",
      "Epoch:5 \n",
      "Iteration:386 \n",
      "Loss:0.029676303267478943\n",
      "Epoch:5 \n",
      "Iteration:387 \n",
      "Loss:0.039401840418577194\n",
      "Epoch:5 \n",
      "Iteration:388 \n",
      "Loss:0.04313652962446213\n",
      "Epoch:5 \n",
      "Iteration:389 \n",
      "Loss:0.04242105409502983\n",
      "Epoch:5 \n",
      "Iteration:390 \n",
      "Loss:0.046509504318237305\n",
      "Epoch:5 \n",
      "Iteration:391 \n",
      "Loss:0.012387670576572418\n",
      "Epoch:5 \n",
      "Iteration:392 \n",
      "Loss:0.004667161498218775\n",
      "Epoch:5 \n",
      "Iteration:393 \n",
      "Loss:0.00928991287946701\n",
      "Epoch:5 \n",
      "Iteration:394 \n",
      "Loss:0.08838345855474472\n",
      "Epoch:5 \n",
      "Iteration:395 \n",
      "Loss:0.024059347808361053\n",
      "Epoch:5 \n",
      "Iteration:396 \n",
      "Loss:0.05232882499694824\n",
      "Epoch:5 \n",
      "Iteration:397 \n",
      "Loss:0.07347489148378372\n",
      "Epoch:5 \n",
      "Iteration:398 \n",
      "Loss:0.04029689356684685\n",
      "Epoch:5 \n",
      "Iteration:399 \n",
      "Loss:0.051091793924570084\n",
      "Epoch:5 \n",
      "Iteration:400 \n",
      "Loss:0.08678067475557327\n",
      "Epoch:5 \n",
      "Iteration:401 \n",
      "Loss:0.005495588295161724\n",
      "Epoch:5 \n",
      "Iteration:402 \n",
      "Loss:0.05553276836872101\n",
      "Epoch:5 \n",
      "Iteration:403 \n",
      "Loss:0.03457173332571983\n",
      "Epoch:5 \n",
      "Iteration:404 \n",
      "Loss:0.031582266092300415\n",
      "Epoch:5 \n",
      "Iteration:405 \n",
      "Loss:0.04680706933140755\n",
      "Epoch:5 \n",
      "Iteration:406 \n",
      "Loss:0.054624851793050766\n",
      "Epoch:5 \n",
      "Iteration:407 \n",
      "Loss:0.05530458316206932\n",
      "Epoch:5 \n",
      "Iteration:408 \n",
      "Loss:0.06197701394557953\n",
      "Epoch:5 \n",
      "Iteration:409 \n",
      "Loss:0.0868837758898735\n",
      "Epoch:5 \n",
      "Iteration:410 \n",
      "Loss:0.11450186371803284\n",
      "Epoch:5 \n",
      "Iteration:411 \n",
      "Loss:0.03578885272145271\n",
      "Epoch:5 \n",
      "Iteration:412 \n",
      "Loss:0.0013503247173503041\n",
      "Epoch:5 \n",
      "Iteration:413 \n",
      "Loss:0.031604208052158356\n",
      "Epoch:5 \n",
      "Iteration:414 \n",
      "Loss:0.0172459464520216\n",
      "Epoch:5 \n",
      "Iteration:415 \n",
      "Loss:0.12543347477912903\n",
      "Epoch:5 \n",
      "Iteration:416 \n",
      "Loss:0.0702885314822197\n",
      "Epoch:5 \n",
      "Iteration:417 \n",
      "Loss:0.03554667532444\n",
      "Epoch:5 \n",
      "Iteration:418 \n",
      "Loss:0.08244433999061584\n",
      "Epoch:5 \n",
      "Iteration:419 \n",
      "Loss:0.04671969264745712\n",
      "Epoch:5 \n",
      "Iteration:420 \n",
      "Loss:0.05437220633029938\n",
      "Epoch:5 \n",
      "Iteration:421 \n",
      "Loss:0.05023599788546562\n",
      "Epoch:5 \n",
      "Iteration:422 \n",
      "Loss:0.06934364140033722\n",
      "Epoch:5 \n",
      "Iteration:423 \n",
      "Loss:0.041576795279979706\n",
      "Epoch:5 \n",
      "Iteration:424 \n",
      "Loss:0.013442312367260456\n",
      "Epoch:5 \n",
      "Iteration:425 \n",
      "Loss:0.06820803880691528\n",
      "Epoch:5 \n",
      "Iteration:426 \n",
      "Loss:0.0948287770152092\n",
      "Epoch:5 \n",
      "Iteration:427 \n",
      "Loss:0.08503484725952148\n",
      "Epoch:5 \n",
      "Iteration:428 \n",
      "Loss:0.04229574277997017\n",
      "Epoch:5 \n",
      "Iteration:429 \n",
      "Loss:0.025745704770088196\n",
      "Epoch:5 \n",
      "Iteration:430 \n",
      "Loss:0.032654061913490295\n",
      "Epoch:5 \n",
      "Iteration:431 \n",
      "Loss:0.01710253767669201\n",
      "Epoch:5 \n",
      "Iteration:432 \n",
      "Loss:0.020856276154518127\n",
      "Epoch:5 \n",
      "Iteration:433 \n",
      "Loss:0.0195842906832695\n",
      "Epoch:5 \n",
      "Iteration:434 \n",
      "Loss:0.04392298310995102\n",
      "Epoch:5 \n",
      "Iteration:435 \n",
      "Loss:0.07840470224618912\n",
      "Epoch:5 \n",
      "Iteration:436 \n",
      "Loss:0.038389235734939575\n",
      "Epoch:5 \n",
      "Iteration:437 \n",
      "Loss:0.03839346393942833\n",
      "Epoch:5 \n",
      "Iteration:438 \n",
      "Loss:0.08290597051382065\n",
      "Epoch:5 \n",
      "Iteration:439 \n",
      "Loss:0.02240535244345665\n",
      "Epoch:5 \n",
      "Iteration:440 \n",
      "Loss:0.055692605674266815\n",
      "Epoch:5 \n",
      "Iteration:441 \n",
      "Loss:0.027674470096826553\n",
      "Epoch:5 \n",
      "Iteration:442 \n",
      "Loss:0.022056199610233307\n",
      "Epoch:5 \n",
      "Iteration:443 \n",
      "Loss:0.026656026020646095\n",
      "Epoch:5 \n",
      "Iteration:444 \n",
      "Loss:0.028085336089134216\n",
      "Epoch:5 \n",
      "Iteration:445 \n",
      "Loss:0.011384980753064156\n",
      "Epoch:5 \n",
      "Iteration:446 \n",
      "Loss:0.05817548185586929\n",
      "Epoch:5 \n",
      "Iteration:447 \n",
      "Loss:0.04625573009252548\n",
      "Epoch:5 \n",
      "Iteration:448 \n",
      "Loss:0.03665824979543686\n",
      "Epoch:5 \n",
      "Iteration:449 \n",
      "Loss:0.043323710560798645\n",
      "Epoch:5 \n",
      "Iteration:450 \n",
      "Loss:0.017426228150725365\n",
      "Epoch:5 \n",
      "Iteration:451 \n",
      "Loss:0.0365583710372448\n",
      "Epoch:5 \n",
      "Iteration:452 \n",
      "Loss:0.05465990677475929\n",
      "Epoch:5 \n",
      "Iteration:453 \n",
      "Loss:0.07669070363044739\n",
      "Epoch:5 \n",
      "Iteration:454 \n",
      "Loss:0.09443493187427521\n",
      "Epoch:5 \n",
      "Iteration:455 \n",
      "Loss:0.08030182123184204\n",
      "Epoch:5 \n",
      "Iteration:456 \n",
      "Loss:0.04013628140091896\n",
      "Epoch:5 \n",
      "Iteration:457 \n",
      "Loss:0.06333023309707642\n",
      "Epoch:5 \n",
      "Iteration:458 \n",
      "Loss:0.041470542550086975\n",
      "Epoch:5 \n",
      "Iteration:459 \n",
      "Loss:0.010485414415597916\n",
      "Epoch:5 \n",
      "Iteration:460 \n",
      "Loss:0.024206167086958885\n",
      "Epoch:5 \n",
      "Iteration:461 \n",
      "Loss:0.007060531992465258\n",
      "Epoch:5 \n",
      "Iteration:462 \n",
      "Loss:0.01706262119114399\n",
      "Epoch:5 \n",
      "Iteration:463 \n",
      "Loss:0.19753897190093994\n",
      "Epoch:5 \n",
      "Iteration:464 \n",
      "Loss:0.04516153410077095\n",
      "Epoch:5 \n",
      "Iteration:465 \n",
      "Loss:0.06909158825874329\n",
      "Epoch:5 \n",
      "Iteration:466 \n",
      "Loss:0.03989846631884575\n",
      "Epoch:5 \n",
      "Iteration:467 \n",
      "Loss:0.03725603222846985\n",
      "Epoch:5 \n",
      "Iteration:468 \n",
      "Loss:0.03270873799920082\n",
      "Epoch:5 \n",
      "Iteration:469 \n",
      "Loss:0.12758269906044006\n",
      "Epoch:5 \n",
      "Iteration:470 \n",
      "Loss:0.02676538936793804\n",
      "Epoch:5 \n",
      "Iteration:471 \n",
      "Loss:0.14908429980278015\n",
      "Epoch:5 \n",
      "Iteration:472 \n",
      "Loss:0.036945320665836334\n",
      "Epoch:5 \n",
      "Iteration:473 \n",
      "Loss:0.031125718727707863\n",
      "Epoch:5 \n",
      "Iteration:474 \n",
      "Loss:0.03110155276954174\n",
      "Epoch:5 \n",
      "Iteration:475 \n",
      "Loss:0.024923808872699738\n",
      "Epoch:5 \n",
      "Iteration:476 \n",
      "Loss:0.008140339516103268\n",
      "Epoch:5 \n",
      "Iteration:477 \n",
      "Loss:0.013475959189236164\n",
      "Epoch:5 \n",
      "Iteration:478 \n",
      "Loss:0.022068817168474197\n",
      "Epoch:5 \n",
      "Iteration:479 \n",
      "Loss:0.021191684529185295\n",
      "Epoch:5 \n",
      "Iteration:480 \n",
      "Loss:0.10442979633808136\n",
      "Epoch:5 \n",
      "Iteration:481 \n",
      "Loss:0.0060240221209824085\n",
      "Epoch:5 \n",
      "Iteration:482 \n",
      "Loss:0.022857988253235817\n",
      "Epoch:5 \n",
      "Iteration:483 \n",
      "Loss:0.04332411661744118\n",
      "Epoch:5 \n",
      "Iteration:484 \n",
      "Loss:0.11777931451797485\n",
      "Epoch:5 \n",
      "Iteration:485 \n",
      "Loss:0.12377697974443436\n",
      "Epoch:5 \n",
      "Iteration:486 \n",
      "Loss:0.04110099375247955\n",
      "Epoch:5 \n",
      "Iteration:487 \n",
      "Loss:0.039203811436891556\n",
      "Epoch:5 \n",
      "Iteration:488 \n",
      "Loss:0.05967998132109642\n",
      "Epoch:5 \n",
      "Iteration:489 \n",
      "Loss:0.003850245149806142\n",
      "Epoch:5 \n",
      "Iteration:490 \n",
      "Loss:0.04961865767836571\n",
      "Epoch:5 \n",
      "Iteration:491 \n",
      "Loss:0.02007894590497017\n",
      "Epoch:5 \n",
      "Iteration:492 \n",
      "Loss:0.0012080129235982895\n",
      "Epoch:5 \n",
      "Iteration:493 \n",
      "Loss:0.06124259531497955\n",
      "Epoch:5 \n",
      "Iteration:494 \n",
      "Loss:0.02832639031112194\n",
      "Epoch:5 \n",
      "Iteration:495 \n",
      "Loss:0.050372667610645294\n",
      "Epoch:5 \n",
      "Iteration:496 \n",
      "Loss:0.009329910390079021\n",
      "Epoch:5 \n",
      "Iteration:497 \n",
      "Loss:0.058764442801475525\n",
      "Epoch:5 \n",
      "Iteration:498 \n",
      "Loss:0.06463534384965897\n",
      "Epoch:5 \n",
      "Iteration:499 \n",
      "Loss:0.030944380909204483\n",
      "Epoch:5 \n",
      "Iteration:500 \n",
      "Loss:0.026387058198451996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5 \n",
      "Iteration:501 \n",
      "Loss:0.07025323808193207\n",
      "Epoch:5 \n",
      "Iteration:502 \n",
      "Loss:0.0074350880458951\n",
      "Epoch:5 \n",
      "Iteration:503 \n",
      "Loss:0.031213246285915375\n",
      "Epoch:5 \n",
      "Iteration:504 \n",
      "Loss:0.009925741702318192\n",
      "Epoch:5 \n",
      "Iteration:505 \n",
      "Loss:0.1001056581735611\n",
      "Epoch:5 \n",
      "Iteration:506 \n",
      "Loss:0.16858603060245514\n",
      "Epoch:5 \n",
      "Iteration:507 \n",
      "Loss:0.011763614602386951\n",
      "Epoch:5 \n",
      "Iteration:508 \n",
      "Loss:0.07400573045015335\n",
      "Epoch:5 \n",
      "Iteration:509 \n",
      "Loss:0.0965205579996109\n",
      "Epoch:5 \n",
      "Iteration:510 \n",
      "Loss:0.05325038731098175\n",
      "Epoch:5 \n",
      "Iteration:511 \n",
      "Loss:0.10820859670639038\n",
      "Epoch:5 \n",
      "Iteration:512 \n",
      "Loss:0.013834051787853241\n",
      "Epoch:5 \n",
      "Iteration:513 \n",
      "Loss:0.00935860350728035\n",
      "Epoch:5 \n",
      "Iteration:514 \n",
      "Loss:0.03320690989494324\n",
      "Epoch:5 \n",
      "Iteration:515 \n",
      "Loss:0.05758867785334587\n",
      "Epoch:5 \n",
      "Iteration:516 \n",
      "Loss:0.0857243537902832\n",
      "Epoch:5 \n",
      "Iteration:517 \n",
      "Loss:0.007965043187141418\n",
      "Epoch:5 \n",
      "Iteration:518 \n",
      "Loss:0.09572593867778778\n",
      "Epoch:5 \n",
      "Iteration:519 \n",
      "Loss:0.04329109564423561\n",
      "Epoch:5 \n",
      "Iteration:520 \n",
      "Loss:0.012259460985660553\n",
      "Epoch:5 \n",
      "Iteration:521 \n",
      "Loss:0.02906518243253231\n",
      "Epoch:5 \n",
      "Iteration:522 \n",
      "Loss:0.03238600119948387\n",
      "Epoch:5 \n",
      "Iteration:523 \n",
      "Loss:0.07829958945512772\n",
      "Epoch:5 \n",
      "Iteration:524 \n",
      "Loss:0.08853375166654587\n",
      "Epoch:5 \n",
      "Iteration:525 \n",
      "Loss:0.03739073872566223\n",
      "Epoch:5 \n",
      "Iteration:526 \n",
      "Loss:0.009950990788638592\n",
      "Epoch:5 \n",
      "Iteration:527 \n",
      "Loss:0.028542401269078255\n",
      "Epoch:5 \n",
      "Iteration:528 \n",
      "Loss:0.04543047398328781\n",
      "Epoch:5 \n",
      "Iteration:529 \n",
      "Loss:0.036084841936826706\n",
      "Epoch:5 \n",
      "Iteration:530 \n",
      "Loss:0.020066287368535995\n",
      "Epoch:5 \n",
      "Iteration:531 \n",
      "Loss:0.01566131040453911\n",
      "Epoch:5 \n",
      "Iteration:532 \n",
      "Loss:0.06999991834163666\n",
      "Epoch:5 \n",
      "Iteration:533 \n",
      "Loss:0.035319503396749496\n",
      "Epoch:5 \n",
      "Iteration:534 \n",
      "Loss:0.13384738564491272\n",
      "Epoch:5 \n",
      "Iteration:535 \n",
      "Loss:0.05361543223261833\n",
      "Epoch:5 \n",
      "Iteration:536 \n",
      "Loss:0.041168030351400375\n",
      "Epoch:5 \n",
      "Iteration:537 \n",
      "Loss:0.09533627331256866\n",
      "Epoch:5 \n",
      "Iteration:538 \n",
      "Loss:0.02034333162009716\n",
      "Epoch:5 \n",
      "Iteration:539 \n",
      "Loss:0.023726437240839005\n",
      "Epoch:5 \n",
      "Iteration:540 \n",
      "Loss:0.02835550159215927\n",
      "Epoch:5 \n",
      "Iteration:541 \n",
      "Loss:0.009913735091686249\n",
      "Epoch:5 \n",
      "Iteration:542 \n",
      "Loss:0.03806377574801445\n",
      "Epoch:5 \n",
      "Iteration:543 \n",
      "Loss:0.06259583681821823\n",
      "Epoch:5 \n",
      "Iteration:544 \n",
      "Loss:0.031196046620607376\n",
      "Epoch:5 \n",
      "Iteration:545 \n",
      "Loss:0.031106457114219666\n",
      "Epoch:5 \n",
      "Iteration:546 \n",
      "Loss:0.10589409619569778\n",
      "Epoch:5 \n",
      "Iteration:547 \n",
      "Loss:0.08020887523889542\n",
      "Epoch:5 \n",
      "Iteration:548 \n",
      "Loss:0.045092593878507614\n",
      "Epoch:5 \n",
      "Iteration:549 \n",
      "Loss:0.05215921998023987\n",
      "Epoch:5 \n",
      "Iteration:550 \n",
      "Loss:0.016143472865223885\n",
      "Epoch:5 \n",
      "Iteration:551 \n",
      "Loss:0.00945336651057005\n",
      "Epoch:5 \n",
      "Iteration:552 \n",
      "Loss:0.06134295463562012\n",
      "Epoch:5 \n",
      "Iteration:553 \n",
      "Loss:0.034022457897663116\n",
      "Epoch:5 \n",
      "Iteration:554 \n",
      "Loss:0.010873165912926197\n",
      "Epoch:5 \n",
      "Iteration:555 \n",
      "Loss:0.018323291093111038\n",
      "Epoch:5 \n",
      "Iteration:556 \n",
      "Loss:0.020297009497880936\n",
      "Epoch:5 \n",
      "Iteration:557 \n",
      "Loss:0.010244459845125675\n",
      "Epoch:5 \n",
      "Iteration:558 \n",
      "Loss:0.037558481097221375\n",
      "Epoch:5 \n",
      "Iteration:559 \n",
      "Loss:0.01061056274920702\n",
      "Epoch:5 \n",
      "Iteration:560 \n",
      "Loss:0.04536796733736992\n",
      "Epoch:5 \n",
      "Iteration:561 \n",
      "Loss:0.08902322500944138\n",
      "Epoch:5 \n",
      "Iteration:562 \n",
      "Loss:0.04351144656538963\n",
      "Epoch:5 \n",
      "Iteration:563 \n",
      "Loss:0.04123517870903015\n",
      "Epoch:5 \n",
      "Iteration:564 \n",
      "Loss:0.03861682862043381\n",
      "Epoch:5 \n",
      "Iteration:565 \n",
      "Loss:0.04593491554260254\n",
      "Epoch:5 \n",
      "Iteration:566 \n",
      "Loss:0.1014302670955658\n",
      "Epoch:5 \n",
      "Iteration:567 \n",
      "Loss:0.052195657044649124\n",
      "Epoch:5 \n",
      "Iteration:568 \n",
      "Loss:0.015729490667581558\n",
      "Epoch:5 \n",
      "Iteration:569 \n",
      "Loss:0.010034353472292423\n",
      "Epoch:5 \n",
      "Iteration:570 \n",
      "Loss:0.11708416044712067\n",
      "Epoch:5 \n",
      "Iteration:571 \n",
      "Loss:0.1108635738492012\n",
      "Epoch:5 \n",
      "Iteration:572 \n",
      "Loss:0.056082434952259064\n",
      "Epoch:5 \n",
      "Iteration:573 \n",
      "Loss:0.012722469866275787\n",
      "Epoch:5 \n",
      "Iteration:574 \n",
      "Loss:0.06322310119867325\n",
      "Epoch:5 \n",
      "Iteration:575 \n",
      "Loss:0.019840845838189125\n",
      "Epoch:5 \n",
      "Iteration:576 \n",
      "Loss:0.06076742708683014\n",
      "Epoch:5 \n",
      "Iteration:577 \n",
      "Loss:0.02260960079729557\n",
      "Epoch:5 \n",
      "Iteration:578 \n",
      "Loss:0.032762885093688965\n",
      "Epoch:5 \n",
      "Iteration:579 \n",
      "Loss:0.0068029360845685005\n",
      "Epoch:5 \n",
      "Iteration:580 \n",
      "Loss:0.05213358998298645\n",
      "Epoch:5 \n",
      "Iteration:581 \n",
      "Loss:0.013620147481560707\n",
      "Epoch:5 \n",
      "Iteration:582 \n",
      "Loss:0.04285508021712303\n",
      "Epoch:5 \n",
      "Iteration:583 \n",
      "Loss:0.04328902065753937\n",
      "Epoch:5 \n",
      "Iteration:584 \n",
      "Loss:0.0504206120967865\n",
      "Epoch:5 \n",
      "Iteration:585 \n",
      "Loss:0.03718406707048416\n",
      "Epoch:5 \n",
      "Iteration:586 \n",
      "Loss:0.0032064556144177914\n",
      "Epoch:5 \n",
      "Iteration:587 \n",
      "Loss:0.00622851587831974\n",
      "Epoch:5 \n",
      "Iteration:588 \n",
      "Loss:0.08114097267389297\n",
      "Epoch:5 \n",
      "Iteration:589 \n",
      "Loss:0.03547229617834091\n",
      "Epoch:5 \n",
      "Iteration:590 \n",
      "Loss:0.006339258514344692\n",
      "Epoch:5 \n",
      "Iteration:591 \n",
      "Loss:0.1124071404337883\n",
      "Epoch:5 \n",
      "Iteration:592 \n",
      "Loss:0.07256704568862915\n",
      "Epoch:5 \n",
      "Iteration:593 \n",
      "Loss:0.09187868982553482\n",
      "Epoch:5 \n",
      "Iteration:594 \n",
      "Loss:0.04057544097304344\n",
      "Epoch:5 \n",
      "Iteration:595 \n",
      "Loss:0.11772045493125916\n",
      "Epoch:5 \n",
      "Iteration:596 \n",
      "Loss:0.052355241030454636\n",
      "Epoch:5 \n",
      "Iteration:597 \n",
      "Loss:0.008064932189881802\n",
      "Epoch:5 \n",
      "Iteration:598 \n",
      "Loss:0.017516866326332092\n",
      "Epoch:5 \n",
      "Iteration:599 \n",
      "Loss:0.0536358505487442\n",
      "Epoch:5 \n",
      "Iteration:600 \n",
      "Loss:0.0071176690980792046\n",
      "\n",
      "Accuracy of network in epoch 5: 98.695\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs = 5):\n",
    "    accuraccy_list = []\n",
    "    for epoch in range(epochs):\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch:{epoch + 1} \\nIteration:{i + 1} \\nLoss:{loss}')\n",
    "            with torch.no_grad():\n",
    "                total += labels.size(0)\n",
    "                _,prediction = torch.max(outputs, 1)\n",
    "                correct += (prediction == labels).sum().item()\n",
    "        print(f'\\nAccuracy of network in epoch {epoch + 1}: {100 * correct / total}')\n",
    "    writer.flush()\n",
    "\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network:98.02\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for data, labels in test_loader:\n",
    "    data = data.to(torch.device(\"cuda:0\"))\n",
    "    with torch.no_grad():\n",
    "        validation = model(data)\n",
    "        _,prediction = torch.max(validation, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (prediction.cpu() == labels).sum().item()\n",
    "    \n",
    "print(f'Accuracy of the network:{100 * correct / total}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
