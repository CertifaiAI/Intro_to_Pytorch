{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Dataloader\n",
    "### What is Dataloader\n",
    "Dataloader is a class that helps with shuffling and organizing the data in minibatches. We can import this class from `torch.utils.data`.\n",
    "\n",
    "The job of a data loader is to sample minibatches from a dataset, giving us the flexibility to choose the size of our minibatch to be use for training in each iteration. The constructor takes a `Dataset` object as input, along with `batch_size` and a `shuffle` boolean variable that indicates whether the data needs to be shuffled at the beginning of each epoch.\n",
    "\n",
    "In this chapter, we are going to do classification task based on Fashion MNIST dataset. Fashion MNIST dataset could be directly imported and downloaded from `torchvision.datasets.FashionMNIST`. Pytorch has collected several datasets (CIFAR, COCO, Cityscapes, etc..) in the `torchvision` library, you may have a look of the full list of datasets at [here](https://pytorch.org/docs/stable/torchvision/datasets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required library\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading/Downloading the FashionMNIST dataset, download might takes some time \n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root = '../data',\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor()\n",
    "    )\n",
    "test_set = torchvision.datasets.FashionMNIST(\n",
    "    root = '../data',\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transforms.ToTensor()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset into the `DataLoader` and input your desired batch size for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size = 32, shuffle = True)\n",
    "test_loader = DataLoader(test_set, batch_size = 32, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# A view of the DataLoader\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch\n",
    "\n",
    "# Output the size of each batch\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each images are assigned to one of the following labels:\n",
    "\n",
    "- 0 T-shirt/top\n",
    "- 1 Trouser\n",
    "- 2 Pullover\n",
    "- 3 Dress\n",
    "- 4 Coat\n",
    "- 5 Sandal\n",
    "- 6 Shirt\n",
    "- 7 Sneaker\n",
    "- 8 Bag\n",
    "- 9 Ankle boot\n",
    "\n",
    "Let us plot the image out to have a look on how does the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting numeric labels to text label\n",
    "\n",
    "def labelsText(labels):\n",
    "    labelDict = {\n",
    "                 0: \"T-shirt/Top\",\n",
    "                 1: \"Trouser\",\n",
    "                 2: \"Pullover\",\n",
    "                 3: \"Dress\",\n",
    "                 4: \"Coat\", \n",
    "                 5: \"Sandal\", \n",
    "                 6: \"Shirt\",\n",
    "                 7: \"Sneaker\",\n",
    "                 8: \"Bag\",\n",
    "                 9: \"Ankle Boot\"\n",
    "                 }\n",
    "    label = (labels.item() if type(labels) == torch.Tensor else labels)\n",
    "    return labelDict[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: \n",
      "Sneaker, Pullover, Coat, T-shirt/Top, Shirt, Dress, Sneaker, Dress, Pullover, Pullover, "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting out the images in the dataset\n",
    "\n",
    "grid = torchvision.utils.make_grid(images[0:10], nrow = 10)\n",
    "\n",
    "plt.figure(figsize = (15, 15))\n",
    "plt.imshow(np.transpose(grid, (1, 2, 0)))\n",
    "\n",
    "print(\"Labels: \")\n",
    "for i in labels[0:10]:\n",
    "    print(labelsText(i) + \", \", end = \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Build your first Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Model Training\n",
    "We had loaded our dataset into training and testing set, now let us build a simple Feedfoward Neural Network to perform classification on this dataset.\n",
    "\n",
    "PyTorch has a whole submodule dedicated to neural networks, called `torch.nn`. It contains the building blocks needed to create all sorts of neural network architectures.\n",
    "\n",
    "To build a Neural Network, it could be done in two ways :\n",
    "- Calling the `nn.Sequential()` for fast implementation of the network\n",
    "- Subclassing `nn.Module` to have more flexibility on designing the network, eg: writing the your own `foward()` method\n",
    "\n",
    "\n",
    "Now let us start building the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to build a 4 layers neural network with ReLU activation function. Apply dropout with 20% probability to reduce the effect of overfitting. Let us try build our model using `nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Sequential()\n",
    "torch.manual_seed(0)\n",
    "model_sequential = nn.Sequential(nn.Linear(784,256),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(256,128),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(128,64),\n",
    "                                 nn.Dropout(0.2),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(64,10),\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a wrapper function for our training called `training`. This wrapper function will take on parameters:\n",
    "- n_epochs\n",
    "- optimizer\n",
    "- model\n",
    "- loss_fn\n",
    "- train_loader\n",
    "- writer (Instance of Summary Writer to use TensorBoard for visualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch does support TensorBoard which provides the visualization and tooling needed for machine learning experimentation. It is a useful tool that we can use during our training. Now let's define our training loop and implement some of the TensorBoard methods. \n",
    "\n",
    "If you wish to know more on TensorBoard, you can access it at [here](https://pytorch.org/docs/stable/tensorboard.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def training(n_epochs, optimizer, model, loss_fn, train_loader, writer):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for imgs, labels in train_loader:\n",
    "            # Clearing gradient from previous mini-batch gradient computation  \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Reshape the tensor so that it fits the dimension of our input layer\n",
    "            # Get predictions output from the model\n",
    "            outputs = model(imgs.view(-1, 784))\n",
    "            \n",
    "            # Calculate the loss for curernt batch\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            # Calculating the gradient\n",
    "            loss.backward()\n",
    "            \n",
    "            # Updating the weights and biases using optimizer.step\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Summing up the loss over each epoch\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "            # Calculating the accuracy\n",
    "            predictions = torch.max(outputs, 1)[1]\n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += len(labels)\n",
    "\n",
    "        accuracy = correct * 100 / total\n",
    "        writer.add_scalar('Loss ', loss_train / len(train_loader), epoch)\n",
    "        writer.add_scalar('Accuracy ', accuracy, epoch)\n",
    "        print('Epoch {}, Training loss {} , Accuracy {:.2f} %'.format(epoch, loss_train / len(train_loader), accuracy))\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can open our TensorBoard in the terminal with the command of `tensorboard --logdir=runs`. Do remember change to the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready for training. Let's use `SGD` as our optimizer and `CrossEntropy` as loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 2.2894855969746906 , Accuracy 18.28 %\n",
      "Epoch 2, Training loss 2.2401039816538493 , Accuracy 28.62 %\n",
      "Epoch 3, Training loss 2.0709991912841796 , Accuracy 29.36 %\n",
      "Epoch 4, Training loss 1.6963001889546712 , Accuracy 35.94 %\n",
      "Epoch 5, Training loss 1.406517707824707 , Accuracy 46.59 %\n",
      "Epoch 6, Training loss 1.2153509410858154 , Accuracy 52.72 %\n",
      "Epoch 7, Training loss 1.103756807899475 , Accuracy 56.64 %\n",
      "Epoch 8, Training loss 1.0318541956583658 , Accuracy 59.33 %\n",
      "Epoch 9, Training loss 0.9726073985735575 , Accuracy 62.38 %\n",
      "Epoch 10, Training loss 0.931044387404124 , Accuracy 64.05 %\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model_SGD = model_sequential \n",
    "optimizer = optim.SGD(model_SGD.parameters(), lr = 1e-3) \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(comment = 'SGD')\n",
    "training(\n",
    "    n_epochs = 10,\n",
    "    optimizer = optimizer,\n",
    "    model = model_SGD,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    writer = writer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us build another model which we set log softmax as the activation function at the output layer and uses Negative log-likelihood loss function. Compare the results for both of these setting. This time we are going to build by subclassing `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subclassing nn.Module\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(784, 256)\n",
    "        self.act_1 = nn.ReLU()\n",
    "        self.fc_2 = nn.Linear(256, 128)\n",
    "        self.act_2 = nn.ReLU()\n",
    "        self.fc_3 = nn.Linear(128, 64)\n",
    "        self.act_3 = nn.ReLU()\n",
    "        self.fc_4 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.dropout(self.act_1(self.fc_1(x)))\n",
    "        out = self.dropout(self.act_2(self.fc_2(out)))\n",
    "        out = self.dropout(self.act_3(self.fc_3(out)))\n",
    "        # adding in softmax\n",
    "        out = F.log_softmax(self.fc_4(out), dim = 1)\n",
    "        return out\n",
    "    \n",
    "# Or you can use the Pytorch provided functional API when defining the forward method. Both of these are the same.\n",
    "\n",
    "class Classifier_F(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc_1 = nn.Linear(784, 256)\n",
    "        self.fc_2 = nn.Linear(256, 128)\n",
    "        self.fc_3 = nn.Linear(128, 64)\n",
    "        self.fc_4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.dropout(F.relu(self.fc_1(x)), p = 0.2)\n",
    "        out = F.dropout(F.relu(self.fc_2(out)), p = 0.2)\n",
    "        out = F.dropout(F.relu(self.fc_3(out)), p = 0.2)\n",
    "        out = F.log_softmax(self.fc_4(out), dim = 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 2.2894890218098958 , Accuracy 18.39 %\n",
      "Epoch 2, Training loss 2.2399076170603434 , Accuracy 28.73 %\n",
      "Epoch 3, Training loss 2.068951116498311 , Accuracy 29.45 %\n",
      "Epoch 4, Training loss 1.695164651552836 , Accuracy 36.16 %\n",
      "Epoch 5, Training loss 1.4096814838409424 , Accuracy 46.69 %\n",
      "Epoch 6, Training loss 1.2168791191418966 , Accuracy 52.60 %\n",
      "Epoch 7, Training loss 1.1041425074577331 , Accuracy 56.25 %\n",
      "Epoch 8, Training loss 1.0339518047332763 , Accuracy 59.37 %\n",
      "Epoch 9, Training loss 0.9758181870142619 , Accuracy 62.02 %\n",
      "Epoch 10, Training loss 0.9312916868527731 , Accuracy 63.92 %\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "model_SGD = Classifier() \n",
    "optimizer = optim.SGD(model_SGD.parameters(), lr = 1e-3) \n",
    "loss_fn = nn.NLLLoss()\n",
    "writer = SummaryWriter(comment = 'SGD')\n",
    "training(\n",
    "    n_epochs = 10,\n",
    "    optimizer = optimizer,\n",
    "    model = model_SGD,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    writer = writer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CrossEntropy is actually performing log softmax and negative log likelihood at the same time. Therefore during the construction of our model we could neglect the declaration of activation function at the output layer and save some memory during the backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try using other optimizer `Adam` to do our training. Optimizer is one of the hyperparameters that we can tune on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss 0.5945944479823112 , Accuracy 78.32 %\n",
      "Epoch 2, Training loss 0.423241344755888 , Accuracy 84.78 %\n",
      "Epoch 3, Training loss 0.38519719421068827 , Accuracy 86.15 %\n",
      "Epoch 4, Training loss 0.36408053546349206 , Accuracy 86.94 %\n",
      "Epoch 5, Training loss 0.35000673046310743 , Accuracy 87.39 %\n",
      "Epoch 6, Training loss 0.3385574172397455 , Accuracy 87.74 %\n",
      "Epoch 7, Training loss 0.32801985016465185 , Accuracy 88.09 %\n",
      "Epoch 8, Training loss 0.3184917394856612 , Accuracy 88.41 %\n",
      "Epoch 9, Training loss 0.31102090905706087 , Accuracy 88.59 %\n",
      "Epoch 10, Training loss 0.3041634604026874 , Accuracy 88.89 %\n"
     ]
    }
   ],
   "source": [
    "model_Adam = Classifier() \n",
    "optimizer = optim.Adam(model_Adam.parameters(), lr = 1e-3) \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "writer = SummaryWriter(comment = 'Adam')\n",
    "training(\n",
    "    n_epochs = 10,\n",
    "    optimizer = optimizer,\n",
    "    model = model_Adam,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    writer = writer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we can see that `Adam` is performing better than the `SGD` with the same setting. Hyperparameter tuning is very important in order to obtain desired result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Model Saving\n",
    "After training the model, we would like to save it for future usages. There are some pretty useful functions you might need to familar with:\n",
    "\n",
    "- `torch.save`: It serialize the object to save to your machine. Models, tensors, and dictionaries of all kinds of objects can be saved using this function.\n",
    "- `torch.load`: This function uses pickles unpickling facilities to deserialize pickled object files to memory.\n",
    "- `torch.nn.Module.load_state_dict`: Loads a models parameter dictionary using a deserialized state_dict.\n",
    "\n",
    "If you wish to know more on model saving, you can access it at [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving only the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('../generated_model'):\n",
    "    os.mkdir('../generated_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the weights only of the model\n",
    "torch.save(model_Adam.state_dict(),  '../generated_model/mnist_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To load the state_dict, you must have an instance of the model\n",
    "modelLoad = Classifier()\n",
    "modelLoad.load_state_dict(torch.load('../generated_model/mnist_state_dict.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the entire model\n",
    "torch.save(model_Adam, '../generated_model/mnist_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "modelLoad = torch.load('../generated_model/mnist_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add-ons: Saving Model in ONNX format\n",
    "Pytorch also support saving model as ONNX (Open Neural Network Exchange) file type, which is a open format built to represent machine learning models. Let's see how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph(%input : Float(32:784, 784:1),\n",
      "      %fc_1.weight : Float(256:784, 784:1),\n",
      "      %fc_1.bias : Float(256:1),\n",
      "      %fc_2.weight : Float(128:256, 256:1),\n",
      "      %fc_2.bias : Float(128:1),\n",
      "      %fc_3.weight : Float(64:128, 128:1),\n",
      "      %fc_3.bias : Float(64:1),\n",
      "      %fc_4.weight : Float(10:64, 64:1),\n",
      "      %fc_4.bias : Float(10:1)):\n",
      "  %9 : Float(32:256, 256:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%input, %fc_1.weight, %fc_1.bias) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1674:0\n",
      "  %10 : Float(32:256, 256:1) = onnx::Relu(%9) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:973:0\n",
      "  %11 : Float(32:128, 128:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%10, %fc_2.weight, %fc_2.bias) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1674:0\n",
      "  %12 : Float(32:128, 128:1) = onnx::Relu(%11) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:973:0\n",
      "  %13 : Float(32:64, 64:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%12, %fc_3.weight, %fc_3.bias) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1674:0\n",
      "  %14 : Float(32:64, 64:1) = onnx::Relu(%13) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:973:0\n",
      "  %15 : Float(32:10, 10:1) = onnx::Gemm[alpha=1., beta=1., transB=1](%14, %fc_4.weight, %fc_4.bias) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1674:0\n",
      "  %output : Float(32:10, 10:1) = onnx::LogSoftmax[axis=1](%15) # C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torch\\nn\\functional.py:1591:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.onnx \n",
    "dummy_input = torch.randn(32, 784, requires_grad = True)\n",
    "torch.onnx.export(model_Adam, dummy_input, '../generated_model/model.onnx', verbose = True, input_names = ['input'], output_names = ['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import onnx\n",
    "#loading the onnx format model\n",
    "model = onnx.load('../generated_model/model.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Inference\n",
    "Sometimes, we would like to inference on the trained model to evaluate the performance. `model.eval()` will set the model to evaluation(inference) mode to set dropout, batch normalization layers, etc.. to evaluation mode. Evaluation mode will disable the usage of dropout and batch normalization during the `foward` method as it is not required during the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc_1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (act_1): ReLU()\n",
       "  (fc_2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (act_2): ReLU()\n",
       "  (fc_3): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (act_3): ReLU()\n",
       "  (fc_4): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using previous loaded model\n",
    "modelLoad.eval()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting it to inference mode, we could pass in test data with the setting of \n",
    "```python \n",
    "with torch.no_grad():\n",
    "``` \n",
    "as we do not have to calculate the gradient during the inference, this can help us save some memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 88.09 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = modelLoad(images.view(-1, 784))\n",
    "        predictions = torch.max(outputs, 1)[1]\n",
    "        correct += (predictions == labels).sum()\n",
    "        total += len(labels)\n",
    "    accuracy_test = correct.item() * 100 / total\n",
    "print(\"Test Accuracy : {:.2f} %\".format(accuracy_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Build your second Neural Network\n",
    "### 3.3.1 Model Training\n",
    "\n",
    "Altough there are many other machine learning techniques to tackle multi-variate linear regression, it would be interesting for us to tackle it using deep learning for learning purposes.\n",
    "<br>In this sub-section, we will try to perform said regression using PyTorch `SequentialModel` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Real Estate dataset from the `realEstate.csv` for our linear regression example. \n",
    "\n",
    "Description of data:\n",
    "- House Age\n",
    "- Distance from the unit to MRT station\n",
    "- The number of Convenience Stores around the unit\n",
    "- House Unit Price per 1000 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we use pandas to load in the csv.<br>\n",
    "Note that in this dataset there are a total of $3$ features and $1$ label.<br>\n",
    "Thus from the data we will use `.iloc[]` to distinguish the features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/Regression/realEstate.csv\", header = 0)\n",
    "n_features = 3\n",
    "X = data.iloc[:, 0:3].values\n",
    "y = data.iloc[:, 3].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, we split our dataset into 70/30 train/test ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, shuffle = True, random_state = 1022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform feature scaling onto `X_train` and `X_test` using `StandardScaler` from `scikit-learn`.<br>\n",
    "*Note: only fit the train_set but transform both train and test sets*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 3.1, we've touch on how Dataloaders are initialized and used in model training. It was simple, which is to pass in whatever `Dataset` we need into the Dataloader initializer. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are using a custom dataset from a csv file as compared to the previous one which was prepared readily from torchvision. Thus in this case, we will have to build our own by subclassing from `torch.utils.data.Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst subclassing `Dataset`, PyTorch [documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) notes that we have to override the `__getitem__()` method and optionally the `__len__()` method.<br>\n",
    "We will mainly have three methods in this `Dataset` class:\n",
    "- `__init__(self, data, label)`: helps us pass in the feature and labels into the dataset\n",
    "- `__len__(self)`:allows the dataset to know how many instances of data there is \n",
    "- `__getitem__(self, idx)`:allows the dataset to get items from the data and labels by indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Dataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.tensor(features, dtype = torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype  = torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.features.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After feature scaling, we initialize our custom datasets and put them into `Dataloader` constructor and our data is prepared. The next step will be modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Custom_Dataset(X_train, y_train)\n",
    "test_dataset = Custom_Dataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 128 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we previously stated, there are two approaches of modeling.\n",
    "- Subclassing `nn.Module` \n",
    "- Calling the `nn.Sequential()` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.nn.Sequential` is a simple function that accepts a list of `nn.Modules` and returns a model with all the sequential layers. We will be implementing these few layers:\n",
    "1. nn.Linear(3,50)\n",
    "2. nn.ReLU()\n",
    "3. nn.Linear(50,25)\n",
    "4. nn.ReLU()\n",
    "5. nn.Linear(25,10)\n",
    "6. nn.ReLU()\n",
    "7. nn.Linear(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model_sequential = nn.Sequential(nn.Linear(n_features, 50),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(50, 25),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(25, 10),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Linear(10, 1)\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this regression probelm, the loss/criterion we will use is Mean-Squared-Error loss, which in PyTorch is `nn.MSELoss()`<br>\n",
    "We will also choose to use `Adam` as our optimizer.<br> Remember, `torch.optim.*any_optimizer*` accepts `model.parameters()` to keep track of the model's parameters, hence we should always initialize our model first before our optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_sequential.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our modeling is done, let's commence our training with using the training loop that defined previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a wrapper function for our training called `train_model`. This wrapper function will take on parameters:\n",
    "- model\n",
    "- loader\n",
    "- loss_function/criterion\n",
    "- optimizer\n",
    "- number_of_epochs (optional)\n",
    "- iteration_check (optional): *if False is passed in, losses of each iteration per epoch will not be printed>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below will be an overall workings an explaination of our train_model function:\n",
    "1. In each epoch, each minibatch starts with `optimizer.zero_grad()`. This is to clear previously computed gradients from previous minibatches.\n",
    "2. We get the features and labels by indexing our minibatch.\n",
    "3. Compute forward propagation by calling `model(features)` and assigning it to a variable `prediction`\n",
    "4. Compute the loss by calling `criterion(prediction, torch.unsqueeze(labels, dim=1))`\n",
    "    - the reason we unsqueeze is to make sure the shape of the labels are the same as the predictions, which is (batch_size,1) \n",
    "5. Compute backward propagation by calling `loss.backward()`\n",
    "6. Update the parameters(learning rate etc.) of the model by calling `optimizer.step()`\n",
    "7. Increment our `running_loss` with the loss of our current batch\n",
    "8. At the end of each epoch, compute the accuracy by dividing the accumulated loss and the amount of data samples, and finally zero the `running_loss` for the next epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, criterion, optimizer,epochs=5000):\n",
    "#   this running_loss will keep track of the losses of every epoch from each respective iteration\n",
    "    running_loss = 0.0\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        for i, data in enumerate(loader):\n",
    "#           zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            features, labels = data[0],data[1]\n",
    "            prediction = model(features)\n",
    "            loss = criterion(prediction, torch.unsqueeze(labels,dim=1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        if (epoch % 100 == 0 or epoch == 1):\n",
    "            print(f\"Epoch {epoch} Loss: {running_loss / len(loader)}\")     \n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1559.314471435547\n",
      "Epoch 100 Loss: 61.67083594799042\n",
      "Epoch 200 Loss: 57.53920102566481\n",
      "Epoch 300 Loss: 54.614624582976106\n",
      "Epoch 400 Loss: 51.69376365095377\n",
      "Epoch 500 Loss: 49.110941734910014\n",
      "Epoch 600 Loss: 44.46782956123352\n",
      "Epoch 700 Loss: 45.49254035949707\n",
      "Epoch 800 Loss: 45.39475156664848\n",
      "Epoch 900 Loss: 43.348855590820314\n",
      "Epoch 1000 Loss: 42.04828781485558\n",
      "Epoch 1100 Loss: 39.37081394195557\n",
      "Epoch 1200 Loss: 42.60350239276886\n",
      "Epoch 1300 Loss: 38.945985350012776\n",
      "Epoch 1400 Loss: 39.63016664907336\n",
      "Epoch 1500 Loss: 36.81087758541107\n",
      "Epoch 1600 Loss: 34.936926842236424\n",
      "Epoch 1700 Loss: 35.42953658103943\n",
      "Epoch 1800 Loss: 32.789571383502334\n",
      "Epoch 1900 Loss: 34.93219475212682\n",
      "Epoch 2000 Loss: 33.54853103160858\n",
      "Epoch 2100 Loss: 28.336665666103364\n",
      "Epoch 2200 Loss: 25.664763996377587\n",
      "Epoch 2300 Loss: 24.103572607040405\n",
      "Epoch 2400 Loss: 17.353846311569214\n",
      "Epoch 2500 Loss: 15.863344663381577\n",
      "Epoch 2600 Loss: 13.111431193351745\n",
      "Epoch 2700 Loss: 12.318226540088654\n",
      "Epoch 2800 Loss: 19.141652542352677\n",
      "Epoch 2900 Loss: 17.75134304985404\n",
      "Epoch 3000 Loss: 16.94328822637908\n",
      "Epoch 3100 Loss: 18.66891082525253\n",
      "Epoch 3200 Loss: 17.63850952475368\n",
      "Epoch 3300 Loss: 14.46680794209242\n",
      "Epoch 3400 Loss: 21.239836806058882\n",
      "Epoch 3500 Loss: 20.83810586631298\n",
      "Epoch 3600 Loss: 15.835954087972642\n",
      "Epoch 3700 Loss: 22.814937913417815\n",
      "Epoch 3800 Loss: 18.055061160423794\n",
      "Epoch 3900 Loss: 18.024778324365617\n",
      "Epoch 4000 Loss: 19.730439281463624\n",
      "Epoch 4100 Loss: 16.410921066999435\n",
      "Epoch 4200 Loss: 14.806035457924008\n",
      "Epoch 4300 Loss: 12.136985358595847\n",
      "Epoch 4400 Loss: 20.251971996575595\n",
      "Epoch 4500 Loss: 17.372333994880318\n",
      "Epoch 4600 Loss: 18.757385206222533\n",
      "Epoch 4700 Loss: 18.26095001846552\n",
      "Epoch 4800 Loss: 19.056522417068482\n",
      "Epoch 4900 Loss: 22.027928829193115\n",
      "Epoch 5000 Loss: 16.290424835681915\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "train_model(model_sequential, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Inference\n",
    "\n",
    "Now let's evaluate our model. Use `model.eval()` to set the model to inference mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=3, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=25, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=25, out_features=10, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=10, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sequential.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say your house age is 10, distance to MRT is 100 meters, and there are 6 convenience stores around the unit, could you predict your house price? Let's use our trained model to find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction for your house price is : 54032.859802246094\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inference = torch.tensor([[10, 100, 6]])\n",
    "    inference = torch.from_numpy(scaler.transform(inference))\n",
    "    predict = model_sequential.forward(inference.float())\n",
    "        \n",
    "print(\"The prediction for your house price is :\", predict.item() * 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will try to build a classifier for our MNIST Handwriting dataset.\n",
    "\n",
    "Construct transform with the following transforms:\n",
    "- coverting to tensor\n",
    "- normalize the tensor with mean=0.15 and std=0.3081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.15,), (0.3081,))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the MNIST dataset from `torchvision.datasets`. Load them into respective `Dataloaders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ../data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ../data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GuanSheng.Wong\\anaconda3\\envs\\Intro_to_Pytorch\\lib\\site-packages\\torchvision\\datasets\\mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "\n",
    "train = MNIST(\"../data\", download = True, transform = transform, train = True)\n",
    "test = MNIST(\"../data\", download = True, transform = transform, train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, 100, shuffle = True, num_workers = 0)\n",
    "test_loader = DataLoader(test, 100, shuffle = False, num_workers = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare `SummaryWriter` for TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Model with the following layers:\n",
    "- 4 linear/dense layers\n",
    "- First 3 with ReLU activation functions\n",
    "\n",
    "*Note: Remember to resize the incoming tensor first*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features = 28 * 28, out_features = 1000)\n",
    "        self.fc2 = nn.Linear(in_features = 1000, out_features = 500)\n",
    "        self.fc3 = nn.Linear(in_features = 500, out_features = 100)\n",
    "        self.fc4 = nn.Linear(in_features = 100, out_features = 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model and load it to our **GPU**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize criterion: `CrossEntropyLoss` and optimizer `Adam`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a wrapper function `train_model` to train the model using `CUDA`. `add_scalar` which shows a loss against epoch graph on TensorBoard.<br>\n",
    "Here is a checklist for you to keep check what to do:\n",
    "1. For each iteration in each epoch, zero the gradients of the parameters\n",
    "2. Forward propagate\n",
    "3. Calculate loss\n",
    "4. Write the loss and train to TensorBoard\n",
    "5. Back propagate\n",
    "6. Update the parameters\n",
    "7. For each epoch, calculate the accuracy on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \n",
      "Iteration:1 \n",
      "Loss:2.3120129108428955\n",
      "Epoch:1 \n",
      "Iteration:2 \n",
      "Loss:2.243009090423584\n",
      "Epoch:1 \n",
      "Iteration:3 \n",
      "Loss:2.103304862976074\n",
      "Epoch:1 \n",
      "Iteration:4 \n",
      "Loss:1.938184380531311\n",
      "Epoch:1 \n",
      "Iteration:5 \n",
      "Loss:1.7360073328018188\n",
      "Epoch:1 \n",
      "Iteration:6 \n",
      "Loss:1.4185400009155273\n",
      "Epoch:1 \n",
      "Iteration:7 \n",
      "Loss:1.3077017068862915\n",
      "Epoch:1 \n",
      "Iteration:8 \n",
      "Loss:0.979144811630249\n",
      "Epoch:1 \n",
      "Iteration:9 \n",
      "Loss:0.8673616051673889\n",
      "Epoch:1 \n",
      "Iteration:10 \n",
      "Loss:0.7848854660987854\n",
      "Epoch:1 \n",
      "Iteration:11 \n",
      "Loss:0.9053862690925598\n",
      "Epoch:1 \n",
      "Iteration:12 \n",
      "Loss:0.7195755243301392\n",
      "Epoch:1 \n",
      "Iteration:13 \n",
      "Loss:0.8481418490409851\n",
      "Epoch:1 \n",
      "Iteration:14 \n",
      "Loss:0.7561290860176086\n",
      "Epoch:1 \n",
      "Iteration:15 \n",
      "Loss:0.6766089797019958\n",
      "Epoch:1 \n",
      "Iteration:16 \n",
      "Loss:0.5629891157150269\n",
      "Epoch:1 \n",
      "Iteration:17 \n",
      "Loss:0.6399906277656555\n",
      "Epoch:1 \n",
      "Iteration:18 \n",
      "Loss:0.6289631128311157\n",
      "Epoch:1 \n",
      "Iteration:19 \n",
      "Loss:0.7176056504249573\n",
      "Epoch:1 \n",
      "Iteration:20 \n",
      "Loss:0.6771294474601746\n",
      "Epoch:1 \n",
      "Iteration:21 \n",
      "Loss:0.43189147114753723\n",
      "Epoch:1 \n",
      "Iteration:22 \n",
      "Loss:0.5592374801635742\n",
      "Epoch:1 \n",
      "Iteration:23 \n",
      "Loss:0.6598607897758484\n",
      "Epoch:1 \n",
      "Iteration:24 \n",
      "Loss:0.5911179184913635\n",
      "Epoch:1 \n",
      "Iteration:25 \n",
      "Loss:0.5268451571464539\n",
      "Epoch:1 \n",
      "Iteration:26 \n",
      "Loss:0.34881362318992615\n",
      "Epoch:1 \n",
      "Iteration:27 \n",
      "Loss:0.4407460689544678\n",
      "Epoch:1 \n",
      "Iteration:28 \n",
      "Loss:0.3689185380935669\n",
      "Epoch:1 \n",
      "Iteration:29 \n",
      "Loss:0.4429815411567688\n",
      "Epoch:1 \n",
      "Iteration:30 \n",
      "Loss:0.6116948127746582\n",
      "Epoch:1 \n",
      "Iteration:31 \n",
      "Loss:0.6002848148345947\n",
      "Epoch:1 \n",
      "Iteration:32 \n",
      "Loss:0.5611258149147034\n",
      "Epoch:1 \n",
      "Iteration:33 \n",
      "Loss:0.666802167892456\n",
      "Epoch:1 \n",
      "Iteration:34 \n",
      "Loss:0.42656680941581726\n",
      "Epoch:1 \n",
      "Iteration:35 \n",
      "Loss:0.5240041017532349\n",
      "Epoch:1 \n",
      "Iteration:36 \n",
      "Loss:0.29298585653305054\n",
      "Epoch:1 \n",
      "Iteration:37 \n",
      "Loss:0.508000910282135\n",
      "Epoch:1 \n",
      "Iteration:38 \n",
      "Loss:0.46748197078704834\n",
      "Epoch:1 \n",
      "Iteration:39 \n",
      "Loss:0.33440831303596497\n",
      "Epoch:1 \n",
      "Iteration:40 \n",
      "Loss:0.6263558864593506\n",
      "Epoch:1 \n",
      "Iteration:41 \n",
      "Loss:0.3841659128665924\n",
      "Epoch:1 \n",
      "Iteration:42 \n",
      "Loss:0.3341437876224518\n",
      "Epoch:1 \n",
      "Iteration:43 \n",
      "Loss:0.45079100131988525\n",
      "Epoch:1 \n",
      "Iteration:44 \n",
      "Loss:0.44888055324554443\n",
      "Epoch:1 \n",
      "Iteration:45 \n",
      "Loss:0.46881726384162903\n",
      "Epoch:1 \n",
      "Iteration:46 \n",
      "Loss:0.31650739908218384\n",
      "Epoch:1 \n",
      "Iteration:47 \n",
      "Loss:0.39410457015037537\n",
      "Epoch:1 \n",
      "Iteration:48 \n",
      "Loss:0.41094696521759033\n",
      "Epoch:1 \n",
      "Iteration:49 \n",
      "Loss:0.5384355783462524\n",
      "Epoch:1 \n",
      "Iteration:50 \n",
      "Loss:0.333896279335022\n",
      "Epoch:1 \n",
      "Iteration:51 \n",
      "Loss:0.40745270252227783\n",
      "Epoch:1 \n",
      "Iteration:52 \n",
      "Loss:0.37681883573532104\n",
      "Epoch:1 \n",
      "Iteration:53 \n",
      "Loss:0.30641573667526245\n",
      "Epoch:1 \n",
      "Iteration:54 \n",
      "Loss:0.255819708108902\n",
      "Epoch:1 \n",
      "Iteration:55 \n",
      "Loss:0.34836751222610474\n",
      "Epoch:1 \n",
      "Iteration:56 \n",
      "Loss:0.19757182896137238\n",
      "Epoch:1 \n",
      "Iteration:57 \n",
      "Loss:0.3527303636074066\n",
      "Epoch:1 \n",
      "Iteration:58 \n",
      "Loss:0.26464834809303284\n",
      "Epoch:1 \n",
      "Iteration:59 \n",
      "Loss:0.369119256734848\n",
      "Epoch:1 \n",
      "Iteration:60 \n",
      "Loss:0.2571631073951721\n",
      "Epoch:1 \n",
      "Iteration:61 \n",
      "Loss:0.28121277689933777\n",
      "Epoch:1 \n",
      "Iteration:62 \n",
      "Loss:0.2682133913040161\n",
      "Epoch:1 \n",
      "Iteration:63 \n",
      "Loss:0.29948434233665466\n",
      "Epoch:1 \n",
      "Iteration:64 \n",
      "Loss:0.2584401071071625\n",
      "Epoch:1 \n",
      "Iteration:65 \n",
      "Loss:0.1802312135696411\n",
      "Epoch:1 \n",
      "Iteration:66 \n",
      "Loss:0.3833845555782318\n",
      "Epoch:1 \n",
      "Iteration:67 \n",
      "Loss:0.19513483345508575\n",
      "Epoch:1 \n",
      "Iteration:68 \n",
      "Loss:0.271013081073761\n",
      "Epoch:1 \n",
      "Iteration:69 \n",
      "Loss:0.3728995621204376\n",
      "Epoch:1 \n",
      "Iteration:70 \n",
      "Loss:0.33933544158935547\n",
      "Epoch:1 \n",
      "Iteration:71 \n",
      "Loss:0.24964481592178345\n",
      "Epoch:1 \n",
      "Iteration:72 \n",
      "Loss:0.1458766907453537\n",
      "Epoch:1 \n",
      "Iteration:73 \n",
      "Loss:0.2318328619003296\n",
      "Epoch:1 \n",
      "Iteration:74 \n",
      "Loss:0.35721245408058167\n",
      "Epoch:1 \n",
      "Iteration:75 \n",
      "Loss:0.27163660526275635\n",
      "Epoch:1 \n",
      "Iteration:76 \n",
      "Loss:0.2832474410533905\n",
      "Epoch:1 \n",
      "Iteration:77 \n",
      "Loss:0.28073111176490784\n",
      "Epoch:1 \n",
      "Iteration:78 \n",
      "Loss:0.18299219012260437\n",
      "Epoch:1 \n",
      "Iteration:79 \n",
      "Loss:0.19779279828071594\n",
      "Epoch:1 \n",
      "Iteration:80 \n",
      "Loss:0.32478073239326477\n",
      "Epoch:1 \n",
      "Iteration:81 \n",
      "Loss:0.5900135636329651\n",
      "Epoch:1 \n",
      "Iteration:82 \n",
      "Loss:0.25531479716300964\n",
      "Epoch:1 \n",
      "Iteration:83 \n",
      "Loss:0.1387094408273697\n",
      "Epoch:1 \n",
      "Iteration:84 \n",
      "Loss:0.24132877588272095\n",
      "Epoch:1 \n",
      "Iteration:85 \n",
      "Loss:0.30440443754196167\n",
      "Epoch:1 \n",
      "Iteration:86 \n",
      "Loss:0.36016371846199036\n",
      "Epoch:1 \n",
      "Iteration:87 \n",
      "Loss:0.2922019362449646\n",
      "Epoch:1 \n",
      "Iteration:88 \n",
      "Loss:0.19534306228160858\n",
      "Epoch:1 \n",
      "Iteration:89 \n",
      "Loss:0.25497016310691833\n",
      "Epoch:1 \n",
      "Iteration:90 \n",
      "Loss:0.31121373176574707\n",
      "Epoch:1 \n",
      "Iteration:91 \n",
      "Loss:0.19418592751026154\n",
      "Epoch:1 \n",
      "Iteration:92 \n",
      "Loss:0.31320226192474365\n",
      "Epoch:1 \n",
      "Iteration:93 \n",
      "Loss:0.17160598933696747\n",
      "Epoch:1 \n",
      "Iteration:94 \n",
      "Loss:0.3011602759361267\n",
      "Epoch:1 \n",
      "Iteration:95 \n",
      "Loss:0.2664591073989868\n",
      "Epoch:1 \n",
      "Iteration:96 \n",
      "Loss:0.34600746631622314\n",
      "Epoch:1 \n",
      "Iteration:97 \n",
      "Loss:0.15365272760391235\n",
      "Epoch:1 \n",
      "Iteration:98 \n",
      "Loss:0.3775344789028168\n",
      "Epoch:1 \n",
      "Iteration:99 \n",
      "Loss:0.2457958608865738\n",
      "Epoch:1 \n",
      "Iteration:100 \n",
      "Loss:0.20113806426525116\n",
      "Epoch:1 \n",
      "Iteration:101 \n",
      "Loss:0.2597142457962036\n",
      "Epoch:1 \n",
      "Iteration:102 \n",
      "Loss:0.3785651922225952\n",
      "Epoch:1 \n",
      "Iteration:103 \n",
      "Loss:0.2910976707935333\n",
      "Epoch:1 \n",
      "Iteration:104 \n",
      "Loss:0.32003748416900635\n",
      "Epoch:1 \n",
      "Iteration:105 \n",
      "Loss:0.3702780604362488\n",
      "Epoch:1 \n",
      "Iteration:106 \n",
      "Loss:0.2612571716308594\n",
      "Epoch:1 \n",
      "Iteration:107 \n",
      "Loss:0.20184503495693207\n",
      "Epoch:1 \n",
      "Iteration:108 \n",
      "Loss:0.30470114946365356\n",
      "Epoch:1 \n",
      "Iteration:109 \n",
      "Loss:0.2566049098968506\n",
      "Epoch:1 \n",
      "Iteration:110 \n",
      "Loss:0.30411121249198914\n",
      "Epoch:1 \n",
      "Iteration:111 \n",
      "Loss:0.28380855917930603\n",
      "Epoch:1 \n",
      "Iteration:112 \n",
      "Loss:0.3514578342437744\n",
      "Epoch:1 \n",
      "Iteration:113 \n",
      "Loss:0.3024035096168518\n",
      "Epoch:1 \n",
      "Iteration:114 \n",
      "Loss:0.3007325232028961\n",
      "Epoch:1 \n",
      "Iteration:115 \n",
      "Loss:0.23290877044200897\n",
      "Epoch:1 \n",
      "Iteration:116 \n",
      "Loss:0.3148707449436188\n",
      "Epoch:1 \n",
      "Iteration:117 \n",
      "Loss:0.1848905235528946\n",
      "Epoch:1 \n",
      "Iteration:118 \n",
      "Loss:0.15175089240074158\n",
      "Epoch:1 \n",
      "Iteration:119 \n",
      "Loss:0.21671690046787262\n",
      "Epoch:1 \n",
      "Iteration:120 \n",
      "Loss:0.2821066379547119\n",
      "Epoch:1 \n",
      "Iteration:121 \n",
      "Loss:0.25291892886161804\n",
      "Epoch:1 \n",
      "Iteration:122 \n",
      "Loss:0.21602274477481842\n",
      "Epoch:1 \n",
      "Iteration:123 \n",
      "Loss:0.1897137463092804\n",
      "Epoch:1 \n",
      "Iteration:124 \n",
      "Loss:0.18207958340644836\n",
      "Epoch:1 \n",
      "Iteration:125 \n",
      "Loss:0.22877010703086853\n",
      "Epoch:1 \n",
      "Iteration:126 \n",
      "Loss:0.24930299818515778\n",
      "Epoch:1 \n",
      "Iteration:127 \n",
      "Loss:0.2413063496351242\n",
      "Epoch:1 \n",
      "Iteration:128 \n",
      "Loss:0.24214984476566315\n",
      "Epoch:1 \n",
      "Iteration:129 \n",
      "Loss:0.30584508180618286\n",
      "Epoch:1 \n",
      "Iteration:130 \n",
      "Loss:0.13950319588184357\n",
      "Epoch:1 \n",
      "Iteration:131 \n",
      "Loss:0.3294107913970947\n",
      "Epoch:1 \n",
      "Iteration:132 \n",
      "Loss:0.28691741824150085\n",
      "Epoch:1 \n",
      "Iteration:133 \n",
      "Loss:0.13431206345558167\n",
      "Epoch:1 \n",
      "Iteration:134 \n",
      "Loss:0.22785593569278717\n",
      "Epoch:1 \n",
      "Iteration:135 \n",
      "Loss:0.2721596360206604\n",
      "Epoch:1 \n",
      "Iteration:136 \n",
      "Loss:0.13127829134464264\n",
      "Epoch:1 \n",
      "Iteration:137 \n",
      "Loss:0.252728670835495\n",
      "Epoch:1 \n",
      "Iteration:138 \n",
      "Loss:0.2905021011829376\n",
      "Epoch:1 \n",
      "Iteration:139 \n",
      "Loss:0.22047342360019684\n",
      "Epoch:1 \n",
      "Iteration:140 \n",
      "Loss:0.48403823375701904\n",
      "Epoch:1 \n",
      "Iteration:141 \n",
      "Loss:0.15702365338802338\n",
      "Epoch:1 \n",
      "Iteration:142 \n",
      "Loss:0.2312907576560974\n",
      "Epoch:1 \n",
      "Iteration:143 \n",
      "Loss:0.319017618894577\n",
      "Epoch:1 \n",
      "Iteration:144 \n",
      "Loss:0.30859729647636414\n",
      "Epoch:1 \n",
      "Iteration:145 \n",
      "Loss:0.255501389503479\n",
      "Epoch:1 \n",
      "Iteration:146 \n",
      "Loss:0.19951361417770386\n",
      "Epoch:1 \n",
      "Iteration:147 \n",
      "Loss:0.15566644072532654\n",
      "Epoch:1 \n",
      "Iteration:148 \n",
      "Loss:0.291554719209671\n",
      "Epoch:1 \n",
      "Iteration:149 \n",
      "Loss:0.2254704236984253\n",
      "Epoch:1 \n",
      "Iteration:150 \n",
      "Loss:0.3103751838207245\n",
      "Epoch:1 \n",
      "Iteration:151 \n",
      "Loss:0.23330779373645782\n",
      "Epoch:1 \n",
      "Iteration:152 \n",
      "Loss:0.19241328537464142\n",
      "Epoch:1 \n",
      "Iteration:153 \n",
      "Loss:0.275493860244751\n",
      "Epoch:1 \n",
      "Iteration:154 \n",
      "Loss:0.16837461292743683\n",
      "Epoch:1 \n",
      "Iteration:155 \n",
      "Loss:0.19802793860435486\n",
      "Epoch:1 \n",
      "Iteration:156 \n",
      "Loss:0.3491751551628113\n",
      "Epoch:1 \n",
      "Iteration:157 \n",
      "Loss:0.43757039308547974\n",
      "Epoch:1 \n",
      "Iteration:158 \n",
      "Loss:0.1347665637731552\n",
      "Epoch:1 \n",
      "Iteration:159 \n",
      "Loss:0.06288599222898483\n",
      "Epoch:1 \n",
      "Iteration:160 \n",
      "Loss:0.3957083523273468\n",
      "Epoch:1 \n",
      "Iteration:161 \n",
      "Loss:0.3086799383163452\n",
      "Epoch:1 \n",
      "Iteration:162 \n",
      "Loss:0.12891536951065063\n",
      "Epoch:1 \n",
      "Iteration:163 \n",
      "Loss:0.1177622377872467\n",
      "Epoch:1 \n",
      "Iteration:164 \n",
      "Loss:0.08659529685974121\n",
      "Epoch:1 \n",
      "Iteration:165 \n",
      "Loss:0.21118642389774323\n",
      "Epoch:1 \n",
      "Iteration:166 \n",
      "Loss:0.2700453996658325\n",
      "Epoch:1 \n",
      "Iteration:167 \n",
      "Loss:0.31985050439834595\n",
      "Epoch:1 \n",
      "Iteration:168 \n",
      "Loss:0.19454088807106018\n",
      "Epoch:1 \n",
      "Iteration:169 \n",
      "Loss:0.19190970063209534\n",
      "Epoch:1 \n",
      "Iteration:170 \n",
      "Loss:0.26456591486930847\n",
      "Epoch:1 \n",
      "Iteration:171 \n",
      "Loss:0.27670493721961975\n",
      "Epoch:1 \n",
      "Iteration:172 \n",
      "Loss:0.31067776679992676\n",
      "Epoch:1 \n",
      "Iteration:173 \n",
      "Loss:0.2188132256269455\n",
      "Epoch:1 \n",
      "Iteration:174 \n",
      "Loss:0.23364071547985077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \n",
      "Iteration:175 \n",
      "Loss:0.1508840024471283\n",
      "Epoch:1 \n",
      "Iteration:176 \n",
      "Loss:0.23261654376983643\n",
      "Epoch:1 \n",
      "Iteration:177 \n",
      "Loss:0.23869295418262482\n",
      "Epoch:1 \n",
      "Iteration:178 \n",
      "Loss:0.18907514214515686\n",
      "Epoch:1 \n",
      "Iteration:179 \n",
      "Loss:0.16858088970184326\n",
      "Epoch:1 \n",
      "Iteration:180 \n",
      "Loss:0.3811591863632202\n",
      "Epoch:1 \n",
      "Iteration:181 \n",
      "Loss:0.124379463493824\n",
      "Epoch:1 \n",
      "Iteration:182 \n",
      "Loss:0.31516680121421814\n",
      "Epoch:1 \n",
      "Iteration:183 \n",
      "Loss:0.31521129608154297\n",
      "Epoch:1 \n",
      "Iteration:184 \n",
      "Loss:0.1377878487110138\n",
      "Epoch:1 \n",
      "Iteration:185 \n",
      "Loss:0.18656201660633087\n",
      "Epoch:1 \n",
      "Iteration:186 \n",
      "Loss:0.1332310140132904\n",
      "Epoch:1 \n",
      "Iteration:187 \n",
      "Loss:0.19603176414966583\n",
      "Epoch:1 \n",
      "Iteration:188 \n",
      "Loss:0.19425024092197418\n",
      "Epoch:1 \n",
      "Iteration:189 \n",
      "Loss:0.13017690181732178\n",
      "Epoch:1 \n",
      "Iteration:190 \n",
      "Loss:0.13372930884361267\n",
      "Epoch:1 \n",
      "Iteration:191 \n",
      "Loss:0.1804359406232834\n",
      "Epoch:1 \n",
      "Iteration:192 \n",
      "Loss:0.2930707633495331\n",
      "Epoch:1 \n",
      "Iteration:193 \n",
      "Loss:0.14517799019813538\n",
      "Epoch:1 \n",
      "Iteration:194 \n",
      "Loss:0.32116764783859253\n",
      "Epoch:1 \n",
      "Iteration:195 \n",
      "Loss:0.14592529833316803\n",
      "Epoch:1 \n",
      "Iteration:196 \n",
      "Loss:0.1323719173669815\n",
      "Epoch:1 \n",
      "Iteration:197 \n",
      "Loss:0.23956510424613953\n",
      "Epoch:1 \n",
      "Iteration:198 \n",
      "Loss:0.1127374917268753\n",
      "Epoch:1 \n",
      "Iteration:199 \n",
      "Loss:0.1258353739976883\n",
      "Epoch:1 \n",
      "Iteration:200 \n",
      "Loss:0.13518422842025757\n",
      "Epoch:1 \n",
      "Iteration:201 \n",
      "Loss:0.19253356754779816\n",
      "Epoch:1 \n",
      "Iteration:202 \n",
      "Loss:0.2624629735946655\n",
      "Epoch:1 \n",
      "Iteration:203 \n",
      "Loss:0.19795076549053192\n",
      "Epoch:1 \n",
      "Iteration:204 \n",
      "Loss:0.22124363481998444\n",
      "Epoch:1 \n",
      "Iteration:205 \n",
      "Loss:0.13537326455116272\n",
      "Epoch:1 \n",
      "Iteration:206 \n",
      "Loss:0.07512052357196808\n",
      "Epoch:1 \n",
      "Iteration:207 \n",
      "Loss:0.3330065608024597\n",
      "Epoch:1 \n",
      "Iteration:208 \n",
      "Loss:0.30471986532211304\n",
      "Epoch:1 \n",
      "Iteration:209 \n",
      "Loss:0.16882148385047913\n",
      "Epoch:1 \n",
      "Iteration:210 \n",
      "Loss:0.27691519260406494\n",
      "Epoch:1 \n",
      "Iteration:211 \n",
      "Loss:0.1952899694442749\n",
      "Epoch:1 \n",
      "Iteration:212 \n",
      "Loss:0.19514437019824982\n",
      "Epoch:1 \n",
      "Iteration:213 \n",
      "Loss:0.2352014183998108\n",
      "Epoch:1 \n",
      "Iteration:214 \n",
      "Loss:0.12370365113019943\n",
      "Epoch:1 \n",
      "Iteration:215 \n",
      "Loss:0.274437814950943\n",
      "Epoch:1 \n",
      "Iteration:216 \n",
      "Loss:0.06159636005759239\n",
      "Epoch:1 \n",
      "Iteration:217 \n",
      "Loss:0.32039517164230347\n",
      "Epoch:1 \n",
      "Iteration:218 \n",
      "Loss:0.1663617342710495\n",
      "Epoch:1 \n",
      "Iteration:219 \n",
      "Loss:0.1241391971707344\n",
      "Epoch:1 \n",
      "Iteration:220 \n",
      "Loss:0.2916605472564697\n",
      "Epoch:1 \n",
      "Iteration:221 \n",
      "Loss:0.26142963767051697\n",
      "Epoch:1 \n",
      "Iteration:222 \n",
      "Loss:0.1519048511981964\n",
      "Epoch:1 \n",
      "Iteration:223 \n",
      "Loss:0.16335809230804443\n",
      "Epoch:1 \n",
      "Iteration:224 \n",
      "Loss:0.07499317079782486\n",
      "Epoch:1 \n",
      "Iteration:225 \n",
      "Loss:0.11926721781492233\n",
      "Epoch:1 \n",
      "Iteration:226 \n",
      "Loss:0.1341986507177353\n",
      "Epoch:1 \n",
      "Iteration:227 \n",
      "Loss:0.21553756296634674\n",
      "Epoch:1 \n",
      "Iteration:228 \n",
      "Loss:0.2085375338792801\n",
      "Epoch:1 \n",
      "Iteration:229 \n",
      "Loss:0.13790622353553772\n",
      "Epoch:1 \n",
      "Iteration:230 \n",
      "Loss:0.14417821168899536\n",
      "Epoch:1 \n",
      "Iteration:231 \n",
      "Loss:0.13521558046340942\n",
      "Epoch:1 \n",
      "Iteration:232 \n",
      "Loss:0.25427722930908203\n",
      "Epoch:1 \n",
      "Iteration:233 \n",
      "Loss:0.061873696744441986\n",
      "Epoch:1 \n",
      "Iteration:234 \n",
      "Loss:0.12129952758550644\n",
      "Epoch:1 \n",
      "Iteration:235 \n",
      "Loss:0.08585207164287567\n",
      "Epoch:1 \n",
      "Iteration:236 \n",
      "Loss:0.09578448534011841\n",
      "Epoch:1 \n",
      "Iteration:237 \n",
      "Loss:0.24739769101142883\n",
      "Epoch:1 \n",
      "Iteration:238 \n",
      "Loss:0.20082828402519226\n",
      "Epoch:1 \n",
      "Iteration:239 \n",
      "Loss:0.16660286486148834\n",
      "Epoch:1 \n",
      "Iteration:240 \n",
      "Loss:0.12448081374168396\n",
      "Epoch:1 \n",
      "Iteration:241 \n",
      "Loss:0.1126178577542305\n",
      "Epoch:1 \n",
      "Iteration:242 \n",
      "Loss:0.1494017094373703\n",
      "Epoch:1 \n",
      "Iteration:243 \n",
      "Loss:0.061474427580833435\n",
      "Epoch:1 \n",
      "Iteration:244 \n",
      "Loss:0.08451125025749207\n",
      "Epoch:1 \n",
      "Iteration:245 \n",
      "Loss:0.17013268172740936\n",
      "Epoch:1 \n",
      "Iteration:246 \n",
      "Loss:0.18701007962226868\n",
      "Epoch:1 \n",
      "Iteration:247 \n",
      "Loss:0.16942645609378815\n",
      "Epoch:1 \n",
      "Iteration:248 \n",
      "Loss:0.13958017528057098\n",
      "Epoch:1 \n",
      "Iteration:249 \n",
      "Loss:0.084477499127388\n",
      "Epoch:1 \n",
      "Iteration:250 \n",
      "Loss:0.1355639100074768\n",
      "Epoch:1 \n",
      "Iteration:251 \n",
      "Loss:0.22790712118148804\n",
      "Epoch:1 \n",
      "Iteration:252 \n",
      "Loss:0.16315774619579315\n",
      "Epoch:1 \n",
      "Iteration:253 \n",
      "Loss:0.16369369626045227\n",
      "Epoch:1 \n",
      "Iteration:254 \n",
      "Loss:0.1281852126121521\n",
      "Epoch:1 \n",
      "Iteration:255 \n",
      "Loss:0.34571897983551025\n",
      "Epoch:1 \n",
      "Iteration:256 \n",
      "Loss:0.21460860967636108\n",
      "Epoch:1 \n",
      "Iteration:257 \n",
      "Loss:0.3529081344604492\n",
      "Epoch:1 \n",
      "Iteration:258 \n",
      "Loss:0.14152777194976807\n",
      "Epoch:1 \n",
      "Iteration:259 \n",
      "Loss:0.20369009673595428\n",
      "Epoch:1 \n",
      "Iteration:260 \n",
      "Loss:0.11168790608644485\n",
      "Epoch:1 \n",
      "Iteration:261 \n",
      "Loss:0.20936529338359833\n",
      "Epoch:1 \n",
      "Iteration:262 \n",
      "Loss:0.1992996633052826\n",
      "Epoch:1 \n",
      "Iteration:263 \n",
      "Loss:0.2548293173313141\n",
      "Epoch:1 \n",
      "Iteration:264 \n",
      "Loss:0.22339364886283875\n",
      "Epoch:1 \n",
      "Iteration:265 \n",
      "Loss:0.2226775586605072\n",
      "Epoch:1 \n",
      "Iteration:266 \n",
      "Loss:0.15242652595043182\n",
      "Epoch:1 \n",
      "Iteration:267 \n",
      "Loss:0.11700218170881271\n",
      "Epoch:1 \n",
      "Iteration:268 \n",
      "Loss:0.07112956047058105\n",
      "Epoch:1 \n",
      "Iteration:269 \n",
      "Loss:0.16115371882915497\n",
      "Epoch:1 \n",
      "Iteration:270 \n",
      "Loss:0.09328000247478485\n",
      "Epoch:1 \n",
      "Iteration:271 \n",
      "Loss:0.11228461563587189\n",
      "Epoch:1 \n",
      "Iteration:272 \n",
      "Loss:0.09753935039043427\n",
      "Epoch:1 \n",
      "Iteration:273 \n",
      "Loss:0.08504918962717056\n",
      "Epoch:1 \n",
      "Iteration:274 \n",
      "Loss:0.18780617415905\n",
      "Epoch:1 \n",
      "Iteration:275 \n",
      "Loss:0.12763231992721558\n",
      "Epoch:1 \n",
      "Iteration:276 \n",
      "Loss:0.15267400443553925\n",
      "Epoch:1 \n",
      "Iteration:277 \n",
      "Loss:0.1715623140335083\n",
      "Epoch:1 \n",
      "Iteration:278 \n",
      "Loss:0.18402642011642456\n",
      "Epoch:1 \n",
      "Iteration:279 \n",
      "Loss:0.21424001455307007\n",
      "Epoch:1 \n",
      "Iteration:280 \n",
      "Loss:0.08497539162635803\n",
      "Epoch:1 \n",
      "Iteration:281 \n",
      "Loss:0.1401849389076233\n",
      "Epoch:1 \n",
      "Iteration:282 \n",
      "Loss:0.13807371258735657\n",
      "Epoch:1 \n",
      "Iteration:283 \n",
      "Loss:0.15613220632076263\n",
      "Epoch:1 \n",
      "Iteration:284 \n",
      "Loss:0.17785942554473877\n",
      "Epoch:1 \n",
      "Iteration:285 \n",
      "Loss:0.29615554213523865\n",
      "Epoch:1 \n",
      "Iteration:286 \n",
      "Loss:0.2421244978904724\n",
      "Epoch:1 \n",
      "Iteration:287 \n",
      "Loss:0.17601080238819122\n",
      "Epoch:1 \n",
      "Iteration:288 \n",
      "Loss:0.16622912883758545\n",
      "Epoch:1 \n",
      "Iteration:289 \n",
      "Loss:0.32603874802589417\n",
      "Epoch:1 \n",
      "Iteration:290 \n",
      "Loss:0.1167239248752594\n",
      "Epoch:1 \n",
      "Iteration:291 \n",
      "Loss:0.1310199797153473\n",
      "Epoch:1 \n",
      "Iteration:292 \n",
      "Loss:0.14816546440124512\n",
      "Epoch:1 \n",
      "Iteration:293 \n",
      "Loss:0.1895904541015625\n",
      "Epoch:1 \n",
      "Iteration:294 \n",
      "Loss:0.17430013418197632\n",
      "Epoch:1 \n",
      "Iteration:295 \n",
      "Loss:0.30275866389274597\n",
      "Epoch:1 \n",
      "Iteration:296 \n",
      "Loss:0.1549168825149536\n",
      "Epoch:1 \n",
      "Iteration:297 \n",
      "Loss:0.2380707859992981\n",
      "Epoch:1 \n",
      "Iteration:298 \n",
      "Loss:0.11344513297080994\n",
      "Epoch:1 \n",
      "Iteration:299 \n",
      "Loss:0.2677982449531555\n",
      "Epoch:1 \n",
      "Iteration:300 \n",
      "Loss:0.17318986356258392\n",
      "Epoch:1 \n",
      "Iteration:301 \n",
      "Loss:0.16743168234825134\n",
      "Epoch:1 \n",
      "Iteration:302 \n",
      "Loss:0.05891239270567894\n",
      "Epoch:1 \n",
      "Iteration:303 \n",
      "Loss:0.1507492959499359\n",
      "Epoch:1 \n",
      "Iteration:304 \n",
      "Loss:0.19562767446041107\n",
      "Epoch:1 \n",
      "Iteration:305 \n",
      "Loss:0.04008667171001434\n",
      "Epoch:1 \n",
      "Iteration:306 \n",
      "Loss:0.1803441196680069\n",
      "Epoch:1 \n",
      "Iteration:307 \n",
      "Loss:0.19751280546188354\n",
      "Epoch:1 \n",
      "Iteration:308 \n",
      "Loss:0.18159769475460052\n",
      "Epoch:1 \n",
      "Iteration:309 \n",
      "Loss:0.09003745764493942\n",
      "Epoch:1 \n",
      "Iteration:310 \n",
      "Loss:0.17185933887958527\n",
      "Epoch:1 \n",
      "Iteration:311 \n",
      "Loss:0.10974127799272537\n",
      "Epoch:1 \n",
      "Iteration:312 \n",
      "Loss:0.3070503771305084\n",
      "Epoch:1 \n",
      "Iteration:313 \n",
      "Loss:0.1234973594546318\n",
      "Epoch:1 \n",
      "Iteration:314 \n",
      "Loss:0.13988421857357025\n",
      "Epoch:1 \n",
      "Iteration:315 \n",
      "Loss:0.1775360107421875\n",
      "Epoch:1 \n",
      "Iteration:316 \n",
      "Loss:0.14908771216869354\n",
      "Epoch:1 \n",
      "Iteration:317 \n",
      "Loss:0.08358968794345856\n",
      "Epoch:1 \n",
      "Iteration:318 \n",
      "Loss:0.16719035804271698\n",
      "Epoch:1 \n",
      "Iteration:319 \n",
      "Loss:0.13801692426204681\n",
      "Epoch:1 \n",
      "Iteration:320 \n",
      "Loss:0.15283483266830444\n",
      "Epoch:1 \n",
      "Iteration:321 \n",
      "Loss:0.09023714065551758\n",
      "Epoch:1 \n",
      "Iteration:322 \n",
      "Loss:0.12085916846990585\n",
      "Epoch:1 \n",
      "Iteration:323 \n",
      "Loss:0.19208984076976776\n",
      "Epoch:1 \n",
      "Iteration:324 \n",
      "Loss:0.10534988343715668\n",
      "Epoch:1 \n",
      "Iteration:325 \n",
      "Loss:0.09955122321844101\n",
      "Epoch:1 \n",
      "Iteration:326 \n",
      "Loss:0.10314197838306427\n",
      "Epoch:1 \n",
      "Iteration:327 \n",
      "Loss:0.19099943339824677\n",
      "Epoch:1 \n",
      "Iteration:328 \n",
      "Loss:0.15097354352474213\n",
      "Epoch:1 \n",
      "Iteration:329 \n",
      "Loss:0.05294886603951454\n",
      "Epoch:1 \n",
      "Iteration:330 \n",
      "Loss:0.1789417862892151\n",
      "Epoch:1 \n",
      "Iteration:331 \n",
      "Loss:0.13349762558937073\n",
      "Epoch:1 \n",
      "Iteration:332 \n",
      "Loss:0.08608779311180115\n",
      "Epoch:1 \n",
      "Iteration:333 \n",
      "Loss:0.2064773440361023\n",
      "Epoch:1 \n",
      "Iteration:334 \n",
      "Loss:0.17000742256641388\n",
      "Epoch:1 \n",
      "Iteration:335 \n",
      "Loss:0.3031696379184723\n",
      "Epoch:1 \n",
      "Iteration:336 \n",
      "Loss:0.1303916722536087\n",
      "Epoch:1 \n",
      "Iteration:337 \n",
      "Loss:0.15825255215168\n",
      "Epoch:1 \n",
      "Iteration:338 \n",
      "Loss:0.24353379011154175\n",
      "Epoch:1 \n",
      "Iteration:339 \n",
      "Loss:0.23819094896316528\n",
      "Epoch:1 \n",
      "Iteration:340 \n",
      "Loss:0.23188480734825134\n",
      "Epoch:1 \n",
      "Iteration:341 \n",
      "Loss:0.23926052451133728\n",
      "Epoch:1 \n",
      "Iteration:342 \n",
      "Loss:0.1374325305223465\n",
      "Epoch:1 \n",
      "Iteration:343 \n",
      "Loss:0.12234268337488174\n",
      "Epoch:1 \n",
      "Iteration:344 \n",
      "Loss:0.08939297497272491\n",
      "Epoch:1 \n",
      "Iteration:345 \n",
      "Loss:0.1573348492383957\n",
      "Epoch:1 \n",
      "Iteration:346 \n",
      "Loss:0.04774055629968643\n",
      "Epoch:1 \n",
      "Iteration:347 \n",
      "Loss:0.31348198652267456\n",
      "Epoch:1 \n",
      "Iteration:348 \n",
      "Loss:0.12617792189121246\n",
      "Epoch:1 \n",
      "Iteration:349 \n",
      "Loss:0.12596957385540009\n",
      "Epoch:1 \n",
      "Iteration:350 \n",
      "Loss:0.23239940404891968\n",
      "Epoch:1 \n",
      "Iteration:351 \n",
      "Loss:0.22754289209842682\n",
      "Epoch:1 \n",
      "Iteration:352 \n",
      "Loss:0.10897237807512283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \n",
      "Iteration:353 \n",
      "Loss:0.12211742252111435\n",
      "Epoch:1 \n",
      "Iteration:354 \n",
      "Loss:0.17609861493110657\n",
      "Epoch:1 \n",
      "Iteration:355 \n",
      "Loss:0.16741515696048737\n",
      "Epoch:1 \n",
      "Iteration:356 \n",
      "Loss:0.1579798460006714\n",
      "Epoch:1 \n",
      "Iteration:357 \n",
      "Loss:0.1322966068983078\n",
      "Epoch:1 \n",
      "Iteration:358 \n",
      "Loss:0.17488032579421997\n",
      "Epoch:1 \n",
      "Iteration:359 \n",
      "Loss:0.16777494549751282\n",
      "Epoch:1 \n",
      "Iteration:360 \n",
      "Loss:0.14855535328388214\n",
      "Epoch:1 \n",
      "Iteration:361 \n",
      "Loss:0.22715777158737183\n",
      "Epoch:1 \n",
      "Iteration:362 \n",
      "Loss:0.15137185156345367\n",
      "Epoch:1 \n",
      "Iteration:363 \n",
      "Loss:0.3162267804145813\n",
      "Epoch:1 \n",
      "Iteration:364 \n",
      "Loss:0.1435621976852417\n",
      "Epoch:1 \n",
      "Iteration:365 \n",
      "Loss:0.11869576573371887\n",
      "Epoch:1 \n",
      "Iteration:366 \n",
      "Loss:0.06551741808652878\n",
      "Epoch:1 \n",
      "Iteration:367 \n",
      "Loss:0.12392859160900116\n",
      "Epoch:1 \n",
      "Iteration:368 \n",
      "Loss:0.1441977471113205\n",
      "Epoch:1 \n",
      "Iteration:369 \n",
      "Loss:0.17136967182159424\n",
      "Epoch:1 \n",
      "Iteration:370 \n",
      "Loss:0.3830975592136383\n",
      "Epoch:1 \n",
      "Iteration:371 \n",
      "Loss:0.12549568712711334\n",
      "Epoch:1 \n",
      "Iteration:372 \n",
      "Loss:0.09441132843494415\n",
      "Epoch:1 \n",
      "Iteration:373 \n",
      "Loss:0.11308066546916962\n",
      "Epoch:1 \n",
      "Iteration:374 \n",
      "Loss:0.2946067750453949\n",
      "Epoch:1 \n",
      "Iteration:375 \n",
      "Loss:0.30554139614105225\n",
      "Epoch:1 \n",
      "Iteration:376 \n",
      "Loss:0.29851454496383667\n",
      "Epoch:1 \n",
      "Iteration:377 \n",
      "Loss:0.18765129148960114\n",
      "Epoch:1 \n",
      "Iteration:378 \n",
      "Loss:0.14548975229263306\n",
      "Epoch:1 \n",
      "Iteration:379 \n",
      "Loss:0.23775292932987213\n",
      "Epoch:1 \n",
      "Iteration:380 \n",
      "Loss:0.16772422194480896\n",
      "Epoch:1 \n",
      "Iteration:381 \n",
      "Loss:0.16343961656093597\n",
      "Epoch:1 \n",
      "Iteration:382 \n",
      "Loss:0.0664227083325386\n",
      "Epoch:1 \n",
      "Iteration:383 \n",
      "Loss:0.251888632774353\n",
      "Epoch:1 \n",
      "Iteration:384 \n",
      "Loss:0.17794324457645416\n",
      "Epoch:1 \n",
      "Iteration:385 \n",
      "Loss:0.14521580934524536\n",
      "Epoch:1 \n",
      "Iteration:386 \n",
      "Loss:0.12497042864561081\n",
      "Epoch:1 \n",
      "Iteration:387 \n",
      "Loss:0.2176038920879364\n",
      "Epoch:1 \n",
      "Iteration:388 \n",
      "Loss:0.12465760856866837\n",
      "Epoch:1 \n",
      "Iteration:389 \n",
      "Loss:0.08923342823982239\n",
      "Epoch:1 \n",
      "Iteration:390 \n",
      "Loss:0.23708225786685944\n",
      "Epoch:1 \n",
      "Iteration:391 \n",
      "Loss:0.18922735750675201\n",
      "Epoch:1 \n",
      "Iteration:392 \n",
      "Loss:0.07804815471172333\n",
      "Epoch:1 \n",
      "Iteration:393 \n",
      "Loss:0.18437808752059937\n",
      "Epoch:1 \n",
      "Iteration:394 \n",
      "Loss:0.13805130124092102\n",
      "Epoch:1 \n",
      "Iteration:395 \n",
      "Loss:0.16958487033843994\n",
      "Epoch:1 \n",
      "Iteration:396 \n",
      "Loss:0.16531363129615784\n",
      "Epoch:1 \n",
      "Iteration:397 \n",
      "Loss:0.12704482674598694\n",
      "Epoch:1 \n",
      "Iteration:398 \n",
      "Loss:0.1614457368850708\n",
      "Epoch:1 \n",
      "Iteration:399 \n",
      "Loss:0.22057953476905823\n",
      "Epoch:1 \n",
      "Iteration:400 \n",
      "Loss:0.14540232717990875\n",
      "Epoch:1 \n",
      "Iteration:401 \n",
      "Loss:0.0864008367061615\n",
      "Epoch:1 \n",
      "Iteration:402 \n",
      "Loss:0.14857891201972961\n",
      "Epoch:1 \n",
      "Iteration:403 \n",
      "Loss:0.07551080733537674\n",
      "Epoch:1 \n",
      "Iteration:404 \n",
      "Loss:0.14636777341365814\n",
      "Epoch:1 \n",
      "Iteration:405 \n",
      "Loss:0.22663001716136932\n",
      "Epoch:1 \n",
      "Iteration:406 \n",
      "Loss:0.1538436859846115\n",
      "Epoch:1 \n",
      "Iteration:407 \n",
      "Loss:0.09959450364112854\n",
      "Epoch:1 \n",
      "Iteration:408 \n",
      "Loss:0.24725183844566345\n",
      "Epoch:1 \n",
      "Iteration:409 \n",
      "Loss:0.27957770228385925\n",
      "Epoch:1 \n",
      "Iteration:410 \n",
      "Loss:0.07834064960479736\n",
      "Epoch:1 \n",
      "Iteration:411 \n",
      "Loss:0.14370116591453552\n",
      "Epoch:1 \n",
      "Iteration:412 \n",
      "Loss:0.20632970333099365\n",
      "Epoch:1 \n",
      "Iteration:413 \n",
      "Loss:0.18289625644683838\n",
      "Epoch:1 \n",
      "Iteration:414 \n",
      "Loss:0.22489890456199646\n",
      "Epoch:1 \n",
      "Iteration:415 \n",
      "Loss:0.16003383696079254\n",
      "Epoch:1 \n",
      "Iteration:416 \n",
      "Loss:0.2523242235183716\n",
      "Epoch:1 \n",
      "Iteration:417 \n",
      "Loss:0.28450965881347656\n",
      "Epoch:1 \n",
      "Iteration:418 \n",
      "Loss:0.24086427688598633\n",
      "Epoch:1 \n",
      "Iteration:419 \n",
      "Loss:0.12649831175804138\n",
      "Epoch:1 \n",
      "Iteration:420 \n",
      "Loss:0.18169528245925903\n",
      "Epoch:1 \n",
      "Iteration:421 \n",
      "Loss:0.18053504824638367\n",
      "Epoch:1 \n",
      "Iteration:422 \n",
      "Loss:0.1623576581478119\n",
      "Epoch:1 \n",
      "Iteration:423 \n",
      "Loss:0.09776506572961807\n",
      "Epoch:1 \n",
      "Iteration:424 \n",
      "Loss:0.23213250935077667\n",
      "Epoch:1 \n",
      "Iteration:425 \n",
      "Loss:0.16054318845272064\n",
      "Epoch:1 \n",
      "Iteration:426 \n",
      "Loss:0.15931454300880432\n",
      "Epoch:1 \n",
      "Iteration:427 \n",
      "Loss:0.06718993932008743\n",
      "Epoch:1 \n",
      "Iteration:428 \n",
      "Loss:0.13157907128334045\n",
      "Epoch:1 \n",
      "Iteration:429 \n",
      "Loss:0.15056023001670837\n",
      "Epoch:1 \n",
      "Iteration:430 \n",
      "Loss:0.13571284711360931\n",
      "Epoch:1 \n",
      "Iteration:431 \n",
      "Loss:0.1203511655330658\n",
      "Epoch:1 \n",
      "Iteration:432 \n",
      "Loss:0.1570037603378296\n",
      "Epoch:1 \n",
      "Iteration:433 \n",
      "Loss:0.11855635046958923\n",
      "Epoch:1 \n",
      "Iteration:434 \n",
      "Loss:0.09084481000900269\n",
      "Epoch:1 \n",
      "Iteration:435 \n",
      "Loss:0.19788722693920135\n",
      "Epoch:1 \n",
      "Iteration:436 \n",
      "Loss:0.09556737542152405\n",
      "Epoch:1 \n",
      "Iteration:437 \n",
      "Loss:0.2238909751176834\n",
      "Epoch:1 \n",
      "Iteration:438 \n",
      "Loss:0.11157716065645218\n",
      "Epoch:1 \n",
      "Iteration:439 \n",
      "Loss:0.08179385960102081\n",
      "Epoch:1 \n",
      "Iteration:440 \n",
      "Loss:0.0864664614200592\n",
      "Epoch:1 \n",
      "Iteration:441 \n",
      "Loss:0.11941032111644745\n",
      "Epoch:1 \n",
      "Iteration:442 \n",
      "Loss:0.09747246652841568\n",
      "Epoch:1 \n",
      "Iteration:443 \n",
      "Loss:0.14056435227394104\n",
      "Epoch:1 \n",
      "Iteration:444 \n",
      "Loss:0.12694060802459717\n",
      "Epoch:1 \n",
      "Iteration:445 \n",
      "Loss:0.17020730674266815\n",
      "Epoch:1 \n",
      "Iteration:446 \n",
      "Loss:0.15442584455013275\n",
      "Epoch:1 \n",
      "Iteration:447 \n",
      "Loss:0.1218116283416748\n",
      "Epoch:1 \n",
      "Iteration:448 \n",
      "Loss:0.1488742083311081\n",
      "Epoch:1 \n",
      "Iteration:449 \n",
      "Loss:0.141063392162323\n",
      "Epoch:1 \n",
      "Iteration:450 \n",
      "Loss:0.05923749506473541\n",
      "Epoch:1 \n",
      "Iteration:451 \n",
      "Loss:0.1407221555709839\n",
      "Epoch:1 \n",
      "Iteration:452 \n",
      "Loss:0.08396798372268677\n",
      "Epoch:1 \n",
      "Iteration:453 \n",
      "Loss:0.12817895412445068\n",
      "Epoch:1 \n",
      "Iteration:454 \n",
      "Loss:0.14802919328212738\n",
      "Epoch:1 \n",
      "Iteration:455 \n",
      "Loss:0.06963115930557251\n",
      "Epoch:1 \n",
      "Iteration:456 \n",
      "Loss:0.07898536324501038\n",
      "Epoch:1 \n",
      "Iteration:457 \n",
      "Loss:0.018804341554641724\n",
      "Epoch:1 \n",
      "Iteration:458 \n",
      "Loss:0.18707171082496643\n",
      "Epoch:1 \n",
      "Iteration:459 \n",
      "Loss:0.20028729736804962\n",
      "Epoch:1 \n",
      "Iteration:460 \n",
      "Loss:0.26463189721107483\n",
      "Epoch:1 \n",
      "Iteration:461 \n",
      "Loss:0.06801345944404602\n",
      "Epoch:1 \n",
      "Iteration:462 \n",
      "Loss:0.23379573225975037\n",
      "Epoch:1 \n",
      "Iteration:463 \n",
      "Loss:0.09285418689250946\n",
      "Epoch:1 \n",
      "Iteration:464 \n",
      "Loss:0.1288536787033081\n",
      "Epoch:1 \n",
      "Iteration:465 \n",
      "Loss:0.19858354330062866\n",
      "Epoch:1 \n",
      "Iteration:466 \n",
      "Loss:0.10527375340461731\n",
      "Epoch:1 \n",
      "Iteration:467 \n",
      "Loss:0.21339210867881775\n",
      "Epoch:1 \n",
      "Iteration:468 \n",
      "Loss:0.10691437125205994\n",
      "Epoch:1 \n",
      "Iteration:469 \n",
      "Loss:0.17181193828582764\n",
      "Epoch:1 \n",
      "Iteration:470 \n",
      "Loss:0.12533560395240784\n",
      "Epoch:1 \n",
      "Iteration:471 \n",
      "Loss:0.05817952752113342\n",
      "Epoch:1 \n",
      "Iteration:472 \n",
      "Loss:0.16680331528186798\n",
      "Epoch:1 \n",
      "Iteration:473 \n",
      "Loss:0.09557946026325226\n",
      "Epoch:1 \n",
      "Iteration:474 \n",
      "Loss:0.15664370357990265\n",
      "Epoch:1 \n",
      "Iteration:475 \n",
      "Loss:0.12586389482021332\n",
      "Epoch:1 \n",
      "Iteration:476 \n",
      "Loss:0.1069115623831749\n",
      "Epoch:1 \n",
      "Iteration:477 \n",
      "Loss:0.04154718294739723\n",
      "Epoch:1 \n",
      "Iteration:478 \n",
      "Loss:0.18392904102802277\n",
      "Epoch:1 \n",
      "Iteration:479 \n",
      "Loss:0.13265304267406464\n",
      "Epoch:1 \n",
      "Iteration:480 \n",
      "Loss:0.10919620841741562\n",
      "Epoch:1 \n",
      "Iteration:481 \n",
      "Loss:0.11138859391212463\n",
      "Epoch:1 \n",
      "Iteration:482 \n",
      "Loss:0.07535132765769958\n",
      "Epoch:1 \n",
      "Iteration:483 \n",
      "Loss:0.0642189085483551\n",
      "Epoch:1 \n",
      "Iteration:484 \n",
      "Loss:0.16047172248363495\n",
      "Epoch:1 \n",
      "Iteration:485 \n",
      "Loss:0.1741226613521576\n",
      "Epoch:1 \n",
      "Iteration:486 \n",
      "Loss:0.06477146595716476\n",
      "Epoch:1 \n",
      "Iteration:487 \n",
      "Loss:0.17340566217899323\n",
      "Epoch:1 \n",
      "Iteration:488 \n",
      "Loss:0.10121779143810272\n",
      "Epoch:1 \n",
      "Iteration:489 \n",
      "Loss:0.17965959012508392\n",
      "Epoch:1 \n",
      "Iteration:490 \n",
      "Loss:0.058013156056404114\n",
      "Epoch:1 \n",
      "Iteration:491 \n",
      "Loss:0.12729431688785553\n",
      "Epoch:1 \n",
      "Iteration:492 \n",
      "Loss:0.15683141350746155\n",
      "Epoch:1 \n",
      "Iteration:493 \n",
      "Loss:0.2584000825881958\n",
      "Epoch:1 \n",
      "Iteration:494 \n",
      "Loss:0.053398095071315765\n",
      "Epoch:1 \n",
      "Iteration:495 \n",
      "Loss:0.06947608292102814\n",
      "Epoch:1 \n",
      "Iteration:496 \n",
      "Loss:0.08760039508342743\n",
      "Epoch:1 \n",
      "Iteration:497 \n",
      "Loss:0.16803324222564697\n",
      "Epoch:1 \n",
      "Iteration:498 \n",
      "Loss:0.14669391512870789\n",
      "Epoch:1 \n",
      "Iteration:499 \n",
      "Loss:0.1604669690132141\n",
      "Epoch:1 \n",
      "Iteration:500 \n",
      "Loss:0.03770060837268829\n",
      "Epoch:1 \n",
      "Iteration:501 \n",
      "Loss:0.18763208389282227\n",
      "Epoch:1 \n",
      "Iteration:502 \n",
      "Loss:0.057373832911252975\n",
      "Epoch:1 \n",
      "Iteration:503 \n",
      "Loss:0.11099676787853241\n",
      "Epoch:1 \n",
      "Iteration:504 \n",
      "Loss:0.09875714033842087\n",
      "Epoch:1 \n",
      "Iteration:505 \n",
      "Loss:0.052983954548835754\n",
      "Epoch:1 \n",
      "Iteration:506 \n",
      "Loss:0.05324944481253624\n",
      "Epoch:1 \n",
      "Iteration:507 \n",
      "Loss:0.09516720473766327\n",
      "Epoch:1 \n",
      "Iteration:508 \n",
      "Loss:0.2105506956577301\n",
      "Epoch:1 \n",
      "Iteration:509 \n",
      "Loss:0.16112357378005981\n",
      "Epoch:1 \n",
      "Iteration:510 \n",
      "Loss:0.1232694536447525\n",
      "Epoch:1 \n",
      "Iteration:511 \n",
      "Loss:0.05190546065568924\n",
      "Epoch:1 \n",
      "Iteration:512 \n",
      "Loss:0.12582232058048248\n",
      "Epoch:1 \n",
      "Iteration:513 \n",
      "Loss:0.2602381706237793\n",
      "Epoch:1 \n",
      "Iteration:514 \n",
      "Loss:0.20105312764644623\n",
      "Epoch:1 \n",
      "Iteration:515 \n",
      "Loss:0.11387726664543152\n",
      "Epoch:1 \n",
      "Iteration:516 \n",
      "Loss:0.1497097909450531\n",
      "Epoch:1 \n",
      "Iteration:517 \n",
      "Loss:0.2781929075717926\n",
      "Epoch:1 \n",
      "Iteration:518 \n",
      "Loss:0.14638608694076538\n",
      "Epoch:1 \n",
      "Iteration:519 \n",
      "Loss:0.24004524946212769\n",
      "Epoch:1 \n",
      "Iteration:520 \n",
      "Loss:0.19831907749176025\n",
      "Epoch:1 \n",
      "Iteration:521 \n",
      "Loss:0.05353476479649544\n",
      "Epoch:1 \n",
      "Iteration:522 \n",
      "Loss:0.23325391113758087\n",
      "Epoch:1 \n",
      "Iteration:523 \n",
      "Loss:0.18582992255687714\n",
      "Epoch:1 \n",
      "Iteration:524 \n",
      "Loss:0.09449417889118195\n",
      "Epoch:1 \n",
      "Iteration:525 \n",
      "Loss:0.09688922017812729\n",
      "Epoch:1 \n",
      "Iteration:526 \n",
      "Loss:0.027983758598566055\n",
      "Epoch:1 \n",
      "Iteration:527 \n",
      "Loss:0.11870747804641724\n",
      "Epoch:1 \n",
      "Iteration:528 \n",
      "Loss:0.1011987179517746\n",
      "Epoch:1 \n",
      "Iteration:529 \n",
      "Loss:0.1776442676782608\n",
      "Epoch:1 \n",
      "Iteration:530 \n",
      "Loss:0.14917439222335815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 \n",
      "Iteration:531 \n",
      "Loss:0.14289936423301697\n",
      "Epoch:1 \n",
      "Iteration:532 \n",
      "Loss:0.05317782983183861\n",
      "Epoch:1 \n",
      "Iteration:533 \n",
      "Loss:0.14169421792030334\n",
      "Epoch:1 \n",
      "Iteration:534 \n",
      "Loss:0.19296100735664368\n",
      "Epoch:1 \n",
      "Iteration:535 \n",
      "Loss:0.0548328198492527\n",
      "Epoch:1 \n",
      "Iteration:536 \n",
      "Loss:0.11793999373912811\n",
      "Epoch:1 \n",
      "Iteration:537 \n",
      "Loss:0.1974993497133255\n",
      "Epoch:1 \n",
      "Iteration:538 \n",
      "Loss:0.14726309478282928\n",
      "Epoch:1 \n",
      "Iteration:539 \n",
      "Loss:0.05452985689043999\n",
      "Epoch:1 \n",
      "Iteration:540 \n",
      "Loss:0.08887743204832077\n",
      "Epoch:1 \n",
      "Iteration:541 \n",
      "Loss:0.05349624156951904\n",
      "Epoch:1 \n",
      "Iteration:542 \n",
      "Loss:0.09971152245998383\n",
      "Epoch:1 \n",
      "Iteration:543 \n",
      "Loss:0.12198226898908615\n",
      "Epoch:1 \n",
      "Iteration:544 \n",
      "Loss:0.19008606672286987\n",
      "Epoch:1 \n",
      "Iteration:545 \n",
      "Loss:0.1730809360742569\n",
      "Epoch:1 \n",
      "Iteration:546 \n",
      "Loss:0.12990449368953705\n",
      "Epoch:1 \n",
      "Iteration:547 \n",
      "Loss:0.3251248896121979\n",
      "Epoch:1 \n",
      "Iteration:548 \n",
      "Loss:0.04347158968448639\n",
      "Epoch:1 \n",
      "Iteration:549 \n",
      "Loss:0.05970887839794159\n",
      "Epoch:1 \n",
      "Iteration:550 \n",
      "Loss:0.10945221036672592\n",
      "Epoch:1 \n",
      "Iteration:551 \n",
      "Loss:0.15638773143291473\n",
      "Epoch:1 \n",
      "Iteration:552 \n",
      "Loss:0.12025328725576401\n",
      "Epoch:1 \n",
      "Iteration:553 \n",
      "Loss:0.18447792530059814\n",
      "Epoch:1 \n",
      "Iteration:554 \n",
      "Loss:0.09545507282018661\n",
      "Epoch:1 \n",
      "Iteration:555 \n",
      "Loss:0.18379849195480347\n",
      "Epoch:1 \n",
      "Iteration:556 \n",
      "Loss:0.05731698498129845\n",
      "Epoch:1 \n",
      "Iteration:557 \n",
      "Loss:0.17120161652565002\n",
      "Epoch:1 \n",
      "Iteration:558 \n",
      "Loss:0.21065430343151093\n",
      "Epoch:1 \n",
      "Iteration:559 \n",
      "Loss:0.1444886028766632\n",
      "Epoch:1 \n",
      "Iteration:560 \n",
      "Loss:0.08166111260652542\n",
      "Epoch:1 \n",
      "Iteration:561 \n",
      "Loss:0.1046004667878151\n",
      "Epoch:1 \n",
      "Iteration:562 \n",
      "Loss:0.09464655071496964\n",
      "Epoch:1 \n",
      "Iteration:563 \n",
      "Loss:0.12497057020664215\n",
      "Epoch:1 \n",
      "Iteration:564 \n",
      "Loss:0.04581575095653534\n",
      "Epoch:1 \n",
      "Iteration:565 \n",
      "Loss:0.22922943532466888\n",
      "Epoch:1 \n",
      "Iteration:566 \n",
      "Loss:0.28213006258010864\n",
      "Epoch:1 \n",
      "Iteration:567 \n",
      "Loss:0.1906711757183075\n",
      "Epoch:1 \n",
      "Iteration:568 \n",
      "Loss:0.05716053023934364\n",
      "Epoch:1 \n",
      "Iteration:569 \n",
      "Loss:0.08213618397712708\n",
      "Epoch:1 \n",
      "Iteration:570 \n",
      "Loss:0.14924253523349762\n",
      "Epoch:1 \n",
      "Iteration:571 \n",
      "Loss:0.06512930244207382\n",
      "Epoch:1 \n",
      "Iteration:572 \n",
      "Loss:0.05922549217939377\n",
      "Epoch:1 \n",
      "Iteration:573 \n",
      "Loss:0.11661934107542038\n",
      "Epoch:1 \n",
      "Iteration:574 \n",
      "Loss:0.20099715888500214\n",
      "Epoch:1 \n",
      "Iteration:575 \n",
      "Loss:0.1725538671016693\n",
      "Epoch:1 \n",
      "Iteration:576 \n",
      "Loss:0.2321486622095108\n",
      "Epoch:1 \n",
      "Iteration:577 \n",
      "Loss:0.14236022531986237\n",
      "Epoch:1 \n",
      "Iteration:578 \n",
      "Loss:0.10517439246177673\n",
      "Epoch:1 \n",
      "Iteration:579 \n",
      "Loss:0.07205498218536377\n",
      "Epoch:1 \n",
      "Iteration:580 \n",
      "Loss:0.10681230574846268\n",
      "Epoch:1 \n",
      "Iteration:581 \n",
      "Loss:0.1372218132019043\n",
      "Epoch:1 \n",
      "Iteration:582 \n",
      "Loss:0.10903928428888321\n",
      "Epoch:1 \n",
      "Iteration:583 \n",
      "Loss:0.11332202702760696\n",
      "Epoch:1 \n",
      "Iteration:584 \n",
      "Loss:0.08661611378192902\n",
      "Epoch:1 \n",
      "Iteration:585 \n",
      "Loss:0.04033790901303291\n",
      "Epoch:1 \n",
      "Iteration:586 \n",
      "Loss:0.05375756695866585\n",
      "Epoch:1 \n",
      "Iteration:587 \n",
      "Loss:0.07908552139997482\n",
      "Epoch:1 \n",
      "Iteration:588 \n",
      "Loss:0.2648504674434662\n",
      "Epoch:1 \n",
      "Iteration:589 \n",
      "Loss:0.16532693803310394\n",
      "Epoch:1 \n",
      "Iteration:590 \n",
      "Loss:0.14812788367271423\n",
      "Epoch:1 \n",
      "Iteration:591 \n",
      "Loss:0.054632458835840225\n",
      "Epoch:1 \n",
      "Iteration:592 \n",
      "Loss:0.0649971216917038\n",
      "Epoch:1 \n",
      "Iteration:593 \n",
      "Loss:0.09802078455686569\n",
      "Epoch:1 \n",
      "Iteration:594 \n",
      "Loss:0.06874732673168182\n",
      "Epoch:1 \n",
      "Iteration:595 \n",
      "Loss:0.10750783234834671\n",
      "Epoch:1 \n",
      "Iteration:596 \n",
      "Loss:0.065363809466362\n",
      "Epoch:1 \n",
      "Iteration:597 \n",
      "Loss:0.06026590242981911\n",
      "Epoch:1 \n",
      "Iteration:598 \n",
      "Loss:0.07939674705266953\n",
      "Epoch:1 \n",
      "Iteration:599 \n",
      "Loss:0.14536221325397491\n",
      "Epoch:1 \n",
      "Iteration:600 \n",
      "Loss:0.18349143862724304\n",
      "\n",
      "Accuracy of network in epoch 1: 93.03166666666667\n",
      "Epoch:2 \n",
      "Iteration:1 \n",
      "Loss:0.03768860921263695\n",
      "Epoch:2 \n",
      "Iteration:2 \n",
      "Loss:0.1256306916475296\n",
      "Epoch:2 \n",
      "Iteration:3 \n",
      "Loss:0.13695181906223297\n",
      "Epoch:2 \n",
      "Iteration:4 \n",
      "Loss:0.10828784853219986\n",
      "Epoch:2 \n",
      "Iteration:5 \n",
      "Loss:0.09610897302627563\n",
      "Epoch:2 \n",
      "Iteration:6 \n",
      "Loss:0.10789445787668228\n",
      "Epoch:2 \n",
      "Iteration:7 \n",
      "Loss:0.17946983873844147\n",
      "Epoch:2 \n",
      "Iteration:8 \n",
      "Loss:0.11347386986017227\n",
      "Epoch:2 \n",
      "Iteration:9 \n",
      "Loss:0.05046037212014198\n",
      "Epoch:2 \n",
      "Iteration:10 \n",
      "Loss:0.05603288114070892\n",
      "Epoch:2 \n",
      "Iteration:11 \n",
      "Loss:0.049103789031505585\n",
      "Epoch:2 \n",
      "Iteration:12 \n",
      "Loss:0.14062586426734924\n",
      "Epoch:2 \n",
      "Iteration:13 \n",
      "Loss:0.016742099076509476\n",
      "Epoch:2 \n",
      "Iteration:14 \n",
      "Loss:0.14218811690807343\n",
      "Epoch:2 \n",
      "Iteration:15 \n",
      "Loss:0.0857653021812439\n",
      "Epoch:2 \n",
      "Iteration:16 \n",
      "Loss:0.10725483298301697\n",
      "Epoch:2 \n",
      "Iteration:17 \n",
      "Loss:0.06952115893363953\n",
      "Epoch:2 \n",
      "Iteration:18 \n",
      "Loss:0.08461152017116547\n",
      "Epoch:2 \n",
      "Iteration:19 \n",
      "Loss:0.0807526558637619\n",
      "Epoch:2 \n",
      "Iteration:20 \n",
      "Loss:0.04271145164966583\n",
      "Epoch:2 \n",
      "Iteration:21 \n",
      "Loss:0.10021254420280457\n",
      "Epoch:2 \n",
      "Iteration:22 \n",
      "Loss:0.2145097851753235\n",
      "Epoch:2 \n",
      "Iteration:23 \n",
      "Loss:0.1163840964436531\n",
      "Epoch:2 \n",
      "Iteration:24 \n",
      "Loss:0.15726019442081451\n",
      "Epoch:2 \n",
      "Iteration:25 \n",
      "Loss:0.02367512881755829\n",
      "Epoch:2 \n",
      "Iteration:26 \n",
      "Loss:0.1830364167690277\n",
      "Epoch:2 \n",
      "Iteration:27 \n",
      "Loss:0.1608065664768219\n",
      "Epoch:2 \n",
      "Iteration:28 \n",
      "Loss:0.13235792517662048\n",
      "Epoch:2 \n",
      "Iteration:29 \n",
      "Loss:0.08429094403982162\n",
      "Epoch:2 \n",
      "Iteration:30 \n",
      "Loss:0.082579106092453\n",
      "Epoch:2 \n",
      "Iteration:31 \n",
      "Loss:0.08736960589885712\n",
      "Epoch:2 \n",
      "Iteration:32 \n",
      "Loss:0.09068331867456436\n",
      "Epoch:2 \n",
      "Iteration:33 \n",
      "Loss:0.055903684347867966\n",
      "Epoch:2 \n",
      "Iteration:34 \n",
      "Loss:0.19918718934059143\n",
      "Epoch:2 \n",
      "Iteration:35 \n",
      "Loss:0.13910189270973206\n",
      "Epoch:2 \n",
      "Iteration:36 \n",
      "Loss:0.17552827298641205\n",
      "Epoch:2 \n",
      "Iteration:37 \n",
      "Loss:0.1186172366142273\n",
      "Epoch:2 \n",
      "Iteration:38 \n",
      "Loss:0.08279170840978622\n",
      "Epoch:2 \n",
      "Iteration:39 \n",
      "Loss:0.10063126683235168\n",
      "Epoch:2 \n",
      "Iteration:40 \n",
      "Loss:0.03503945469856262\n",
      "Epoch:2 \n",
      "Iteration:41 \n",
      "Loss:0.07224889099597931\n",
      "Epoch:2 \n",
      "Iteration:42 \n",
      "Loss:0.05653027445077896\n",
      "Epoch:2 \n",
      "Iteration:43 \n",
      "Loss:0.16159400343894958\n",
      "Epoch:2 \n",
      "Iteration:44 \n",
      "Loss:0.1234220489859581\n",
      "Epoch:2 \n",
      "Iteration:45 \n",
      "Loss:0.0317281112074852\n",
      "Epoch:2 \n",
      "Iteration:46 \n",
      "Loss:0.06791935116052628\n",
      "Epoch:2 \n",
      "Iteration:47 \n",
      "Loss:0.047314103692770004\n",
      "Epoch:2 \n",
      "Iteration:48 \n",
      "Loss:0.1521657258272171\n",
      "Epoch:2 \n",
      "Iteration:49 \n",
      "Loss:0.08422497659921646\n",
      "Epoch:2 \n",
      "Iteration:50 \n",
      "Loss:0.05944348871707916\n",
      "Epoch:2 \n",
      "Iteration:51 \n",
      "Loss:0.05755031108856201\n",
      "Epoch:2 \n",
      "Iteration:52 \n",
      "Loss:0.09010080248117447\n",
      "Epoch:2 \n",
      "Iteration:53 \n",
      "Loss:0.10535210371017456\n",
      "Epoch:2 \n",
      "Iteration:54 \n",
      "Loss:0.13041329383850098\n",
      "Epoch:2 \n",
      "Iteration:55 \n",
      "Loss:0.09273028373718262\n",
      "Epoch:2 \n",
      "Iteration:56 \n",
      "Loss:0.15835565328598022\n",
      "Epoch:2 \n",
      "Iteration:57 \n",
      "Loss:0.15424878895282745\n",
      "Epoch:2 \n",
      "Iteration:58 \n",
      "Loss:0.19810603559017181\n",
      "Epoch:2 \n",
      "Iteration:59 \n",
      "Loss:0.19962118566036224\n",
      "Epoch:2 \n",
      "Iteration:60 \n",
      "Loss:0.03862475976347923\n",
      "Epoch:2 \n",
      "Iteration:61 \n",
      "Loss:0.0819513201713562\n",
      "Epoch:2 \n",
      "Iteration:62 \n",
      "Loss:0.11862997710704803\n",
      "Epoch:2 \n",
      "Iteration:63 \n",
      "Loss:0.17596612870693207\n",
      "Epoch:2 \n",
      "Iteration:64 \n",
      "Loss:0.16683471202850342\n",
      "Epoch:2 \n",
      "Iteration:65 \n",
      "Loss:0.03789473697543144\n",
      "Epoch:2 \n",
      "Iteration:66 \n",
      "Loss:0.028709804639220238\n",
      "Epoch:2 \n",
      "Iteration:67 \n",
      "Loss:0.15830965340137482\n",
      "Epoch:2 \n",
      "Iteration:68 \n",
      "Loss:0.04561738669872284\n",
      "Epoch:2 \n",
      "Iteration:69 \n",
      "Loss:0.08934885263442993\n",
      "Epoch:2 \n",
      "Iteration:70 \n",
      "Loss:0.054945334792137146\n",
      "Epoch:2 \n",
      "Iteration:71 \n",
      "Loss:0.10217737406492233\n",
      "Epoch:2 \n",
      "Iteration:72 \n",
      "Loss:0.024448256939649582\n",
      "Epoch:2 \n",
      "Iteration:73 \n",
      "Loss:0.10416826605796814\n",
      "Epoch:2 \n",
      "Iteration:74 \n",
      "Loss:0.07856085151433945\n",
      "Epoch:2 \n",
      "Iteration:75 \n",
      "Loss:0.08827532082796097\n",
      "Epoch:2 \n",
      "Iteration:76 \n",
      "Loss:0.0962040051817894\n",
      "Epoch:2 \n",
      "Iteration:77 \n",
      "Loss:0.08688598871231079\n",
      "Epoch:2 \n",
      "Iteration:78 \n",
      "Loss:0.010532799176871777\n",
      "Epoch:2 \n",
      "Iteration:79 \n",
      "Loss:0.06756215542554855\n",
      "Epoch:2 \n",
      "Iteration:80 \n",
      "Loss:0.0935513898730278\n",
      "Epoch:2 \n",
      "Iteration:81 \n",
      "Loss:0.025664575397968292\n",
      "Epoch:2 \n",
      "Iteration:82 \n",
      "Loss:0.22062167525291443\n",
      "Epoch:2 \n",
      "Iteration:83 \n",
      "Loss:0.06896625459194183\n",
      "Epoch:2 \n",
      "Iteration:84 \n",
      "Loss:0.12421489506959915\n",
      "Epoch:2 \n",
      "Iteration:85 \n",
      "Loss:0.08726920187473297\n",
      "Epoch:2 \n",
      "Iteration:86 \n",
      "Loss:0.06861772388219833\n",
      "Epoch:2 \n",
      "Iteration:87 \n",
      "Loss:0.04847275838255882\n",
      "Epoch:2 \n",
      "Iteration:88 \n",
      "Loss:0.13171255588531494\n",
      "Epoch:2 \n",
      "Iteration:89 \n",
      "Loss:0.020263519138097763\n",
      "Epoch:2 \n",
      "Iteration:90 \n",
      "Loss:0.08402882516384125\n",
      "Epoch:2 \n",
      "Iteration:91 \n",
      "Loss:0.051180947571992874\n",
      "Epoch:2 \n",
      "Iteration:92 \n",
      "Loss:0.049127716571092606\n",
      "Epoch:2 \n",
      "Iteration:93 \n",
      "Loss:0.08888084441423416\n",
      "Epoch:2 \n",
      "Iteration:94 \n",
      "Loss:0.17704403400421143\n",
      "Epoch:2 \n",
      "Iteration:95 \n",
      "Loss:0.023156234994530678\n",
      "Epoch:2 \n",
      "Iteration:96 \n",
      "Loss:0.050671275705099106\n",
      "Epoch:2 \n",
      "Iteration:97 \n",
      "Loss:0.11057719588279724\n",
      "Epoch:2 \n",
      "Iteration:98 \n",
      "Loss:0.06792283803224564\n",
      "Epoch:2 \n",
      "Iteration:99 \n",
      "Loss:0.1934918463230133\n",
      "Epoch:2 \n",
      "Iteration:100 \n",
      "Loss:0.09845835715532303\n",
      "Epoch:2 \n",
      "Iteration:101 \n",
      "Loss:0.1792984902858734\n",
      "Epoch:2 \n",
      "Iteration:102 \n",
      "Loss:0.13294540345668793\n",
      "Epoch:2 \n",
      "Iteration:103 \n",
      "Loss:0.10784327238798141\n",
      "Epoch:2 \n",
      "Iteration:104 \n",
      "Loss:0.07323049753904343\n",
      "Epoch:2 \n",
      "Iteration:105 \n",
      "Loss:0.04443055018782616\n",
      "Epoch:2 \n",
      "Iteration:106 \n",
      "Loss:0.07564422488212585\n",
      "Epoch:2 \n",
      "Iteration:107 \n",
      "Loss:0.16559630632400513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2 \n",
      "Iteration:108 \n",
      "Loss:0.11632976680994034\n",
      "Epoch:2 \n",
      "Iteration:109 \n",
      "Loss:0.0820557102560997\n",
      "Epoch:2 \n",
      "Iteration:110 \n",
      "Loss:0.058863043785095215\n",
      "Epoch:2 \n",
      "Iteration:111 \n",
      "Loss:0.0629238709807396\n",
      "Epoch:2 \n",
      "Iteration:112 \n",
      "Loss:0.036725856363773346\n",
      "Epoch:2 \n",
      "Iteration:113 \n",
      "Loss:0.08275307714939117\n",
      "Epoch:2 \n",
      "Iteration:114 \n",
      "Loss:0.12446576356887817\n",
      "Epoch:2 \n",
      "Iteration:115 \n",
      "Loss:0.0786079689860344\n",
      "Epoch:2 \n",
      "Iteration:116 \n",
      "Loss:0.09837708622217178\n",
      "Epoch:2 \n",
      "Iteration:117 \n",
      "Loss:0.11518042534589767\n",
      "Epoch:2 \n",
      "Iteration:118 \n",
      "Loss:0.03974189609289169\n",
      "Epoch:2 \n",
      "Iteration:119 \n",
      "Loss:0.07056482881307602\n",
      "Epoch:2 \n",
      "Iteration:120 \n",
      "Loss:0.10706470161676407\n",
      "Epoch:2 \n",
      "Iteration:121 \n",
      "Loss:0.10666216909885406\n",
      "Epoch:2 \n",
      "Iteration:122 \n",
      "Loss:0.049494221806526184\n",
      "Epoch:2 \n",
      "Iteration:123 \n",
      "Loss:0.11394444108009338\n",
      "Epoch:2 \n",
      "Iteration:124 \n",
      "Loss:0.07226788997650146\n",
      "Epoch:2 \n",
      "Iteration:125 \n",
      "Loss:0.09296377003192902\n",
      "Epoch:2 \n",
      "Iteration:126 \n",
      "Loss:0.10244791209697723\n",
      "Epoch:2 \n",
      "Iteration:127 \n",
      "Loss:0.06408727914094925\n",
      "Epoch:2 \n",
      "Iteration:128 \n",
      "Loss:0.07959353178739548\n",
      "Epoch:2 \n",
      "Iteration:129 \n",
      "Loss:0.11107835918664932\n",
      "Epoch:2 \n",
      "Iteration:130 \n",
      "Loss:0.023910660296678543\n",
      "Epoch:2 \n",
      "Iteration:131 \n",
      "Loss:0.1283545196056366\n",
      "Epoch:2 \n",
      "Iteration:132 \n",
      "Loss:0.10096649825572968\n",
      "Epoch:2 \n",
      "Iteration:133 \n",
      "Loss:0.0758940801024437\n",
      "Epoch:2 \n",
      "Iteration:134 \n",
      "Loss:0.07442004233598709\n",
      "Epoch:2 \n",
      "Iteration:135 \n",
      "Loss:0.10350458323955536\n",
      "Epoch:2 \n",
      "Iteration:136 \n",
      "Loss:0.08413533121347427\n",
      "Epoch:2 \n",
      "Iteration:137 \n",
      "Loss:0.09812232851982117\n",
      "Epoch:2 \n",
      "Iteration:138 \n",
      "Loss:0.056328702718019485\n",
      "Epoch:2 \n",
      "Iteration:139 \n",
      "Loss:0.20106303691864014\n",
      "Epoch:2 \n",
      "Iteration:140 \n",
      "Loss:0.0989891067147255\n",
      "Epoch:2 \n",
      "Iteration:141 \n",
      "Loss:0.1570352017879486\n",
      "Epoch:2 \n",
      "Iteration:142 \n",
      "Loss:0.040356818586587906\n",
      "Epoch:2 \n",
      "Iteration:143 \n",
      "Loss:0.03203118219971657\n",
      "Epoch:2 \n",
      "Iteration:144 \n",
      "Loss:0.15831315517425537\n",
      "Epoch:2 \n",
      "Iteration:145 \n",
      "Loss:0.027550697326660156\n",
      "Epoch:2 \n",
      "Iteration:146 \n",
      "Loss:0.08113782107830048\n",
      "Epoch:2 \n",
      "Iteration:147 \n",
      "Loss:0.13548453152179718\n",
      "Epoch:2 \n",
      "Iteration:148 \n",
      "Loss:0.051072634756565094\n",
      "Epoch:2 \n",
      "Iteration:149 \n",
      "Loss:0.07170893996953964\n",
      "Epoch:2 \n",
      "Iteration:150 \n",
      "Loss:0.08512383699417114\n",
      "Epoch:2 \n",
      "Iteration:151 \n",
      "Loss:0.08447585254907608\n",
      "Epoch:2 \n",
      "Iteration:152 \n",
      "Loss:0.10489287972450256\n",
      "Epoch:2 \n",
      "Iteration:153 \n",
      "Loss:0.12810854613780975\n",
      "Epoch:2 \n",
      "Iteration:154 \n",
      "Loss:0.06750764697790146\n",
      "Epoch:2 \n",
      "Iteration:155 \n",
      "Loss:0.1143108382821083\n",
      "Epoch:2 \n",
      "Iteration:156 \n",
      "Loss:0.07451017200946808\n",
      "Epoch:2 \n",
      "Iteration:157 \n",
      "Loss:0.04295981302857399\n",
      "Epoch:2 \n",
      "Iteration:158 \n",
      "Loss:0.04076758399605751\n",
      "Epoch:2 \n",
      "Iteration:159 \n",
      "Loss:0.05125143378973007\n",
      "Epoch:2 \n",
      "Iteration:160 \n",
      "Loss:0.08065800368785858\n",
      "Epoch:2 \n",
      "Iteration:161 \n",
      "Loss:0.032230257987976074\n",
      "Epoch:2 \n",
      "Iteration:162 \n",
      "Loss:0.04377559572458267\n",
      "Epoch:2 \n",
      "Iteration:163 \n",
      "Loss:0.020634200423955917\n",
      "Epoch:2 \n",
      "Iteration:164 \n",
      "Loss:0.1335270255804062\n",
      "Epoch:2 \n",
      "Iteration:165 \n",
      "Loss:0.034214556217193604\n",
      "Epoch:2 \n",
      "Iteration:166 \n",
      "Loss:0.061434391885995865\n",
      "Epoch:2 \n",
      "Iteration:167 \n",
      "Loss:0.03717609867453575\n",
      "Epoch:2 \n",
      "Iteration:168 \n",
      "Loss:0.23791009187698364\n",
      "Epoch:2 \n",
      "Iteration:169 \n",
      "Loss:0.054906487464904785\n",
      "Epoch:2 \n",
      "Iteration:170 \n",
      "Loss:0.08690745383501053\n",
      "Epoch:2 \n",
      "Iteration:171 \n",
      "Loss:0.20042583346366882\n",
      "Epoch:2 \n",
      "Iteration:172 \n",
      "Loss:0.05465885251760483\n",
      "Epoch:2 \n",
      "Iteration:173 \n",
      "Loss:0.017153052613139153\n",
      "Epoch:2 \n",
      "Iteration:174 \n",
      "Loss:0.12338985502719879\n",
      "Epoch:2 \n",
      "Iteration:175 \n",
      "Loss:0.10872585326433182\n",
      "Epoch:2 \n",
      "Iteration:176 \n",
      "Loss:0.12953558564186096\n",
      "Epoch:2 \n",
      "Iteration:177 \n",
      "Loss:0.1391502320766449\n",
      "Epoch:2 \n",
      "Iteration:178 \n",
      "Loss:0.09906962513923645\n",
      "Epoch:2 \n",
      "Iteration:179 \n",
      "Loss:0.0098823681473732\n",
      "Epoch:2 \n",
      "Iteration:180 \n",
      "Loss:0.07000062614679337\n",
      "Epoch:2 \n",
      "Iteration:181 \n",
      "Loss:0.08142376691102982\n",
      "Epoch:2 \n",
      "Iteration:182 \n",
      "Loss:0.1667347401380539\n",
      "Epoch:2 \n",
      "Iteration:183 \n",
      "Loss:0.030405569821596146\n",
      "Epoch:2 \n",
      "Iteration:184 \n",
      "Loss:0.0505489744246006\n",
      "Epoch:2 \n",
      "Iteration:185 \n",
      "Loss:0.08576603978872299\n",
      "Epoch:2 \n",
      "Iteration:186 \n",
      "Loss:0.060864511877298355\n",
      "Epoch:2 \n",
      "Iteration:187 \n",
      "Loss:0.09758847951889038\n",
      "Epoch:2 \n",
      "Iteration:188 \n",
      "Loss:0.03645620495080948\n",
      "Epoch:2 \n",
      "Iteration:189 \n",
      "Loss:0.17544923722743988\n",
      "Epoch:2 \n",
      "Iteration:190 \n",
      "Loss:0.03714743256568909\n",
      "Epoch:2 \n",
      "Iteration:191 \n",
      "Loss:0.08691917359828949\n",
      "Epoch:2 \n",
      "Iteration:192 \n",
      "Loss:0.06506047397851944\n",
      "Epoch:2 \n",
      "Iteration:193 \n",
      "Loss:0.21158522367477417\n",
      "Epoch:2 \n",
      "Iteration:194 \n",
      "Loss:0.019708724692463875\n",
      "Epoch:2 \n",
      "Iteration:195 \n",
      "Loss:0.13108254969120026\n",
      "Epoch:2 \n",
      "Iteration:196 \n",
      "Loss:0.08538669347763062\n",
      "Epoch:2 \n",
      "Iteration:197 \n",
      "Loss:0.09504768252372742\n",
      "Epoch:2 \n",
      "Iteration:198 \n",
      "Loss:0.042060066014528275\n",
      "Epoch:2 \n",
      "Iteration:199 \n",
      "Loss:0.03852638974785805\n",
      "Epoch:2 \n",
      "Iteration:200 \n",
      "Loss:0.12602072954177856\n",
      "Epoch:2 \n",
      "Iteration:201 \n",
      "Loss:0.08068415522575378\n",
      "Epoch:2 \n",
      "Iteration:202 \n",
      "Loss:0.06731382012367249\n",
      "Epoch:2 \n",
      "Iteration:203 \n",
      "Loss:0.04185939207673073\n",
      "Epoch:2 \n",
      "Iteration:204 \n",
      "Loss:0.06309980899095535\n",
      "Epoch:2 \n",
      "Iteration:205 \n",
      "Loss:0.25756776332855225\n",
      "Epoch:2 \n",
      "Iteration:206 \n",
      "Loss:0.121894970536232\n",
      "Epoch:2 \n",
      "Iteration:207 \n",
      "Loss:0.16582271456718445\n",
      "Epoch:2 \n",
      "Iteration:208 \n",
      "Loss:0.05141178518533707\n",
      "Epoch:2 \n",
      "Iteration:209 \n",
      "Loss:0.1772456020116806\n",
      "Epoch:2 \n",
      "Iteration:210 \n",
      "Loss:0.2845035195350647\n",
      "Epoch:2 \n",
      "Iteration:211 \n",
      "Loss:0.07357021421194077\n",
      "Epoch:2 \n",
      "Iteration:212 \n",
      "Loss:0.06464225053787231\n",
      "Epoch:2 \n",
      "Iteration:213 \n",
      "Loss:0.04828151687979698\n",
      "Epoch:2 \n",
      "Iteration:214 \n",
      "Loss:0.053133681416511536\n",
      "Epoch:2 \n",
      "Iteration:215 \n",
      "Loss:0.13257725536823273\n",
      "Epoch:2 \n",
      "Iteration:216 \n",
      "Loss:0.06481591612100601\n",
      "Epoch:2 \n",
      "Iteration:217 \n",
      "Loss:0.1376100778579712\n",
      "Epoch:2 \n",
      "Iteration:218 \n",
      "Loss:0.14807011187076569\n",
      "Epoch:2 \n",
      "Iteration:219 \n",
      "Loss:0.0719819962978363\n",
      "Epoch:2 \n",
      "Iteration:220 \n",
      "Loss:0.039959631860256195\n",
      "Epoch:2 \n",
      "Iteration:221 \n",
      "Loss:0.15952752530574799\n",
      "Epoch:2 \n",
      "Iteration:222 \n",
      "Loss:0.1926977038383484\n",
      "Epoch:2 \n",
      "Iteration:223 \n",
      "Loss:0.06430050730705261\n",
      "Epoch:2 \n",
      "Iteration:224 \n",
      "Loss:0.024328796193003654\n",
      "Epoch:2 \n",
      "Iteration:225 \n",
      "Loss:0.057211656123399734\n",
      "Epoch:2 \n",
      "Iteration:226 \n",
      "Loss:0.10065016150474548\n",
      "Epoch:2 \n",
      "Iteration:227 \n",
      "Loss:0.06397677212953568\n",
      "Epoch:2 \n",
      "Iteration:228 \n",
      "Loss:0.018466521054506302\n",
      "Epoch:2 \n",
      "Iteration:229 \n",
      "Loss:0.07530060410499573\n",
      "Epoch:2 \n",
      "Iteration:230 \n",
      "Loss:0.10649986565113068\n",
      "Epoch:2 \n",
      "Iteration:231 \n",
      "Loss:0.048574239015579224\n",
      "Epoch:2 \n",
      "Iteration:232 \n",
      "Loss:0.0857330858707428\n",
      "Epoch:2 \n",
      "Iteration:233 \n",
      "Loss:0.12259946018457413\n",
      "Epoch:2 \n",
      "Iteration:234 \n",
      "Loss:0.033085696399211884\n",
      "Epoch:2 \n",
      "Iteration:235 \n",
      "Loss:0.1099027469754219\n",
      "Epoch:2 \n",
      "Iteration:236 \n",
      "Loss:0.10122503340244293\n",
      "Epoch:2 \n",
      "Iteration:237 \n",
      "Loss:0.0727650597691536\n",
      "Epoch:2 \n",
      "Iteration:238 \n",
      "Loss:0.2126000076532364\n",
      "Epoch:2 \n",
      "Iteration:239 \n",
      "Loss:0.08605486899614334\n",
      "Epoch:2 \n",
      "Iteration:240 \n",
      "Loss:0.08131510019302368\n",
      "Epoch:2 \n",
      "Iteration:241 \n",
      "Loss:0.08340586721897125\n",
      "Epoch:2 \n",
      "Iteration:242 \n",
      "Loss:0.048440221697092056\n",
      "Epoch:2 \n",
      "Iteration:243 \n",
      "Loss:0.10631762444972992\n",
      "Epoch:2 \n",
      "Iteration:244 \n",
      "Loss:0.05353979021310806\n",
      "Epoch:2 \n",
      "Iteration:245 \n",
      "Loss:0.09776134788990021\n",
      "Epoch:2 \n",
      "Iteration:246 \n",
      "Loss:0.06274985522031784\n",
      "Epoch:2 \n",
      "Iteration:247 \n",
      "Loss:0.15248923003673553\n",
      "Epoch:2 \n",
      "Iteration:248 \n",
      "Loss:0.036425407975912094\n",
      "Epoch:2 \n",
      "Iteration:249 \n",
      "Loss:0.14450109004974365\n",
      "Epoch:2 \n",
      "Iteration:250 \n",
      "Loss:0.03845297545194626\n",
      "Epoch:2 \n",
      "Iteration:251 \n",
      "Loss:0.16880793869495392\n",
      "Epoch:2 \n",
      "Iteration:252 \n",
      "Loss:0.09434308856725693\n",
      "Epoch:2 \n",
      "Iteration:253 \n",
      "Loss:0.17590011656284332\n",
      "Epoch:2 \n",
      "Iteration:254 \n",
      "Loss:0.0871458426117897\n",
      "Epoch:2 \n",
      "Iteration:255 \n",
      "Loss:0.0561591237783432\n",
      "Epoch:2 \n",
      "Iteration:256 \n",
      "Loss:0.018637211993336678\n",
      "Epoch:2 \n",
      "Iteration:257 \n",
      "Loss:0.06569203734397888\n",
      "Epoch:2 \n",
      "Iteration:258 \n",
      "Loss:0.10015752166509628\n",
      "Epoch:2 \n",
      "Iteration:259 \n",
      "Loss:0.04225564002990723\n",
      "Epoch:2 \n",
      "Iteration:260 \n",
      "Loss:0.10267823189496994\n",
      "Epoch:2 \n",
      "Iteration:261 \n",
      "Loss:0.08296890556812286\n",
      "Epoch:2 \n",
      "Iteration:262 \n",
      "Loss:0.03244716301560402\n",
      "Epoch:2 \n",
      "Iteration:263 \n",
      "Loss:0.18013425171375275\n",
      "Epoch:2 \n",
      "Iteration:264 \n",
      "Loss:0.04046235233545303\n",
      "Epoch:2 \n",
      "Iteration:265 \n",
      "Loss:0.024055728688836098\n",
      "Epoch:2 \n",
      "Iteration:266 \n",
      "Loss:0.11884070187807083\n",
      "Epoch:2 \n",
      "Iteration:267 \n",
      "Loss:0.02998245321214199\n",
      "Epoch:2 \n",
      "Iteration:268 \n",
      "Loss:0.052086878567934036\n",
      "Epoch:2 \n",
      "Iteration:269 \n",
      "Loss:0.09080106765031815\n",
      "Epoch:2 \n",
      "Iteration:270 \n",
      "Loss:0.0629081279039383\n",
      "Epoch:2 \n",
      "Iteration:271 \n",
      "Loss:0.0771857425570488\n",
      "Epoch:2 \n",
      "Iteration:272 \n",
      "Loss:0.018934419378638268\n",
      "Epoch:2 \n",
      "Iteration:273 \n",
      "Loss:0.11793094873428345\n",
      "Epoch:2 \n",
      "Iteration:274 \n",
      "Loss:0.04348869249224663\n",
      "Epoch:2 \n",
      "Iteration:275 \n",
      "Loss:0.09788016974925995\n",
      "Epoch:2 \n",
      "Iteration:276 \n",
      "Loss:0.01922612264752388\n",
      "Epoch:2 \n",
      "Iteration:277 \n",
      "Loss:0.13758008182048798\n",
      "Epoch:2 \n",
      "Iteration:278 \n",
      "Loss:0.022951893508434296\n",
      "Epoch:2 \n",
      "Iteration:279 \n",
      "Loss:0.04002572223544121\n",
      "Epoch:2 \n",
      "Iteration:280 \n",
      "Loss:0.058139171451330185\n",
      "Epoch:2 \n",
      "Iteration:281 \n",
      "Loss:0.05044596642255783\n",
      "Epoch:2 \n",
      "Iteration:282 \n",
      "Loss:0.1323990821838379\n",
      "Epoch:2 \n",
      "Iteration:283 \n",
      "Loss:0.12433477491140366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2 \n",
      "Iteration:284 \n",
      "Loss:0.07417905330657959\n",
      "Epoch:2 \n",
      "Iteration:285 \n",
      "Loss:0.0989016517996788\n",
      "Epoch:2 \n",
      "Iteration:286 \n",
      "Loss:0.029146023094654083\n",
      "Epoch:2 \n",
      "Iteration:287 \n",
      "Loss:0.07612933963537216\n",
      "Epoch:2 \n",
      "Iteration:288 \n",
      "Loss:0.060041218996047974\n",
      "Epoch:2 \n",
      "Iteration:289 \n",
      "Loss:0.13334660232067108\n",
      "Epoch:2 \n",
      "Iteration:290 \n",
      "Loss:0.019690461456775665\n",
      "Epoch:2 \n",
      "Iteration:291 \n",
      "Loss:0.017331814393401146\n",
      "Epoch:2 \n",
      "Iteration:292 \n",
      "Loss:0.07126867771148682\n",
      "Epoch:2 \n",
      "Iteration:293 \n",
      "Loss:0.025734635069966316\n",
      "Epoch:2 \n",
      "Iteration:294 \n",
      "Loss:0.04423552379012108\n",
      "Epoch:2 \n",
      "Iteration:295 \n",
      "Loss:0.03229411318898201\n",
      "Epoch:2 \n",
      "Iteration:296 \n",
      "Loss:0.05551575496792793\n",
      "Epoch:2 \n",
      "Iteration:297 \n",
      "Loss:0.02387768030166626\n",
      "Epoch:2 \n",
      "Iteration:298 \n",
      "Loss:0.12588995695114136\n",
      "Epoch:2 \n",
      "Iteration:299 \n",
      "Loss:0.10413670539855957\n",
      "Epoch:2 \n",
      "Iteration:300 \n",
      "Loss:0.14976704120635986\n",
      "Epoch:2 \n",
      "Iteration:301 \n",
      "Loss:0.14102274179458618\n",
      "Epoch:2 \n",
      "Iteration:302 \n",
      "Loss:0.10587026923894882\n",
      "Epoch:2 \n",
      "Iteration:303 \n",
      "Loss:0.02973896451294422\n",
      "Epoch:2 \n",
      "Iteration:304 \n",
      "Loss:0.02672480046749115\n",
      "Epoch:2 \n",
      "Iteration:305 \n",
      "Loss:0.08879692852497101\n",
      "Epoch:2 \n",
      "Iteration:306 \n",
      "Loss:0.07866974174976349\n",
      "Epoch:2 \n",
      "Iteration:307 \n",
      "Loss:0.2881695628166199\n",
      "Epoch:2 \n",
      "Iteration:308 \n",
      "Loss:0.2988487482070923\n",
      "Epoch:2 \n",
      "Iteration:309 \n",
      "Loss:0.10340525954961777\n",
      "Epoch:2 \n",
      "Iteration:310 \n",
      "Loss:0.1028824970126152\n",
      "Epoch:2 \n",
      "Iteration:311 \n",
      "Loss:0.15129925310611725\n",
      "Epoch:2 \n",
      "Iteration:312 \n",
      "Loss:0.08706580847501755\n",
      "Epoch:2 \n",
      "Iteration:313 \n",
      "Loss:0.16873355209827423\n",
      "Epoch:2 \n",
      "Iteration:314 \n",
      "Loss:0.061414770781993866\n",
      "Epoch:2 \n",
      "Iteration:315 \n",
      "Loss:0.09770325571298599\n",
      "Epoch:2 \n",
      "Iteration:316 \n",
      "Loss:0.13110196590423584\n",
      "Epoch:2 \n",
      "Iteration:317 \n",
      "Loss:0.1932714283466339\n",
      "Epoch:2 \n",
      "Iteration:318 \n",
      "Loss:0.0851825550198555\n",
      "Epoch:2 \n",
      "Iteration:319 \n",
      "Loss:0.08326251804828644\n",
      "Epoch:2 \n",
      "Iteration:320 \n",
      "Loss:0.07439133524894714\n",
      "Epoch:2 \n",
      "Iteration:321 \n",
      "Loss:0.08978495746850967\n",
      "Epoch:2 \n",
      "Iteration:322 \n",
      "Loss:0.19639675319194794\n",
      "Epoch:2 \n",
      "Iteration:323 \n",
      "Loss:0.04275340959429741\n",
      "Epoch:2 \n",
      "Iteration:324 \n",
      "Loss:0.09018409997224808\n",
      "Epoch:2 \n",
      "Iteration:325 \n",
      "Loss:0.05520032346248627\n",
      "Epoch:2 \n",
      "Iteration:326 \n",
      "Loss:0.09071093797683716\n",
      "Epoch:2 \n",
      "Iteration:327 \n",
      "Loss:0.047196563333272934\n",
      "Epoch:2 \n",
      "Iteration:328 \n",
      "Loss:0.06127800792455673\n",
      "Epoch:2 \n",
      "Iteration:329 \n",
      "Loss:0.07389719784259796\n",
      "Epoch:2 \n",
      "Iteration:330 \n",
      "Loss:0.04119421914219856\n",
      "Epoch:2 \n",
      "Iteration:331 \n",
      "Loss:0.08584859222173691\n",
      "Epoch:2 \n",
      "Iteration:332 \n",
      "Loss:0.05179165303707123\n",
      "Epoch:2 \n",
      "Iteration:333 \n",
      "Loss:0.17456145584583282\n",
      "Epoch:2 \n",
      "Iteration:334 \n",
      "Loss:0.07832734286785126\n",
      "Epoch:2 \n",
      "Iteration:335 \n",
      "Loss:0.05842089653015137\n",
      "Epoch:2 \n",
      "Iteration:336 \n",
      "Loss:0.06287868320941925\n",
      "Epoch:2 \n",
      "Iteration:337 \n",
      "Loss:0.06237107887864113\n",
      "Epoch:2 \n",
      "Iteration:338 \n",
      "Loss:0.02793659269809723\n",
      "Epoch:2 \n",
      "Iteration:339 \n",
      "Loss:0.16835179924964905\n",
      "Epoch:2 \n",
      "Iteration:340 \n",
      "Loss:0.13800548017024994\n",
      "Epoch:2 \n",
      "Iteration:341 \n",
      "Loss:0.18857212364673615\n",
      "Epoch:2 \n",
      "Iteration:342 \n",
      "Loss:0.06029169633984566\n",
      "Epoch:2 \n",
      "Iteration:343 \n",
      "Loss:0.04826518893241882\n",
      "Epoch:2 \n",
      "Iteration:344 \n",
      "Loss:0.14393354952335358\n",
      "Epoch:2 \n",
      "Iteration:345 \n",
      "Loss:0.06934497505426407\n",
      "Epoch:2 \n",
      "Iteration:346 \n",
      "Loss:0.11738409847021103\n",
      "Epoch:2 \n",
      "Iteration:347 \n",
      "Loss:0.06733574718236923\n",
      "Epoch:2 \n",
      "Iteration:348 \n",
      "Loss:0.016522157937288284\n",
      "Epoch:2 \n",
      "Iteration:349 \n",
      "Loss:0.08302673697471619\n",
      "Epoch:2 \n",
      "Iteration:350 \n",
      "Loss:0.04427957162261009\n",
      "Epoch:2 \n",
      "Iteration:351 \n",
      "Loss:0.06360725313425064\n",
      "Epoch:2 \n",
      "Iteration:352 \n",
      "Loss:0.13549792766571045\n",
      "Epoch:2 \n",
      "Iteration:353 \n",
      "Loss:0.028411608189344406\n",
      "Epoch:2 \n",
      "Iteration:354 \n",
      "Loss:0.12854833900928497\n",
      "Epoch:2 \n",
      "Iteration:355 \n",
      "Loss:0.14960280060768127\n",
      "Epoch:2 \n",
      "Iteration:356 \n",
      "Loss:0.03722245618700981\n",
      "Epoch:2 \n",
      "Iteration:357 \n",
      "Loss:0.09251126646995544\n",
      "Epoch:2 \n",
      "Iteration:358 \n",
      "Loss:0.06516646593809128\n",
      "Epoch:2 \n",
      "Iteration:359 \n",
      "Loss:0.06562315672636032\n",
      "Epoch:2 \n",
      "Iteration:360 \n",
      "Loss:0.042375531047582626\n",
      "Epoch:2 \n",
      "Iteration:361 \n",
      "Loss:0.06699983775615692\n",
      "Epoch:2 \n",
      "Iteration:362 \n",
      "Loss:0.13150759041309357\n",
      "Epoch:2 \n",
      "Iteration:363 \n",
      "Loss:0.04030489921569824\n",
      "Epoch:2 \n",
      "Iteration:364 \n",
      "Loss:0.08023533970117569\n",
      "Epoch:2 \n",
      "Iteration:365 \n",
      "Loss:0.06807806342840195\n",
      "Epoch:2 \n",
      "Iteration:366 \n",
      "Loss:0.07604335993528366\n",
      "Epoch:2 \n",
      "Iteration:367 \n",
      "Loss:0.06862395256757736\n",
      "Epoch:2 \n",
      "Iteration:368 \n",
      "Loss:0.14980755746364594\n",
      "Epoch:2 \n",
      "Iteration:369 \n",
      "Loss:0.034443341195583344\n",
      "Epoch:2 \n",
      "Iteration:370 \n",
      "Loss:0.04800425469875336\n",
      "Epoch:2 \n",
      "Iteration:371 \n",
      "Loss:0.051439229398965836\n",
      "Epoch:2 \n",
      "Iteration:372 \n",
      "Loss:0.09709055721759796\n",
      "Epoch:2 \n",
      "Iteration:373 \n",
      "Loss:0.08504204452037811\n",
      "Epoch:2 \n",
      "Iteration:374 \n",
      "Loss:0.1123960092663765\n",
      "Epoch:2 \n",
      "Iteration:375 \n",
      "Loss:0.1340862363576889\n",
      "Epoch:2 \n",
      "Iteration:376 \n",
      "Loss:0.07038675248622894\n",
      "Epoch:2 \n",
      "Iteration:377 \n",
      "Loss:0.18167418241500854\n",
      "Epoch:2 \n",
      "Iteration:378 \n",
      "Loss:0.06205783411860466\n",
      "Epoch:2 \n",
      "Iteration:379 \n",
      "Loss:0.0854194164276123\n",
      "Epoch:2 \n",
      "Iteration:380 \n",
      "Loss:0.02837475575506687\n",
      "Epoch:2 \n",
      "Iteration:381 \n",
      "Loss:0.06367925554513931\n",
      "Epoch:2 \n",
      "Iteration:382 \n",
      "Loss:0.03235067427158356\n",
      "Epoch:2 \n",
      "Iteration:383 \n",
      "Loss:0.06539309769868851\n",
      "Epoch:2 \n",
      "Iteration:384 \n",
      "Loss:0.00424795551225543\n",
      "Epoch:2 \n",
      "Iteration:385 \n",
      "Loss:0.03472180664539337\n",
      "Epoch:2 \n",
      "Iteration:386 \n",
      "Loss:0.09082392603158951\n",
      "Epoch:2 \n",
      "Iteration:387 \n",
      "Loss:0.03933430835604668\n",
      "Epoch:2 \n",
      "Iteration:388 \n",
      "Loss:0.17413005232810974\n",
      "Epoch:2 \n",
      "Iteration:389 \n",
      "Loss:0.03767053782939911\n",
      "Epoch:2 \n",
      "Iteration:390 \n",
      "Loss:0.12306584417819977\n",
      "Epoch:2 \n",
      "Iteration:391 \n",
      "Loss:0.03919290006160736\n",
      "Epoch:2 \n",
      "Iteration:392 \n",
      "Loss:0.015977347269654274\n",
      "Epoch:2 \n",
      "Iteration:393 \n",
      "Loss:0.11206331104040146\n",
      "Epoch:2 \n",
      "Iteration:394 \n",
      "Loss:0.09667129814624786\n",
      "Epoch:2 \n",
      "Iteration:395 \n",
      "Loss:0.07877352833747864\n",
      "Epoch:2 \n",
      "Iteration:396 \n",
      "Loss:0.10411285609006882\n",
      "Epoch:2 \n",
      "Iteration:397 \n",
      "Loss:0.06105979532003403\n",
      "Epoch:2 \n",
      "Iteration:398 \n",
      "Loss:0.12042148411273956\n",
      "Epoch:2 \n",
      "Iteration:399 \n",
      "Loss:0.01514794584363699\n",
      "Epoch:2 \n",
      "Iteration:400 \n",
      "Loss:0.0318511500954628\n",
      "Epoch:2 \n",
      "Iteration:401 \n",
      "Loss:0.2070971429347992\n",
      "Epoch:2 \n",
      "Iteration:402 \n",
      "Loss:0.1391669511795044\n",
      "Epoch:2 \n",
      "Iteration:403 \n",
      "Loss:0.03206668049097061\n",
      "Epoch:2 \n",
      "Iteration:404 \n",
      "Loss:0.21106724441051483\n",
      "Epoch:2 \n",
      "Iteration:405 \n",
      "Loss:0.10386157780885696\n",
      "Epoch:2 \n",
      "Iteration:406 \n",
      "Loss:0.01880810223519802\n",
      "Epoch:2 \n",
      "Iteration:407 \n",
      "Loss:0.08602999895811081\n",
      "Epoch:2 \n",
      "Iteration:408 \n",
      "Loss:0.19997011125087738\n",
      "Epoch:2 \n",
      "Iteration:409 \n",
      "Loss:0.055298689752817154\n",
      "Epoch:2 \n",
      "Iteration:410 \n",
      "Loss:0.0473397858440876\n",
      "Epoch:2 \n",
      "Iteration:411 \n",
      "Loss:0.20255659520626068\n",
      "Epoch:2 \n",
      "Iteration:412 \n",
      "Loss:0.045019883662462234\n",
      "Epoch:2 \n",
      "Iteration:413 \n",
      "Loss:0.1812419891357422\n",
      "Epoch:2 \n",
      "Iteration:414 \n",
      "Loss:0.08829294145107269\n",
      "Epoch:2 \n",
      "Iteration:415 \n",
      "Loss:0.10385873913764954\n",
      "Epoch:2 \n",
      "Iteration:416 \n",
      "Loss:0.041494231671094894\n",
      "Epoch:2 \n",
      "Iteration:417 \n",
      "Loss:0.030125033110380173\n",
      "Epoch:2 \n",
      "Iteration:418 \n",
      "Loss:0.07113087177276611\n",
      "Epoch:2 \n",
      "Iteration:419 \n",
      "Loss:0.1513689011335373\n",
      "Epoch:2 \n",
      "Iteration:420 \n",
      "Loss:0.08001048117876053\n",
      "Epoch:2 \n",
      "Iteration:421 \n",
      "Loss:0.08398983627557755\n",
      "Epoch:2 \n",
      "Iteration:422 \n",
      "Loss:0.06247904151678085\n",
      "Epoch:2 \n",
      "Iteration:423 \n",
      "Loss:0.08114616572856903\n",
      "Epoch:2 \n",
      "Iteration:424 \n",
      "Loss:0.04297105222940445\n",
      "Epoch:2 \n",
      "Iteration:425 \n",
      "Loss:0.09259044378995895\n",
      "Epoch:2 \n",
      "Iteration:426 \n",
      "Loss:0.1321702003479004\n",
      "Epoch:2 \n",
      "Iteration:427 \n",
      "Loss:0.17949098348617554\n",
      "Epoch:2 \n",
      "Iteration:428 \n",
      "Loss:0.06876982003450394\n",
      "Epoch:2 \n",
      "Iteration:429 \n",
      "Loss:0.09539525955915451\n",
      "Epoch:2 \n",
      "Iteration:430 \n",
      "Loss:0.10135524719953537\n",
      "Epoch:2 \n",
      "Iteration:431 \n",
      "Loss:0.035346776247024536\n",
      "Epoch:2 \n",
      "Iteration:432 \n",
      "Loss:0.12643378973007202\n",
      "Epoch:2 \n",
      "Iteration:433 \n",
      "Loss:0.02688165195286274\n",
      "Epoch:2 \n",
      "Iteration:434 \n",
      "Loss:0.052182093262672424\n",
      "Epoch:2 \n",
      "Iteration:435 \n",
      "Loss:0.23109851777553558\n",
      "Epoch:2 \n",
      "Iteration:436 \n",
      "Loss:0.1420464664697647\n",
      "Epoch:2 \n",
      "Iteration:437 \n",
      "Loss:0.09991009533405304\n",
      "Epoch:2 \n",
      "Iteration:438 \n",
      "Loss:0.03685600310564041\n",
      "Epoch:2 \n",
      "Iteration:439 \n",
      "Loss:0.10966259986162186\n",
      "Epoch:2 \n",
      "Iteration:440 \n",
      "Loss:0.14692892134189606\n",
      "Epoch:2 \n",
      "Iteration:441 \n",
      "Loss:0.07716568559408188\n",
      "Epoch:2 \n",
      "Iteration:442 \n",
      "Loss:0.10208268463611603\n",
      "Epoch:2 \n",
      "Iteration:443 \n",
      "Loss:0.1128358393907547\n",
      "Epoch:2 \n",
      "Iteration:444 \n",
      "Loss:0.062188971787691116\n",
      "Epoch:2 \n",
      "Iteration:445 \n",
      "Loss:0.08713003993034363\n",
      "Epoch:2 \n",
      "Iteration:446 \n",
      "Loss:0.09090324491262436\n",
      "Epoch:2 \n",
      "Iteration:447 \n",
      "Loss:0.11777570843696594\n",
      "Epoch:2 \n",
      "Iteration:448 \n",
      "Loss:0.061372045427560806\n",
      "Epoch:2 \n",
      "Iteration:449 \n",
      "Loss:0.07631354033946991\n",
      "Epoch:2 \n",
      "Iteration:450 \n",
      "Loss:0.05208823084831238\n",
      "Epoch:2 \n",
      "Iteration:451 \n",
      "Loss:0.08311394602060318\n",
      "Epoch:2 \n",
      "Iteration:452 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0697915107011795\n",
      "Epoch:2 \n",
      "Iteration:453 \n",
      "Loss:0.04550196975469589\n",
      "Epoch:2 \n",
      "Iteration:454 \n",
      "Loss:0.01881393976509571\n",
      "Epoch:2 \n",
      "Iteration:455 \n",
      "Loss:0.09792184829711914\n",
      "Epoch:2 \n",
      "Iteration:456 \n",
      "Loss:0.0425495021045208\n",
      "Epoch:2 \n",
      "Iteration:457 \n",
      "Loss:0.048559416085481644\n",
      "Epoch:2 \n",
      "Iteration:458 \n",
      "Loss:0.20353494584560394\n",
      "Epoch:2 \n",
      "Iteration:459 \n",
      "Loss:0.0651637613773346\n",
      "Epoch:2 \n",
      "Iteration:460 \n",
      "Loss:0.068580262362957\n",
      "Epoch:2 \n",
      "Iteration:461 \n",
      "Loss:0.1157805547118187\n",
      "Epoch:2 \n",
      "Iteration:462 \n",
      "Loss:0.09785594791173935\n",
      "Epoch:2 \n",
      "Iteration:463 \n",
      "Loss:0.03745230287313461\n",
      "Epoch:2 \n",
      "Iteration:464 \n",
      "Loss:0.05954829230904579\n",
      "Epoch:2 \n",
      "Iteration:465 \n",
      "Loss:0.08531931787729263\n",
      "Epoch:2 \n",
      "Iteration:466 \n",
      "Loss:0.027128970250487328\n",
      "Epoch:2 \n",
      "Iteration:467 \n",
      "Loss:0.12149880081415176\n",
      "Epoch:2 \n",
      "Iteration:468 \n",
      "Loss:0.16803333163261414\n",
      "Epoch:2 \n",
      "Iteration:469 \n",
      "Loss:0.05600021407008171\n",
      "Epoch:2 \n",
      "Iteration:470 \n",
      "Loss:0.11090677231550217\n",
      "Epoch:2 \n",
      "Iteration:471 \n",
      "Loss:0.06198808550834656\n",
      "Epoch:2 \n",
      "Iteration:472 \n",
      "Loss:0.10487203299999237\n",
      "Epoch:2 \n",
      "Iteration:473 \n",
      "Loss:0.1134944036602974\n",
      "Epoch:2 \n",
      "Iteration:474 \n",
      "Loss:0.027583608403801918\n",
      "Epoch:2 \n",
      "Iteration:475 \n",
      "Loss:0.03191682696342468\n",
      "Epoch:2 \n",
      "Iteration:476 \n",
      "Loss:0.06409953534603119\n",
      "Epoch:2 \n",
      "Iteration:477 \n",
      "Loss:0.10737903416156769\n",
      "Epoch:2 \n",
      "Iteration:478 \n",
      "Loss:0.16667672991752625\n",
      "Epoch:2 \n",
      "Iteration:479 \n",
      "Loss:0.10453347861766815\n",
      "Epoch:2 \n",
      "Iteration:480 \n",
      "Loss:0.06638959050178528\n",
      "Epoch:2 \n",
      "Iteration:481 \n",
      "Loss:0.16401077806949615\n",
      "Epoch:2 \n",
      "Iteration:482 \n",
      "Loss:0.12214735150337219\n",
      "Epoch:2 \n",
      "Iteration:483 \n",
      "Loss:0.043374136090278625\n",
      "Epoch:2 \n",
      "Iteration:484 \n",
      "Loss:0.06023109331727028\n",
      "Epoch:2 \n",
      "Iteration:485 \n",
      "Loss:0.09209244698286057\n",
      "Epoch:2 \n",
      "Iteration:486 \n",
      "Loss:0.17898482084274292\n",
      "Epoch:2 \n",
      "Iteration:487 \n",
      "Loss:0.10224682837724686\n",
      "Epoch:2 \n",
      "Iteration:488 \n",
      "Loss:0.11113987863063812\n",
      "Epoch:2 \n",
      "Iteration:489 \n",
      "Loss:0.05184522643685341\n",
      "Epoch:2 \n",
      "Iteration:490 \n",
      "Loss:0.07663967460393906\n",
      "Epoch:2 \n",
      "Iteration:491 \n",
      "Loss:0.10011367499828339\n",
      "Epoch:2 \n",
      "Iteration:492 \n",
      "Loss:0.04430067539215088\n",
      "Epoch:2 \n",
      "Iteration:493 \n",
      "Loss:0.10282577574253082\n",
      "Epoch:2 \n",
      "Iteration:494 \n",
      "Loss:0.09932348132133484\n",
      "Epoch:2 \n",
      "Iteration:495 \n",
      "Loss:0.05465708300471306\n",
      "Epoch:2 \n",
      "Iteration:496 \n",
      "Loss:0.15924575924873352\n",
      "Epoch:2 \n",
      "Iteration:497 \n",
      "Loss:0.07463816553354263\n",
      "Epoch:2 \n",
      "Iteration:498 \n",
      "Loss:0.04614052176475525\n",
      "Epoch:2 \n",
      "Iteration:499 \n",
      "Loss:0.10149058699607849\n",
      "Epoch:2 \n",
      "Iteration:500 \n",
      "Loss:0.041597623378038406\n",
      "Epoch:2 \n",
      "Iteration:501 \n",
      "Loss:0.10619933903217316\n",
      "Epoch:2 \n",
      "Iteration:502 \n",
      "Loss:0.11472491919994354\n",
      "Epoch:2 \n",
      "Iteration:503 \n",
      "Loss:0.047853611409664154\n",
      "Epoch:2 \n",
      "Iteration:504 \n",
      "Loss:0.09121329337358475\n",
      "Epoch:2 \n",
      "Iteration:505 \n",
      "Loss:0.07136335968971252\n",
      "Epoch:2 \n",
      "Iteration:506 \n",
      "Loss:0.11915867030620575\n",
      "Epoch:2 \n",
      "Iteration:507 \n",
      "Loss:0.07286311686038971\n",
      "Epoch:2 \n",
      "Iteration:508 \n",
      "Loss:0.013039316982030869\n",
      "Epoch:2 \n",
      "Iteration:509 \n",
      "Loss:0.1344774216413498\n",
      "Epoch:2 \n",
      "Iteration:510 \n",
      "Loss:0.06905459612607956\n",
      "Epoch:2 \n",
      "Iteration:511 \n",
      "Loss:0.16367079317569733\n",
      "Epoch:2 \n",
      "Iteration:512 \n",
      "Loss:0.14452095329761505\n",
      "Epoch:2 \n",
      "Iteration:513 \n",
      "Loss:0.09186919033527374\n",
      "Epoch:2 \n",
      "Iteration:514 \n",
      "Loss:0.18742643296718597\n",
      "Epoch:2 \n",
      "Iteration:515 \n",
      "Loss:0.14501602947711945\n",
      "Epoch:2 \n",
      "Iteration:516 \n",
      "Loss:0.056415166705846786\n",
      "Epoch:2 \n",
      "Iteration:517 \n",
      "Loss:0.15252907574176788\n",
      "Epoch:2 \n",
      "Iteration:518 \n",
      "Loss:0.08370443433523178\n",
      "Epoch:2 \n",
      "Iteration:519 \n",
      "Loss:0.05489034205675125\n",
      "Epoch:2 \n",
      "Iteration:520 \n",
      "Loss:0.12789282202720642\n",
      "Epoch:2 \n",
      "Iteration:521 \n",
      "Loss:0.01347032655030489\n",
      "Epoch:2 \n",
      "Iteration:522 \n",
      "Loss:0.16926449537277222\n",
      "Epoch:2 \n",
      "Iteration:523 \n",
      "Loss:0.15164607763290405\n",
      "Epoch:2 \n",
      "Iteration:524 \n",
      "Loss:0.1280922144651413\n",
      "Epoch:2 \n",
      "Iteration:525 \n",
      "Loss:0.042470403015613556\n",
      "Epoch:2 \n",
      "Iteration:526 \n",
      "Loss:0.04008248448371887\n",
      "Epoch:2 \n",
      "Iteration:527 \n",
      "Loss:0.2344820201396942\n",
      "Epoch:2 \n",
      "Iteration:528 \n",
      "Loss:0.15870089828968048\n",
      "Epoch:2 \n",
      "Iteration:529 \n",
      "Loss:0.02038516290485859\n",
      "Epoch:2 \n",
      "Iteration:530 \n",
      "Loss:0.07264091074466705\n",
      "Epoch:2 \n",
      "Iteration:531 \n",
      "Loss:0.13728106021881104\n",
      "Epoch:2 \n",
      "Iteration:532 \n",
      "Loss:0.10198420286178589\n",
      "Epoch:2 \n",
      "Iteration:533 \n",
      "Loss:0.0652262344956398\n",
      "Epoch:2 \n",
      "Iteration:534 \n",
      "Loss:0.028083300217986107\n",
      "Epoch:2 \n",
      "Iteration:535 \n",
      "Loss:0.08931101113557816\n",
      "Epoch:2 \n",
      "Iteration:536 \n",
      "Loss:0.078989677131176\n",
      "Epoch:2 \n",
      "Iteration:537 \n",
      "Loss:0.05935883894562721\n",
      "Epoch:2 \n",
      "Iteration:538 \n",
      "Loss:0.0985092744231224\n",
      "Epoch:2 \n",
      "Iteration:539 \n",
      "Loss:0.09143944084644318\n",
      "Epoch:2 \n",
      "Iteration:540 \n",
      "Loss:0.11642756313085556\n",
      "Epoch:2 \n",
      "Iteration:541 \n",
      "Loss:0.06259001046419144\n",
      "Epoch:2 \n",
      "Iteration:542 \n",
      "Loss:0.03879936784505844\n",
      "Epoch:2 \n",
      "Iteration:543 \n",
      "Loss:0.10426793992519379\n",
      "Epoch:2 \n",
      "Iteration:544 \n",
      "Loss:0.014819770120084286\n",
      "Epoch:2 \n",
      "Iteration:545 \n",
      "Loss:0.036482661962509155\n",
      "Epoch:2 \n",
      "Iteration:546 \n",
      "Loss:0.13940773904323578\n",
      "Epoch:2 \n",
      "Iteration:547 \n",
      "Loss:0.14229685068130493\n",
      "Epoch:2 \n",
      "Iteration:548 \n",
      "Loss:0.20095893740653992\n",
      "Epoch:2 \n",
      "Iteration:549 \n",
      "Loss:0.11181967705488205\n",
      "Epoch:2 \n",
      "Iteration:550 \n",
      "Loss:0.05977347865700722\n",
      "Epoch:2 \n",
      "Iteration:551 \n",
      "Loss:0.08531860262155533\n",
      "Epoch:2 \n",
      "Iteration:552 \n",
      "Loss:0.06092125549912453\n",
      "Epoch:2 \n",
      "Iteration:553 \n",
      "Loss:0.013145566917955875\n",
      "Epoch:2 \n",
      "Iteration:554 \n",
      "Loss:0.17936508357524872\n",
      "Epoch:2 \n",
      "Iteration:555 \n",
      "Loss:0.11644499748945236\n",
      "Epoch:2 \n",
      "Iteration:556 \n",
      "Loss:0.13941636681556702\n",
      "Epoch:2 \n",
      "Iteration:557 \n",
      "Loss:0.05345474183559418\n",
      "Epoch:2 \n",
      "Iteration:558 \n",
      "Loss:0.08788249641656876\n",
      "Epoch:2 \n",
      "Iteration:559 \n",
      "Loss:0.13953299820423126\n",
      "Epoch:2 \n",
      "Iteration:560 \n",
      "Loss:0.1786438524723053\n",
      "Epoch:2 \n",
      "Iteration:561 \n",
      "Loss:0.15164700150489807\n",
      "Epoch:2 \n",
      "Iteration:562 \n",
      "Loss:0.08470664173364639\n",
      "Epoch:2 \n",
      "Iteration:563 \n",
      "Loss:0.13302446901798248\n",
      "Epoch:2 \n",
      "Iteration:564 \n",
      "Loss:0.10721060633659363\n",
      "Epoch:2 \n",
      "Iteration:565 \n",
      "Loss:0.074092335999012\n",
      "Epoch:2 \n",
      "Iteration:566 \n",
      "Loss:0.13409669697284698\n",
      "Epoch:2 \n",
      "Iteration:567 \n",
      "Loss:0.042138777673244476\n",
      "Epoch:2 \n",
      "Iteration:568 \n",
      "Loss:0.12802094221115112\n",
      "Epoch:2 \n",
      "Iteration:569 \n",
      "Loss:0.1266195923089981\n",
      "Epoch:2 \n",
      "Iteration:570 \n",
      "Loss:0.0965368002653122\n",
      "Epoch:2 \n",
      "Iteration:571 \n",
      "Loss:0.06095511093735695\n",
      "Epoch:2 \n",
      "Iteration:572 \n",
      "Loss:0.10491208732128143\n",
      "Epoch:2 \n",
      "Iteration:573 \n",
      "Loss:0.049069810658693314\n",
      "Epoch:2 \n",
      "Iteration:574 \n",
      "Loss:0.06768140941858292\n",
      "Epoch:2 \n",
      "Iteration:575 \n",
      "Loss:0.03246474266052246\n",
      "Epoch:2 \n",
      "Iteration:576 \n",
      "Loss:0.11883799731731415\n",
      "Epoch:2 \n",
      "Iteration:577 \n",
      "Loss:0.06053781881928444\n",
      "Epoch:2 \n",
      "Iteration:578 \n",
      "Loss:0.019635546952486038\n",
      "Epoch:2 \n",
      "Iteration:579 \n",
      "Loss:0.09838902205228806\n",
      "Epoch:2 \n",
      "Iteration:580 \n",
      "Loss:0.16635048389434814\n",
      "Epoch:2 \n",
      "Iteration:581 \n",
      "Loss:0.08783382177352905\n",
      "Epoch:2 \n",
      "Iteration:582 \n",
      "Loss:0.046604935079813004\n",
      "Epoch:2 \n",
      "Iteration:583 \n",
      "Loss:0.06369833648204803\n",
      "Epoch:2 \n",
      "Iteration:584 \n",
      "Loss:0.018946215510368347\n",
      "Epoch:2 \n",
      "Iteration:585 \n",
      "Loss:0.09667161852121353\n",
      "Epoch:2 \n",
      "Iteration:586 \n",
      "Loss:0.12447510659694672\n",
      "Epoch:2 \n",
      "Iteration:587 \n",
      "Loss:0.1134222224354744\n",
      "Epoch:2 \n",
      "Iteration:588 \n",
      "Loss:0.0619090236723423\n",
      "Epoch:2 \n",
      "Iteration:589 \n",
      "Loss:0.03641628101468086\n",
      "Epoch:2 \n",
      "Iteration:590 \n",
      "Loss:0.08511552959680557\n",
      "Epoch:2 \n",
      "Iteration:591 \n",
      "Loss:0.13629785180091858\n",
      "Epoch:2 \n",
      "Iteration:592 \n",
      "Loss:0.037782732397317886\n",
      "Epoch:2 \n",
      "Iteration:593 \n",
      "Loss:0.12178778648376465\n",
      "Epoch:2 \n",
      "Iteration:594 \n",
      "Loss:0.027897292748093605\n",
      "Epoch:2 \n",
      "Iteration:595 \n",
      "Loss:0.0259077288210392\n",
      "Epoch:2 \n",
      "Iteration:596 \n",
      "Loss:0.026701796799898148\n",
      "Epoch:2 \n",
      "Iteration:597 \n",
      "Loss:0.04176654666662216\n",
      "Epoch:2 \n",
      "Iteration:598 \n",
      "Loss:0.03215411677956581\n",
      "Epoch:2 \n",
      "Iteration:599 \n",
      "Loss:0.024355784058570862\n",
      "Epoch:2 \n",
      "Iteration:600 \n",
      "Loss:0.10602573305368423\n",
      "\n",
      "Accuracy of network in epoch 2: 97.215\n",
      "Epoch:3 \n",
      "Iteration:1 \n",
      "Loss:0.06507548689842224\n",
      "Epoch:3 \n",
      "Iteration:2 \n",
      "Loss:0.05685470625758171\n",
      "Epoch:3 \n",
      "Iteration:3 \n",
      "Loss:0.050836361944675446\n",
      "Epoch:3 \n",
      "Iteration:4 \n",
      "Loss:0.1531015932559967\n",
      "Epoch:3 \n",
      "Iteration:5 \n",
      "Loss:0.05276000499725342\n",
      "Epoch:3 \n",
      "Iteration:6 \n",
      "Loss:0.040957726538181305\n",
      "Epoch:3 \n",
      "Iteration:7 \n",
      "Loss:0.009984851814806461\n",
      "Epoch:3 \n",
      "Iteration:8 \n",
      "Loss:0.07752212882041931\n",
      "Epoch:3 \n",
      "Iteration:9 \n",
      "Loss:0.055770035833120346\n",
      "Epoch:3 \n",
      "Iteration:10 \n",
      "Loss:0.011395781300961971\n",
      "Epoch:3 \n",
      "Iteration:11 \n",
      "Loss:0.12790292501449585\n",
      "Epoch:3 \n",
      "Iteration:12 \n",
      "Loss:0.026682408526539803\n",
      "Epoch:3 \n",
      "Iteration:13 \n",
      "Loss:0.061499666422605515\n",
      "Epoch:3 \n",
      "Iteration:14 \n",
      "Loss:0.061734963208436966\n",
      "Epoch:3 \n",
      "Iteration:15 \n",
      "Loss:0.11611274629831314\n",
      "Epoch:3 \n",
      "Iteration:16 \n",
      "Loss:0.14121262729167938\n",
      "Epoch:3 \n",
      "Iteration:17 \n",
      "Loss:0.027529515326023102\n",
      "Epoch:3 \n",
      "Iteration:18 \n",
      "Loss:0.0688527449965477\n",
      "Epoch:3 \n",
      "Iteration:19 \n",
      "Loss:0.05044511705636978\n",
      "Epoch:3 \n",
      "Iteration:20 \n",
      "Loss:0.05832422897219658\n",
      "Epoch:3 \n",
      "Iteration:21 \n",
      "Loss:0.06288693100214005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3 \n",
      "Iteration:22 \n",
      "Loss:0.1361725628376007\n",
      "Epoch:3 \n",
      "Iteration:23 \n",
      "Loss:0.08357694000005722\n",
      "Epoch:3 \n",
      "Iteration:24 \n",
      "Loss:0.008071056567132473\n",
      "Epoch:3 \n",
      "Iteration:25 \n",
      "Loss:0.019149335101246834\n",
      "Epoch:3 \n",
      "Iteration:26 \n",
      "Loss:0.004059730097651482\n",
      "Epoch:3 \n",
      "Iteration:27 \n",
      "Loss:0.046423234045505524\n",
      "Epoch:3 \n",
      "Iteration:28 \n",
      "Loss:0.08173656463623047\n",
      "Epoch:3 \n",
      "Iteration:29 \n",
      "Loss:0.14501319825649261\n",
      "Epoch:3 \n",
      "Iteration:30 \n",
      "Loss:0.06762338429689407\n",
      "Epoch:3 \n",
      "Iteration:31 \n",
      "Loss:0.06413081288337708\n",
      "Epoch:3 \n",
      "Iteration:32 \n",
      "Loss:0.0186809953302145\n",
      "Epoch:3 \n",
      "Iteration:33 \n",
      "Loss:0.055727168917655945\n",
      "Epoch:3 \n",
      "Iteration:34 \n",
      "Loss:0.08849415183067322\n",
      "Epoch:3 \n",
      "Iteration:35 \n",
      "Loss:0.08858504891395569\n",
      "Epoch:3 \n",
      "Iteration:36 \n",
      "Loss:0.07735386490821838\n",
      "Epoch:3 \n",
      "Iteration:37 \n",
      "Loss:0.06992447376251221\n",
      "Epoch:3 \n",
      "Iteration:38 \n",
      "Loss:0.047465477138757706\n",
      "Epoch:3 \n",
      "Iteration:39 \n",
      "Loss:0.10266265273094177\n",
      "Epoch:3 \n",
      "Iteration:40 \n",
      "Loss:0.05181694030761719\n",
      "Epoch:3 \n",
      "Iteration:41 \n",
      "Loss:0.018204815685749054\n",
      "Epoch:3 \n",
      "Iteration:42 \n",
      "Loss:0.07821105420589447\n",
      "Epoch:3 \n",
      "Iteration:43 \n",
      "Loss:0.029496340081095695\n",
      "Epoch:3 \n",
      "Iteration:44 \n",
      "Loss:0.06150510907173157\n",
      "Epoch:3 \n",
      "Iteration:45 \n",
      "Loss:0.09743770956993103\n",
      "Epoch:3 \n",
      "Iteration:46 \n",
      "Loss:0.023362793028354645\n",
      "Epoch:3 \n",
      "Iteration:47 \n",
      "Loss:0.012773985043168068\n",
      "Epoch:3 \n",
      "Iteration:48 \n",
      "Loss:0.026919851079583168\n",
      "Epoch:3 \n",
      "Iteration:49 \n",
      "Loss:0.05501348897814751\n",
      "Epoch:3 \n",
      "Iteration:50 \n",
      "Loss:0.03430558741092682\n",
      "Epoch:3 \n",
      "Iteration:51 \n",
      "Loss:0.06731242686510086\n",
      "Epoch:3 \n",
      "Iteration:52 \n",
      "Loss:0.02945077419281006\n",
      "Epoch:3 \n",
      "Iteration:53 \n",
      "Loss:0.025796061381697655\n",
      "Epoch:3 \n",
      "Iteration:54 \n",
      "Loss:0.036317549645900726\n",
      "Epoch:3 \n",
      "Iteration:55 \n",
      "Loss:0.08765850216150284\n",
      "Epoch:3 \n",
      "Iteration:56 \n",
      "Loss:0.03827374801039696\n",
      "Epoch:3 \n",
      "Iteration:57 \n",
      "Loss:0.10664905607700348\n",
      "Epoch:3 \n",
      "Iteration:58 \n",
      "Loss:0.01799718104302883\n",
      "Epoch:3 \n",
      "Iteration:59 \n",
      "Loss:0.0073890988714993\n",
      "Epoch:3 \n",
      "Iteration:60 \n",
      "Loss:0.06816842406988144\n",
      "Epoch:3 \n",
      "Iteration:61 \n",
      "Loss:0.09402107447385788\n",
      "Epoch:3 \n",
      "Iteration:62 \n",
      "Loss:0.04320809990167618\n",
      "Epoch:3 \n",
      "Iteration:63 \n",
      "Loss:0.02976069040596485\n",
      "Epoch:3 \n",
      "Iteration:64 \n",
      "Loss:0.08735881745815277\n",
      "Epoch:3 \n",
      "Iteration:65 \n",
      "Loss:0.07688412070274353\n",
      "Epoch:3 \n",
      "Iteration:66 \n",
      "Loss:0.06768199056386948\n",
      "Epoch:3 \n",
      "Iteration:67 \n",
      "Loss:0.023938028141856194\n",
      "Epoch:3 \n",
      "Iteration:68 \n",
      "Loss:0.05929949879646301\n",
      "Epoch:3 \n",
      "Iteration:69 \n",
      "Loss:0.02841467782855034\n",
      "Epoch:3 \n",
      "Iteration:70 \n",
      "Loss:0.1679990440607071\n",
      "Epoch:3 \n",
      "Iteration:71 \n",
      "Loss:0.12365128844976425\n",
      "Epoch:3 \n",
      "Iteration:72 \n",
      "Loss:0.2302166223526001\n",
      "Epoch:3 \n",
      "Iteration:73 \n",
      "Loss:0.09471377730369568\n",
      "Epoch:3 \n",
      "Iteration:74 \n",
      "Loss:0.05234023556113243\n",
      "Epoch:3 \n",
      "Iteration:75 \n",
      "Loss:0.07776690274477005\n",
      "Epoch:3 \n",
      "Iteration:76 \n",
      "Loss:0.03717222809791565\n",
      "Epoch:3 \n",
      "Iteration:77 \n",
      "Loss:0.12611238658428192\n",
      "Epoch:3 \n",
      "Iteration:78 \n",
      "Loss:0.049026697874069214\n",
      "Epoch:3 \n",
      "Iteration:79 \n",
      "Loss:0.04879935085773468\n",
      "Epoch:3 \n",
      "Iteration:80 \n",
      "Loss:0.03330663591623306\n",
      "Epoch:3 \n",
      "Iteration:81 \n",
      "Loss:0.053591154515743256\n",
      "Epoch:3 \n",
      "Iteration:82 \n",
      "Loss:0.05310116708278656\n",
      "Epoch:3 \n",
      "Iteration:83 \n",
      "Loss:0.07160241901874542\n",
      "Epoch:3 \n",
      "Iteration:84 \n",
      "Loss:0.046649474650621414\n",
      "Epoch:3 \n",
      "Iteration:85 \n",
      "Loss:0.06963767111301422\n",
      "Epoch:3 \n",
      "Iteration:86 \n",
      "Loss:0.06221383064985275\n",
      "Epoch:3 \n",
      "Iteration:87 \n",
      "Loss:0.037910059094429016\n",
      "Epoch:3 \n",
      "Iteration:88 \n",
      "Loss:0.054913368076086044\n",
      "Epoch:3 \n",
      "Iteration:89 \n",
      "Loss:0.10284756869077682\n",
      "Epoch:3 \n",
      "Iteration:90 \n",
      "Loss:0.022256288677453995\n",
      "Epoch:3 \n",
      "Iteration:91 \n",
      "Loss:0.05771421268582344\n",
      "Epoch:3 \n",
      "Iteration:92 \n",
      "Loss:0.06307995319366455\n",
      "Epoch:3 \n",
      "Iteration:93 \n",
      "Loss:0.08350738883018494\n",
      "Epoch:3 \n",
      "Iteration:94 \n",
      "Loss:0.04036369174718857\n",
      "Epoch:3 \n",
      "Iteration:95 \n",
      "Loss:0.10758436471223831\n",
      "Epoch:3 \n",
      "Iteration:96 \n",
      "Loss:0.052400678396224976\n",
      "Epoch:3 \n",
      "Iteration:97 \n",
      "Loss:0.035749778151512146\n",
      "Epoch:3 \n",
      "Iteration:98 \n",
      "Loss:0.0746956467628479\n",
      "Epoch:3 \n",
      "Iteration:99 \n",
      "Loss:0.01734868809580803\n",
      "Epoch:3 \n",
      "Iteration:100 \n",
      "Loss:0.030712759122252464\n",
      "Epoch:3 \n",
      "Iteration:101 \n",
      "Loss:0.04040813073515892\n",
      "Epoch:3 \n",
      "Iteration:102 \n",
      "Loss:0.0280124731361866\n",
      "Epoch:3 \n",
      "Iteration:103 \n",
      "Loss:0.0047061508521437645\n",
      "Epoch:3 \n",
      "Iteration:104 \n",
      "Loss:0.032903045415878296\n",
      "Epoch:3 \n",
      "Iteration:105 \n",
      "Loss:0.0678003579378128\n",
      "Epoch:3 \n",
      "Iteration:106 \n",
      "Loss:0.07531868666410446\n",
      "Epoch:3 \n",
      "Iteration:107 \n",
      "Loss:0.011015328578650951\n",
      "Epoch:3 \n",
      "Iteration:108 \n",
      "Loss:0.08877000957727432\n",
      "Epoch:3 \n",
      "Iteration:109 \n",
      "Loss:0.039238158613443375\n",
      "Epoch:3 \n",
      "Iteration:110 \n",
      "Loss:0.06281284987926483\n",
      "Epoch:3 \n",
      "Iteration:111 \n",
      "Loss:0.009448271244764328\n",
      "Epoch:3 \n",
      "Iteration:112 \n",
      "Loss:0.011291783303022385\n",
      "Epoch:3 \n",
      "Iteration:113 \n",
      "Loss:0.14380235970020294\n",
      "Epoch:3 \n",
      "Iteration:114 \n",
      "Loss:0.04733504354953766\n",
      "Epoch:3 \n",
      "Iteration:115 \n",
      "Loss:0.0742424800992012\n",
      "Epoch:3 \n",
      "Iteration:116 \n",
      "Loss:0.03219160810112953\n",
      "Epoch:3 \n",
      "Iteration:117 \n",
      "Loss:0.08142344653606415\n",
      "Epoch:3 \n",
      "Iteration:118 \n",
      "Loss:0.0178996492177248\n",
      "Epoch:3 \n",
      "Iteration:119 \n",
      "Loss:0.023358721286058426\n",
      "Epoch:3 \n",
      "Iteration:120 \n",
      "Loss:0.043391358107328415\n",
      "Epoch:3 \n",
      "Iteration:121 \n",
      "Loss:0.006980248261243105\n",
      "Epoch:3 \n",
      "Iteration:122 \n",
      "Loss:0.03172539547085762\n",
      "Epoch:3 \n",
      "Iteration:123 \n",
      "Loss:0.025352146476507187\n",
      "Epoch:3 \n",
      "Iteration:124 \n",
      "Loss:0.10236527770757675\n",
      "Epoch:3 \n",
      "Iteration:125 \n",
      "Loss:0.03848658502101898\n",
      "Epoch:3 \n",
      "Iteration:126 \n",
      "Loss:0.118221215903759\n",
      "Epoch:3 \n",
      "Iteration:127 \n",
      "Loss:0.06300851702690125\n",
      "Epoch:3 \n",
      "Iteration:128 \n",
      "Loss:0.05326468497514725\n",
      "Epoch:3 \n",
      "Iteration:129 \n",
      "Loss:0.040979254990816116\n",
      "Epoch:3 \n",
      "Iteration:130 \n",
      "Loss:0.065855473279953\n",
      "Epoch:3 \n",
      "Iteration:131 \n",
      "Loss:0.1450963169336319\n",
      "Epoch:3 \n",
      "Iteration:132 \n",
      "Loss:0.06824100762605667\n",
      "Epoch:3 \n",
      "Iteration:133 \n",
      "Loss:0.01313074491918087\n",
      "Epoch:3 \n",
      "Iteration:134 \n",
      "Loss:0.02532646618783474\n",
      "Epoch:3 \n",
      "Iteration:135 \n",
      "Loss:0.011330613866448402\n",
      "Epoch:3 \n",
      "Iteration:136 \n",
      "Loss:0.12227289378643036\n",
      "Epoch:3 \n",
      "Iteration:137 \n",
      "Loss:0.042571473866701126\n",
      "Epoch:3 \n",
      "Iteration:138 \n",
      "Loss:0.05427588149905205\n",
      "Epoch:3 \n",
      "Iteration:139 \n",
      "Loss:0.08714954555034637\n",
      "Epoch:3 \n",
      "Iteration:140 \n",
      "Loss:0.19018897414207458\n",
      "Epoch:3 \n",
      "Iteration:141 \n",
      "Loss:0.033999938517808914\n",
      "Epoch:3 \n",
      "Iteration:142 \n",
      "Loss:0.07070242613554001\n",
      "Epoch:3 \n",
      "Iteration:143 \n",
      "Loss:0.05443559214472771\n",
      "Epoch:3 \n",
      "Iteration:144 \n",
      "Loss:0.011954161338508129\n",
      "Epoch:3 \n",
      "Iteration:145 \n",
      "Loss:0.043753813952207565\n",
      "Epoch:3 \n",
      "Iteration:146 \n",
      "Loss:0.036113590002059937\n",
      "Epoch:3 \n",
      "Iteration:147 \n",
      "Loss:0.03572002425789833\n",
      "Epoch:3 \n",
      "Iteration:148 \n",
      "Loss:0.020305495709180832\n",
      "Epoch:3 \n",
      "Iteration:149 \n",
      "Loss:0.13291138410568237\n",
      "Epoch:3 \n",
      "Iteration:150 \n",
      "Loss:0.05774228647351265\n",
      "Epoch:3 \n",
      "Iteration:151 \n",
      "Loss:0.09012045711278915\n",
      "Epoch:3 \n",
      "Iteration:152 \n",
      "Loss:0.07675786316394806\n",
      "Epoch:3 \n",
      "Iteration:153 \n",
      "Loss:0.027684856206178665\n",
      "Epoch:3 \n",
      "Iteration:154 \n",
      "Loss:0.047280844300985336\n",
      "Epoch:3 \n",
      "Iteration:155 \n",
      "Loss:0.03487660735845566\n",
      "Epoch:3 \n",
      "Iteration:156 \n",
      "Loss:0.03292856365442276\n",
      "Epoch:3 \n",
      "Iteration:157 \n",
      "Loss:0.05858949199318886\n",
      "Epoch:3 \n",
      "Iteration:158 \n",
      "Loss:0.08673455566167831\n",
      "Epoch:3 \n",
      "Iteration:159 \n",
      "Loss:0.03973165899515152\n",
      "Epoch:3 \n",
      "Iteration:160 \n",
      "Loss:0.1479998528957367\n",
      "Epoch:3 \n",
      "Iteration:161 \n",
      "Loss:0.04793471470475197\n",
      "Epoch:3 \n",
      "Iteration:162 \n",
      "Loss:0.025470953434705734\n",
      "Epoch:3 \n",
      "Iteration:163 \n",
      "Loss:0.06881735473871231\n",
      "Epoch:3 \n",
      "Iteration:164 \n",
      "Loss:0.017351467162370682\n",
      "Epoch:3 \n",
      "Iteration:165 \n",
      "Loss:0.09779451042413712\n",
      "Epoch:3 \n",
      "Iteration:166 \n",
      "Loss:0.040784209966659546\n",
      "Epoch:3 \n",
      "Iteration:167 \n",
      "Loss:0.07346498966217041\n",
      "Epoch:3 \n",
      "Iteration:168 \n",
      "Loss:0.021280502900481224\n",
      "Epoch:3 \n",
      "Iteration:169 \n",
      "Loss:0.07501194626092911\n",
      "Epoch:3 \n",
      "Iteration:170 \n",
      "Loss:0.0683286115527153\n",
      "Epoch:3 \n",
      "Iteration:171 \n",
      "Loss:0.046312157064676285\n",
      "Epoch:3 \n",
      "Iteration:172 \n",
      "Loss:0.013812290504574776\n",
      "Epoch:3 \n",
      "Iteration:173 \n",
      "Loss:0.08569474518299103\n",
      "Epoch:3 \n",
      "Iteration:174 \n",
      "Loss:0.08229739964008331\n",
      "Epoch:3 \n",
      "Iteration:175 \n",
      "Loss:0.09575134515762329\n",
      "Epoch:3 \n",
      "Iteration:176 \n",
      "Loss:0.05939517542719841\n",
      "Epoch:3 \n",
      "Iteration:177 \n",
      "Loss:0.04195753112435341\n",
      "Epoch:3 \n",
      "Iteration:178 \n",
      "Loss:0.07900586724281311\n",
      "Epoch:3 \n",
      "Iteration:179 \n",
      "Loss:0.11260567605495453\n",
      "Epoch:3 \n",
      "Iteration:180 \n",
      "Loss:0.04570407047867775\n",
      "Epoch:3 \n",
      "Iteration:181 \n",
      "Loss:0.11269282549619675\n",
      "Epoch:3 \n",
      "Iteration:182 \n",
      "Loss:0.046963516622781754\n",
      "Epoch:3 \n",
      "Iteration:183 \n",
      "Loss:0.053563155233860016\n",
      "Epoch:3 \n",
      "Iteration:184 \n",
      "Loss:0.14870327711105347\n",
      "Epoch:3 \n",
      "Iteration:185 \n",
      "Loss:0.022914163768291473\n",
      "Epoch:3 \n",
      "Iteration:186 \n",
      "Loss:0.049962759017944336\n",
      "Epoch:3 \n",
      "Iteration:187 \n",
      "Loss:0.0712253674864769\n",
      "Epoch:3 \n",
      "Iteration:188 \n",
      "Loss:0.1353616863489151\n",
      "Epoch:3 \n",
      "Iteration:189 \n",
      "Loss:0.043921440839767456\n",
      "Epoch:3 \n",
      "Iteration:190 \n",
      "Loss:0.09061047434806824\n",
      "Epoch:3 \n",
      "Iteration:191 \n",
      "Loss:0.22199466824531555\n",
      "Epoch:3 \n",
      "Iteration:192 \n",
      "Loss:0.07586213201284409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3 \n",
      "Iteration:193 \n",
      "Loss:0.0725313276052475\n",
      "Epoch:3 \n",
      "Iteration:194 \n",
      "Loss:0.047340258955955505\n",
      "Epoch:3 \n",
      "Iteration:195 \n",
      "Loss:0.10653276741504669\n",
      "Epoch:3 \n",
      "Iteration:196 \n",
      "Loss:0.09254342317581177\n",
      "Epoch:3 \n",
      "Iteration:197 \n",
      "Loss:0.0783831924200058\n",
      "Epoch:3 \n",
      "Iteration:198 \n",
      "Loss:0.0283550713211298\n",
      "Epoch:3 \n",
      "Iteration:199 \n",
      "Loss:0.029116563498973846\n",
      "Epoch:3 \n",
      "Iteration:200 \n",
      "Loss:0.0898512527346611\n",
      "Epoch:3 \n",
      "Iteration:201 \n",
      "Loss:0.07754413038492203\n",
      "Epoch:3 \n",
      "Iteration:202 \n",
      "Loss:0.27233198285102844\n",
      "Epoch:3 \n",
      "Iteration:203 \n",
      "Loss:0.018377184867858887\n",
      "Epoch:3 \n",
      "Iteration:204 \n",
      "Loss:0.04528307542204857\n",
      "Epoch:3 \n",
      "Iteration:205 \n",
      "Loss:0.02606377750635147\n",
      "Epoch:3 \n",
      "Iteration:206 \n",
      "Loss:0.0827031210064888\n",
      "Epoch:3 \n",
      "Iteration:207 \n",
      "Loss:0.08707594126462936\n",
      "Epoch:3 \n",
      "Iteration:208 \n",
      "Loss:0.10203990340232849\n",
      "Epoch:3 \n",
      "Iteration:209 \n",
      "Loss:0.05694695934653282\n",
      "Epoch:3 \n",
      "Iteration:210 \n",
      "Loss:0.0983322337269783\n",
      "Epoch:3 \n",
      "Iteration:211 \n",
      "Loss:0.0763617530465126\n",
      "Epoch:3 \n",
      "Iteration:212 \n",
      "Loss:0.04667603597044945\n",
      "Epoch:3 \n",
      "Iteration:213 \n",
      "Loss:0.12779290974140167\n",
      "Epoch:3 \n",
      "Iteration:214 \n",
      "Loss:0.09448330104351044\n",
      "Epoch:3 \n",
      "Iteration:215 \n",
      "Loss:0.013670090585947037\n",
      "Epoch:3 \n",
      "Iteration:216 \n",
      "Loss:0.021754853427410126\n",
      "Epoch:3 \n",
      "Iteration:217 \n",
      "Loss:0.18197712302207947\n",
      "Epoch:3 \n",
      "Iteration:218 \n",
      "Loss:0.10635692626237869\n",
      "Epoch:3 \n",
      "Iteration:219 \n",
      "Loss:0.06416255235671997\n",
      "Epoch:3 \n",
      "Iteration:220 \n",
      "Loss:0.06289878487586975\n",
      "Epoch:3 \n",
      "Iteration:221 \n",
      "Loss:0.046780314296483994\n",
      "Epoch:3 \n",
      "Iteration:222 \n",
      "Loss:0.17944437265396118\n",
      "Epoch:3 \n",
      "Iteration:223 \n",
      "Loss:0.04472976177930832\n",
      "Epoch:3 \n",
      "Iteration:224 \n",
      "Loss:0.024951009079813957\n",
      "Epoch:3 \n",
      "Iteration:225 \n",
      "Loss:0.05295839160680771\n",
      "Epoch:3 \n",
      "Iteration:226 \n",
      "Loss:0.07977861166000366\n",
      "Epoch:3 \n",
      "Iteration:227 \n",
      "Loss:0.051057200878858566\n",
      "Epoch:3 \n",
      "Iteration:228 \n",
      "Loss:0.06329610198736191\n",
      "Epoch:3 \n",
      "Iteration:229 \n",
      "Loss:0.032405558973550797\n",
      "Epoch:3 \n",
      "Iteration:230 \n",
      "Loss:0.03913480043411255\n",
      "Epoch:3 \n",
      "Iteration:231 \n",
      "Loss:0.05912960693240166\n",
      "Epoch:3 \n",
      "Iteration:232 \n",
      "Loss:0.04702155664563179\n",
      "Epoch:3 \n",
      "Iteration:233 \n",
      "Loss:0.028853679075837135\n",
      "Epoch:3 \n",
      "Iteration:234 \n",
      "Loss:0.0560724213719368\n",
      "Epoch:3 \n",
      "Iteration:235 \n",
      "Loss:0.08401910960674286\n",
      "Epoch:3 \n",
      "Iteration:236 \n",
      "Loss:0.07216251641511917\n",
      "Epoch:3 \n",
      "Iteration:237 \n",
      "Loss:0.06714190542697906\n",
      "Epoch:3 \n",
      "Iteration:238 \n",
      "Loss:0.018573889508843422\n",
      "Epoch:3 \n",
      "Iteration:239 \n",
      "Loss:0.027786126360297203\n",
      "Epoch:3 \n",
      "Iteration:240 \n",
      "Loss:0.06321242451667786\n",
      "Epoch:3 \n",
      "Iteration:241 \n",
      "Loss:0.07703101634979248\n",
      "Epoch:3 \n",
      "Iteration:242 \n",
      "Loss:0.02014216221868992\n",
      "Epoch:3 \n",
      "Iteration:243 \n",
      "Loss:0.12333828210830688\n",
      "Epoch:3 \n",
      "Iteration:244 \n",
      "Loss:0.08558963984251022\n",
      "Epoch:3 \n",
      "Iteration:245 \n",
      "Loss:0.06808220595121384\n",
      "Epoch:3 \n",
      "Iteration:246 \n",
      "Loss:0.07930892705917358\n",
      "Epoch:3 \n",
      "Iteration:247 \n",
      "Loss:0.04545634239912033\n",
      "Epoch:3 \n",
      "Iteration:248 \n",
      "Loss:0.027262378484010696\n",
      "Epoch:3 \n",
      "Iteration:249 \n",
      "Loss:0.005780881270766258\n",
      "Epoch:3 \n",
      "Iteration:250 \n",
      "Loss:0.0859537348151207\n",
      "Epoch:3 \n",
      "Iteration:251 \n",
      "Loss:0.1368052363395691\n",
      "Epoch:3 \n",
      "Iteration:252 \n",
      "Loss:0.05443878471851349\n",
      "Epoch:3 \n",
      "Iteration:253 \n",
      "Loss:0.04661558195948601\n",
      "Epoch:3 \n",
      "Iteration:254 \n",
      "Loss:0.012230098247528076\n",
      "Epoch:3 \n",
      "Iteration:255 \n",
      "Loss:0.05395623669028282\n",
      "Epoch:3 \n",
      "Iteration:256 \n",
      "Loss:0.04030292108654976\n",
      "Epoch:3 \n",
      "Iteration:257 \n",
      "Loss:0.13883548974990845\n",
      "Epoch:3 \n",
      "Iteration:258 \n",
      "Loss:0.041905637830495834\n",
      "Epoch:3 \n",
      "Iteration:259 \n",
      "Loss:0.04805561527609825\n",
      "Epoch:3 \n",
      "Iteration:260 \n",
      "Loss:0.005061304196715355\n",
      "Epoch:3 \n",
      "Iteration:261 \n",
      "Loss:0.01223828550428152\n",
      "Epoch:3 \n",
      "Iteration:262 \n",
      "Loss:0.06648411601781845\n",
      "Epoch:3 \n",
      "Iteration:263 \n",
      "Loss:0.06587139517068863\n",
      "Epoch:3 \n",
      "Iteration:264 \n",
      "Loss:0.04736965522170067\n",
      "Epoch:3 \n",
      "Iteration:265 \n",
      "Loss:0.18263986706733704\n",
      "Epoch:3 \n",
      "Iteration:266 \n",
      "Loss:0.08784440904855728\n",
      "Epoch:3 \n",
      "Iteration:267 \n",
      "Loss:0.10307471454143524\n",
      "Epoch:3 \n",
      "Iteration:268 \n",
      "Loss:0.023594481870532036\n",
      "Epoch:3 \n",
      "Iteration:269 \n",
      "Loss:0.07121305912733078\n",
      "Epoch:3 \n",
      "Iteration:270 \n",
      "Loss:0.11043193936347961\n",
      "Epoch:3 \n",
      "Iteration:271 \n",
      "Loss:0.0723830983042717\n",
      "Epoch:3 \n",
      "Iteration:272 \n",
      "Loss:0.035881590098142624\n",
      "Epoch:3 \n",
      "Iteration:273 \n",
      "Loss:0.05593635141849518\n",
      "Epoch:3 \n",
      "Iteration:274 \n",
      "Loss:0.14742034673690796\n",
      "Epoch:3 \n",
      "Iteration:275 \n",
      "Loss:0.107148677110672\n",
      "Epoch:3 \n",
      "Iteration:276 \n",
      "Loss:0.02263864129781723\n",
      "Epoch:3 \n",
      "Iteration:277 \n",
      "Loss:0.15307112038135529\n",
      "Epoch:3 \n",
      "Iteration:278 \n",
      "Loss:0.04787875711917877\n",
      "Epoch:3 \n",
      "Iteration:279 \n",
      "Loss:0.01562538929283619\n",
      "Epoch:3 \n",
      "Iteration:280 \n",
      "Loss:0.05024971812963486\n",
      "Epoch:3 \n",
      "Iteration:281 \n",
      "Loss:0.09709260612726212\n",
      "Epoch:3 \n",
      "Iteration:282 \n",
      "Loss:0.11517886817455292\n",
      "Epoch:3 \n",
      "Iteration:283 \n",
      "Loss:0.02666594460606575\n",
      "Epoch:3 \n",
      "Iteration:284 \n",
      "Loss:0.10835277289152145\n",
      "Epoch:3 \n",
      "Iteration:285 \n",
      "Loss:0.06984353065490723\n",
      "Epoch:3 \n",
      "Iteration:286 \n",
      "Loss:0.0071766371838748455\n",
      "Epoch:3 \n",
      "Iteration:287 \n",
      "Loss:0.175770103931427\n",
      "Epoch:3 \n",
      "Iteration:288 \n",
      "Loss:0.17256784439086914\n",
      "Epoch:3 \n",
      "Iteration:289 \n",
      "Loss:0.027325186878442764\n",
      "Epoch:3 \n",
      "Iteration:290 \n",
      "Loss:0.017134616151452065\n",
      "Epoch:3 \n",
      "Iteration:291 \n",
      "Loss:0.10804054141044617\n",
      "Epoch:3 \n",
      "Iteration:292 \n",
      "Loss:0.03294110670685768\n",
      "Epoch:3 \n",
      "Iteration:293 \n",
      "Loss:0.036379944533109665\n",
      "Epoch:3 \n",
      "Iteration:294 \n",
      "Loss:0.04411547631025314\n",
      "Epoch:3 \n",
      "Iteration:295 \n",
      "Loss:0.07565080374479294\n",
      "Epoch:3 \n",
      "Iteration:296 \n",
      "Loss:0.0573832169175148\n",
      "Epoch:3 \n",
      "Iteration:297 \n",
      "Loss:0.08438429981470108\n",
      "Epoch:3 \n",
      "Iteration:298 \n",
      "Loss:0.13010452687740326\n",
      "Epoch:3 \n",
      "Iteration:299 \n",
      "Loss:0.03196795657277107\n",
      "Epoch:3 \n",
      "Iteration:300 \n",
      "Loss:0.01535466592758894\n",
      "Epoch:3 \n",
      "Iteration:301 \n",
      "Loss:0.06136434152722359\n",
      "Epoch:3 \n",
      "Iteration:302 \n",
      "Loss:0.023291394114494324\n",
      "Epoch:3 \n",
      "Iteration:303 \n",
      "Loss:0.0185555312782526\n",
      "Epoch:3 \n",
      "Iteration:304 \n",
      "Loss:0.11194606125354767\n",
      "Epoch:3 \n",
      "Iteration:305 \n",
      "Loss:0.04936112463474274\n",
      "Epoch:3 \n",
      "Iteration:306 \n",
      "Loss:0.05691419914364815\n",
      "Epoch:3 \n",
      "Iteration:307 \n",
      "Loss:0.06733754277229309\n",
      "Epoch:3 \n",
      "Iteration:308 \n",
      "Loss:0.0600847564637661\n",
      "Epoch:3 \n",
      "Iteration:309 \n",
      "Loss:0.128073051571846\n",
      "Epoch:3 \n",
      "Iteration:310 \n",
      "Loss:0.08597646653652191\n",
      "Epoch:3 \n",
      "Iteration:311 \n",
      "Loss:0.09011654555797577\n",
      "Epoch:3 \n",
      "Iteration:312 \n",
      "Loss:0.03729492798447609\n",
      "Epoch:3 \n",
      "Iteration:313 \n",
      "Loss:0.1447417289018631\n",
      "Epoch:3 \n",
      "Iteration:314 \n",
      "Loss:0.026292163878679276\n",
      "Epoch:3 \n",
      "Iteration:315 \n",
      "Loss:0.04945821315050125\n",
      "Epoch:3 \n",
      "Iteration:316 \n",
      "Loss:0.023042850196361542\n",
      "Epoch:3 \n",
      "Iteration:317 \n",
      "Loss:0.043212905526161194\n",
      "Epoch:3 \n",
      "Iteration:318 \n",
      "Loss:0.042006924748420715\n",
      "Epoch:3 \n",
      "Iteration:319 \n",
      "Loss:0.047286033630371094\n",
      "Epoch:3 \n",
      "Iteration:320 \n",
      "Loss:0.05363646149635315\n",
      "Epoch:3 \n",
      "Iteration:321 \n",
      "Loss:0.05747861787676811\n",
      "Epoch:3 \n",
      "Iteration:322 \n",
      "Loss:0.07507288455963135\n",
      "Epoch:3 \n",
      "Iteration:323 \n",
      "Loss:0.049577079713344574\n",
      "Epoch:3 \n",
      "Iteration:324 \n",
      "Loss:0.052529629319906235\n",
      "Epoch:3 \n",
      "Iteration:325 \n",
      "Loss:0.0602988675236702\n",
      "Epoch:3 \n",
      "Iteration:326 \n",
      "Loss:0.08123020082712173\n",
      "Epoch:3 \n",
      "Iteration:327 \n",
      "Loss:0.01547863706946373\n",
      "Epoch:3 \n",
      "Iteration:328 \n",
      "Loss:0.07707434892654419\n",
      "Epoch:3 \n",
      "Iteration:329 \n",
      "Loss:0.04051278531551361\n",
      "Epoch:3 \n",
      "Iteration:330 \n",
      "Loss:0.009196980856359005\n",
      "Epoch:3 \n",
      "Iteration:331 \n",
      "Loss:0.0819530040025711\n",
      "Epoch:3 \n",
      "Iteration:332 \n",
      "Loss:0.20804451406002045\n",
      "Epoch:3 \n",
      "Iteration:333 \n",
      "Loss:0.015630604699254036\n",
      "Epoch:3 \n",
      "Iteration:334 \n",
      "Loss:0.047279275953769684\n",
      "Epoch:3 \n",
      "Iteration:335 \n",
      "Loss:0.015820754691958427\n",
      "Epoch:3 \n",
      "Iteration:336 \n",
      "Loss:0.030621837824583054\n",
      "Epoch:3 \n",
      "Iteration:337 \n",
      "Loss:0.04196449741721153\n",
      "Epoch:3 \n",
      "Iteration:338 \n",
      "Loss:0.26463621854782104\n",
      "Epoch:3 \n",
      "Iteration:339 \n",
      "Loss:0.027494564652442932\n",
      "Epoch:3 \n",
      "Iteration:340 \n",
      "Loss:0.026664409786462784\n",
      "Epoch:3 \n",
      "Iteration:341 \n",
      "Loss:0.021668260917067528\n",
      "Epoch:3 \n",
      "Iteration:342 \n",
      "Loss:0.036023177206516266\n",
      "Epoch:3 \n",
      "Iteration:343 \n",
      "Loss:0.08685459196567535\n",
      "Epoch:3 \n",
      "Iteration:344 \n",
      "Loss:0.03214238956570625\n",
      "Epoch:3 \n",
      "Iteration:345 \n",
      "Loss:0.15978045761585236\n",
      "Epoch:3 \n",
      "Iteration:346 \n",
      "Loss:0.1079966351389885\n",
      "Epoch:3 \n",
      "Iteration:347 \n",
      "Loss:0.016815856099128723\n",
      "Epoch:3 \n",
      "Iteration:348 \n",
      "Loss:0.02298133075237274\n",
      "Epoch:3 \n",
      "Iteration:349 \n",
      "Loss:0.04000164568424225\n",
      "Epoch:3 \n",
      "Iteration:350 \n",
      "Loss:0.028447626158595085\n",
      "Epoch:3 \n",
      "Iteration:351 \n",
      "Loss:0.021516937762498856\n",
      "Epoch:3 \n",
      "Iteration:352 \n",
      "Loss:0.07417622953653336\n",
      "Epoch:3 \n",
      "Iteration:353 \n",
      "Loss:0.06155497953295708\n",
      "Epoch:3 \n",
      "Iteration:354 \n",
      "Loss:0.023866722360253334\n",
      "Epoch:3 \n",
      "Iteration:355 \n",
      "Loss:0.1153397485613823\n",
      "Epoch:3 \n",
      "Iteration:356 \n",
      "Loss:0.06120302155613899\n",
      "Epoch:3 \n",
      "Iteration:357 \n",
      "Loss:0.02457522414624691\n",
      "Epoch:3 \n",
      "Iteration:358 \n",
      "Loss:0.05458023026585579\n",
      "Epoch:3 \n",
      "Iteration:359 \n",
      "Loss:0.08095678687095642\n",
      "Epoch:3 \n",
      "Iteration:360 \n",
      "Loss:0.045584581792354584\n",
      "Epoch:3 \n",
      "Iteration:361 \n",
      "Loss:0.026411592960357666\n",
      "Epoch:3 \n",
      "Iteration:362 \n",
      "Loss:0.026539266109466553\n",
      "Epoch:3 \n",
      "Iteration:363 \n",
      "Loss:0.050297483801841736\n",
      "Epoch:3 \n",
      "Iteration:364 \n",
      "Loss:0.016372786834836006\n",
      "Epoch:3 \n",
      "Iteration:365 \n",
      "Loss:0.027314424514770508\n",
      "Epoch:3 \n",
      "Iteration:366 \n",
      "Loss:0.037081822752952576\n",
      "Epoch:3 \n",
      "Iteration:367 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.15916821360588074\n",
      "Epoch:3 \n",
      "Iteration:368 \n",
      "Loss:0.060564830899238586\n",
      "Epoch:3 \n",
      "Iteration:369 \n",
      "Loss:0.1259833574295044\n",
      "Epoch:3 \n",
      "Iteration:370 \n",
      "Loss:0.10481490939855576\n",
      "Epoch:3 \n",
      "Iteration:371 \n",
      "Loss:0.058637332171201706\n",
      "Epoch:3 \n",
      "Iteration:372 \n",
      "Loss:0.07531488686800003\n",
      "Epoch:3 \n",
      "Iteration:373 \n",
      "Loss:0.19107292592525482\n",
      "Epoch:3 \n",
      "Iteration:374 \n",
      "Loss:0.1496812105178833\n",
      "Epoch:3 \n",
      "Iteration:375 \n",
      "Loss:0.213098406791687\n",
      "Epoch:3 \n",
      "Iteration:376 \n",
      "Loss:0.06078518554568291\n",
      "Epoch:3 \n",
      "Iteration:377 \n",
      "Loss:0.04792303591966629\n",
      "Epoch:3 \n",
      "Iteration:378 \n",
      "Loss:0.12813062965869904\n",
      "Epoch:3 \n",
      "Iteration:379 \n",
      "Loss:0.08556623756885529\n",
      "Epoch:3 \n",
      "Iteration:380 \n",
      "Loss:0.024678921326994896\n",
      "Epoch:3 \n",
      "Iteration:381 \n",
      "Loss:0.0942143127322197\n",
      "Epoch:3 \n",
      "Iteration:382 \n",
      "Loss:0.04351355507969856\n",
      "Epoch:3 \n",
      "Iteration:383 \n",
      "Loss:0.04660025238990784\n",
      "Epoch:3 \n",
      "Iteration:384 \n",
      "Loss:0.06373007595539093\n",
      "Epoch:3 \n",
      "Iteration:385 \n",
      "Loss:0.05564454570412636\n",
      "Epoch:3 \n",
      "Iteration:386 \n",
      "Loss:0.032816123217344284\n",
      "Epoch:3 \n",
      "Iteration:387 \n",
      "Loss:0.08133237808942795\n",
      "Epoch:3 \n",
      "Iteration:388 \n",
      "Loss:0.07473548501729965\n",
      "Epoch:3 \n",
      "Iteration:389 \n",
      "Loss:0.08674032241106033\n",
      "Epoch:3 \n",
      "Iteration:390 \n",
      "Loss:0.03935132920742035\n",
      "Epoch:3 \n",
      "Iteration:391 \n",
      "Loss:0.12914147973060608\n",
      "Epoch:3 \n",
      "Iteration:392 \n",
      "Loss:0.0740227997303009\n",
      "Epoch:3 \n",
      "Iteration:393 \n",
      "Loss:0.07169558852910995\n",
      "Epoch:3 \n",
      "Iteration:394 \n",
      "Loss:0.07901866734027863\n",
      "Epoch:3 \n",
      "Iteration:395 \n",
      "Loss:0.012888790108263493\n",
      "Epoch:3 \n",
      "Iteration:396 \n",
      "Loss:0.023101985454559326\n",
      "Epoch:3 \n",
      "Iteration:397 \n",
      "Loss:0.027346855029463768\n",
      "Epoch:3 \n",
      "Iteration:398 \n",
      "Loss:0.050713587552309036\n",
      "Epoch:3 \n",
      "Iteration:399 \n",
      "Loss:0.13524292409420013\n",
      "Epoch:3 \n",
      "Iteration:400 \n",
      "Loss:0.009454277344048023\n",
      "Epoch:3 \n",
      "Iteration:401 \n",
      "Loss:0.0868653953075409\n",
      "Epoch:3 \n",
      "Iteration:402 \n",
      "Loss:0.09657636284828186\n",
      "Epoch:3 \n",
      "Iteration:403 \n",
      "Loss:0.06118474155664444\n",
      "Epoch:3 \n",
      "Iteration:404 \n",
      "Loss:0.007587381638586521\n",
      "Epoch:3 \n",
      "Iteration:405 \n",
      "Loss:0.07533643394708633\n",
      "Epoch:3 \n",
      "Iteration:406 \n",
      "Loss:0.0638110414147377\n",
      "Epoch:3 \n",
      "Iteration:407 \n",
      "Loss:0.09557557851076126\n",
      "Epoch:3 \n",
      "Iteration:408 \n",
      "Loss:0.18209640681743622\n",
      "Epoch:3 \n",
      "Iteration:409 \n",
      "Loss:0.03930702060461044\n",
      "Epoch:3 \n",
      "Iteration:410 \n",
      "Loss:0.05902566760778427\n",
      "Epoch:3 \n",
      "Iteration:411 \n",
      "Loss:0.030768513679504395\n",
      "Epoch:3 \n",
      "Iteration:412 \n",
      "Loss:0.03843061625957489\n",
      "Epoch:3 \n",
      "Iteration:413 \n",
      "Loss:0.028910653665661812\n",
      "Epoch:3 \n",
      "Iteration:414 \n",
      "Loss:0.05996627360582352\n",
      "Epoch:3 \n",
      "Iteration:415 \n",
      "Loss:0.02921878732740879\n",
      "Epoch:3 \n",
      "Iteration:416 \n",
      "Loss:0.03671879693865776\n",
      "Epoch:3 \n",
      "Iteration:417 \n",
      "Loss:0.033204495906829834\n",
      "Epoch:3 \n",
      "Iteration:418 \n",
      "Loss:0.07315924763679504\n",
      "Epoch:3 \n",
      "Iteration:419 \n",
      "Loss:0.12551261484622955\n",
      "Epoch:3 \n",
      "Iteration:420 \n",
      "Loss:0.029948584735393524\n",
      "Epoch:3 \n",
      "Iteration:421 \n",
      "Loss:0.1145990863442421\n",
      "Epoch:3 \n",
      "Iteration:422 \n",
      "Loss:0.12129977345466614\n",
      "Epoch:3 \n",
      "Iteration:423 \n",
      "Loss:0.11679253727197647\n",
      "Epoch:3 \n",
      "Iteration:424 \n",
      "Loss:0.06504392623901367\n",
      "Epoch:3 \n",
      "Iteration:425 \n",
      "Loss:0.11825316399335861\n",
      "Epoch:3 \n",
      "Iteration:426 \n",
      "Loss:0.021926239132881165\n",
      "Epoch:3 \n",
      "Iteration:427 \n",
      "Loss:0.03096041828393936\n",
      "Epoch:3 \n",
      "Iteration:428 \n",
      "Loss:0.0646766647696495\n",
      "Epoch:3 \n",
      "Iteration:429 \n",
      "Loss:0.08733537048101425\n",
      "Epoch:3 \n",
      "Iteration:430 \n",
      "Loss:0.014815184287726879\n",
      "Epoch:3 \n",
      "Iteration:431 \n",
      "Loss:0.05173107236623764\n",
      "Epoch:3 \n",
      "Iteration:432 \n",
      "Loss:0.00603444641456008\n",
      "Epoch:3 \n",
      "Iteration:433 \n",
      "Loss:0.016772666946053505\n",
      "Epoch:3 \n",
      "Iteration:434 \n",
      "Loss:0.03908924758434296\n",
      "Epoch:3 \n",
      "Iteration:435 \n",
      "Loss:0.03855489566922188\n",
      "Epoch:3 \n",
      "Iteration:436 \n",
      "Loss:0.03819161280989647\n",
      "Epoch:3 \n",
      "Iteration:437 \n",
      "Loss:0.032628774642944336\n",
      "Epoch:3 \n",
      "Iteration:438 \n",
      "Loss:0.036820754408836365\n",
      "Epoch:3 \n",
      "Iteration:439 \n",
      "Loss:0.04800525680184364\n",
      "Epoch:3 \n",
      "Iteration:440 \n",
      "Loss:0.033686865121126175\n",
      "Epoch:3 \n",
      "Iteration:441 \n",
      "Loss:0.0831451416015625\n",
      "Epoch:3 \n",
      "Iteration:442 \n",
      "Loss:0.059462033212184906\n",
      "Epoch:3 \n",
      "Iteration:443 \n",
      "Loss:0.14983554184436798\n",
      "Epoch:3 \n",
      "Iteration:444 \n",
      "Loss:0.012346874922513962\n",
      "Epoch:3 \n",
      "Iteration:445 \n",
      "Loss:0.014506292529404163\n",
      "Epoch:3 \n",
      "Iteration:446 \n",
      "Loss:0.12367135286331177\n",
      "Epoch:3 \n",
      "Iteration:447 \n",
      "Loss:0.017795918509364128\n",
      "Epoch:3 \n",
      "Iteration:448 \n",
      "Loss:0.055968575179576874\n",
      "Epoch:3 \n",
      "Iteration:449 \n",
      "Loss:0.04598839208483696\n",
      "Epoch:3 \n",
      "Iteration:450 \n",
      "Loss:0.015113008208572865\n",
      "Epoch:3 \n",
      "Iteration:451 \n",
      "Loss:0.014321736991405487\n",
      "Epoch:3 \n",
      "Iteration:452 \n",
      "Loss:0.07545002549886703\n",
      "Epoch:3 \n",
      "Iteration:453 \n",
      "Loss:0.019356178119778633\n",
      "Epoch:3 \n",
      "Iteration:454 \n",
      "Loss:0.11621472239494324\n",
      "Epoch:3 \n",
      "Iteration:455 \n",
      "Loss:0.19944007694721222\n",
      "Epoch:3 \n",
      "Iteration:456 \n",
      "Loss:0.04061676561832428\n",
      "Epoch:3 \n",
      "Iteration:457 \n",
      "Loss:0.044181112200021744\n",
      "Epoch:3 \n",
      "Iteration:458 \n",
      "Loss:0.05118435248732567\n",
      "Epoch:3 \n",
      "Iteration:459 \n",
      "Loss:0.12524893879890442\n",
      "Epoch:3 \n",
      "Iteration:460 \n",
      "Loss:0.09160235524177551\n",
      "Epoch:3 \n",
      "Iteration:461 \n",
      "Loss:0.08854662626981735\n",
      "Epoch:3 \n",
      "Iteration:462 \n",
      "Loss:0.018272338435053825\n",
      "Epoch:3 \n",
      "Iteration:463 \n",
      "Loss:0.079646036028862\n",
      "Epoch:3 \n",
      "Iteration:464 \n",
      "Loss:0.017677420750260353\n",
      "Epoch:3 \n",
      "Iteration:465 \n",
      "Loss:0.10727082937955856\n",
      "Epoch:3 \n",
      "Iteration:466 \n",
      "Loss:0.15684157609939575\n",
      "Epoch:3 \n",
      "Iteration:467 \n",
      "Loss:0.07077421993017197\n",
      "Epoch:3 \n",
      "Iteration:468 \n",
      "Loss:0.02392643690109253\n",
      "Epoch:3 \n",
      "Iteration:469 \n",
      "Loss:0.028030812740325928\n",
      "Epoch:3 \n",
      "Iteration:470 \n",
      "Loss:0.023524584248661995\n",
      "Epoch:3 \n",
      "Iteration:471 \n",
      "Loss:0.05504795163869858\n",
      "Epoch:3 \n",
      "Iteration:472 \n",
      "Loss:0.041099730879068375\n",
      "Epoch:3 \n",
      "Iteration:473 \n",
      "Loss:0.05134458839893341\n",
      "Epoch:3 \n",
      "Iteration:474 \n",
      "Loss:0.11096357554197311\n",
      "Epoch:3 \n",
      "Iteration:475 \n",
      "Loss:0.04499058425426483\n",
      "Epoch:3 \n",
      "Iteration:476 \n",
      "Loss:0.036424990743398666\n",
      "Epoch:3 \n",
      "Iteration:477 \n",
      "Loss:0.04963929206132889\n",
      "Epoch:3 \n",
      "Iteration:478 \n",
      "Loss:0.07822341471910477\n",
      "Epoch:3 \n",
      "Iteration:479 \n",
      "Loss:0.038502320647239685\n",
      "Epoch:3 \n",
      "Iteration:480 \n",
      "Loss:0.05495280399918556\n",
      "Epoch:3 \n",
      "Iteration:481 \n",
      "Loss:0.05292540416121483\n",
      "Epoch:3 \n",
      "Iteration:482 \n",
      "Loss:0.08286896347999573\n",
      "Epoch:3 \n",
      "Iteration:483 \n",
      "Loss:0.06266821175813675\n",
      "Epoch:3 \n",
      "Iteration:484 \n",
      "Loss:0.1510794460773468\n",
      "Epoch:3 \n",
      "Iteration:485 \n",
      "Loss:0.02668604627251625\n",
      "Epoch:3 \n",
      "Iteration:486 \n",
      "Loss:0.01735815592110157\n",
      "Epoch:3 \n",
      "Iteration:487 \n",
      "Loss:0.012418835423886776\n",
      "Epoch:3 \n",
      "Iteration:488 \n",
      "Loss:0.026516320183873177\n",
      "Epoch:3 \n",
      "Iteration:489 \n",
      "Loss:0.04831799492239952\n",
      "Epoch:3 \n",
      "Iteration:490 \n",
      "Loss:0.024625005200505257\n",
      "Epoch:3 \n",
      "Iteration:491 \n",
      "Loss:0.031595949083566666\n",
      "Epoch:3 \n",
      "Iteration:492 \n",
      "Loss:0.062487564980983734\n",
      "Epoch:3 \n",
      "Iteration:493 \n",
      "Loss:0.0430024228990078\n",
      "Epoch:3 \n",
      "Iteration:494 \n",
      "Loss:0.09127211570739746\n",
      "Epoch:3 \n",
      "Iteration:495 \n",
      "Loss:0.09712136536836624\n",
      "Epoch:3 \n",
      "Iteration:496 \n",
      "Loss:0.07134747505187988\n",
      "Epoch:3 \n",
      "Iteration:497 \n",
      "Loss:0.121104896068573\n",
      "Epoch:3 \n",
      "Iteration:498 \n",
      "Loss:0.147923082113266\n",
      "Epoch:3 \n",
      "Iteration:499 \n",
      "Loss:0.023370489478111267\n",
      "Epoch:3 \n",
      "Iteration:500 \n",
      "Loss:0.1316874474287033\n",
      "Epoch:3 \n",
      "Iteration:501 \n",
      "Loss:0.031174367293715477\n",
      "Epoch:3 \n",
      "Iteration:502 \n",
      "Loss:0.054284367710351944\n",
      "Epoch:3 \n",
      "Iteration:503 \n",
      "Loss:0.09452662616968155\n",
      "Epoch:3 \n",
      "Iteration:504 \n",
      "Loss:0.006037610117346048\n",
      "Epoch:3 \n",
      "Iteration:505 \n",
      "Loss:0.07172221690416336\n",
      "Epoch:3 \n",
      "Iteration:506 \n",
      "Loss:0.1037885844707489\n",
      "Epoch:3 \n",
      "Iteration:507 \n",
      "Loss:0.03325257450342178\n",
      "Epoch:3 \n",
      "Iteration:508 \n",
      "Loss:0.03974100947380066\n",
      "Epoch:3 \n",
      "Iteration:509 \n",
      "Loss:0.055190183222293854\n",
      "Epoch:3 \n",
      "Iteration:510 \n",
      "Loss:0.12394395470619202\n",
      "Epoch:3 \n",
      "Iteration:511 \n",
      "Loss:0.017157621681690216\n",
      "Epoch:3 \n",
      "Iteration:512 \n",
      "Loss:0.08471744507551193\n",
      "Epoch:3 \n",
      "Iteration:513 \n",
      "Loss:0.19270697236061096\n",
      "Epoch:3 \n",
      "Iteration:514 \n",
      "Loss:0.00915892980992794\n",
      "Epoch:3 \n",
      "Iteration:515 \n",
      "Loss:0.11456885933876038\n",
      "Epoch:3 \n",
      "Iteration:516 \n",
      "Loss:0.05020492896437645\n",
      "Epoch:3 \n",
      "Iteration:517 \n",
      "Loss:0.12951892614364624\n",
      "Epoch:3 \n",
      "Iteration:518 \n",
      "Loss:0.09133592247962952\n",
      "Epoch:3 \n",
      "Iteration:519 \n",
      "Loss:0.06451079249382019\n",
      "Epoch:3 \n",
      "Iteration:520 \n",
      "Loss:0.10322973877191544\n",
      "Epoch:3 \n",
      "Iteration:521 \n",
      "Loss:0.09876592457294464\n",
      "Epoch:3 \n",
      "Iteration:522 \n",
      "Loss:0.071944959461689\n",
      "Epoch:3 \n",
      "Iteration:523 \n",
      "Loss:0.06504212319850922\n",
      "Epoch:3 \n",
      "Iteration:524 \n",
      "Loss:0.08795938640832901\n",
      "Epoch:3 \n",
      "Iteration:525 \n",
      "Loss:0.03814290463924408\n",
      "Epoch:3 \n",
      "Iteration:526 \n",
      "Loss:0.01877966709434986\n",
      "Epoch:3 \n",
      "Iteration:527 \n",
      "Loss:0.05336418002843857\n",
      "Epoch:3 \n",
      "Iteration:528 \n",
      "Loss:0.040779076516628265\n",
      "Epoch:3 \n",
      "Iteration:529 \n",
      "Loss:0.029231682419776917\n",
      "Epoch:3 \n",
      "Iteration:530 \n",
      "Loss:0.022919287905097008\n",
      "Epoch:3 \n",
      "Iteration:531 \n",
      "Loss:0.023215139284729958\n",
      "Epoch:3 \n",
      "Iteration:532 \n",
      "Loss:0.02789212204515934\n",
      "Epoch:3 \n",
      "Iteration:533 \n",
      "Loss:0.08830823749303818\n",
      "Epoch:3 \n",
      "Iteration:534 \n",
      "Loss:0.06086815893650055\n",
      "Epoch:3 \n",
      "Iteration:535 \n",
      "Loss:0.06807231158018112\n",
      "Epoch:3 \n",
      "Iteration:536 \n",
      "Loss:0.045101579278707504\n",
      "Epoch:3 \n",
      "Iteration:537 \n",
      "Loss:0.01850729063153267\n",
      "Epoch:3 \n",
      "Iteration:538 \n",
      "Loss:0.015373787842690945\n",
      "Epoch:3 \n",
      "Iteration:539 \n",
      "Loss:0.07212677597999573\n",
      "Epoch:3 \n",
      "Iteration:540 \n",
      "Loss:0.11486479640007019\n",
      "Epoch:3 \n",
      "Iteration:541 \n",
      "Loss:0.006568409036844969\n",
      "Epoch:3 \n",
      "Iteration:542 \n",
      "Loss:0.04779447987675667\n",
      "Epoch:3 \n",
      "Iteration:543 \n",
      "Loss:0.002478356007486582\n",
      "Epoch:3 \n",
      "Iteration:544 \n",
      "Loss:0.02181730978190899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3 \n",
      "Iteration:545 \n",
      "Loss:0.16042587161064148\n",
      "Epoch:3 \n",
      "Iteration:546 \n",
      "Loss:0.09131696075201035\n",
      "Epoch:3 \n",
      "Iteration:547 \n",
      "Loss:0.04490528255701065\n",
      "Epoch:3 \n",
      "Iteration:548 \n",
      "Loss:0.05089196562767029\n",
      "Epoch:3 \n",
      "Iteration:549 \n",
      "Loss:0.049557413905858994\n",
      "Epoch:3 \n",
      "Iteration:550 \n",
      "Loss:0.06745120882987976\n",
      "Epoch:3 \n",
      "Iteration:551 \n",
      "Loss:0.07445251196622849\n",
      "Epoch:3 \n",
      "Iteration:552 \n",
      "Loss:0.022643830627202988\n",
      "Epoch:3 \n",
      "Iteration:553 \n",
      "Loss:0.0636422410607338\n",
      "Epoch:3 \n",
      "Iteration:554 \n",
      "Loss:0.034051116555929184\n",
      "Epoch:3 \n",
      "Iteration:555 \n",
      "Loss:0.24185127019882202\n",
      "Epoch:3 \n",
      "Iteration:556 \n",
      "Loss:0.04561919346451759\n",
      "Epoch:3 \n",
      "Iteration:557 \n",
      "Loss:0.07687684148550034\n",
      "Epoch:3 \n",
      "Iteration:558 \n",
      "Loss:0.12885460257530212\n",
      "Epoch:3 \n",
      "Iteration:559 \n",
      "Loss:0.047977544367313385\n",
      "Epoch:3 \n",
      "Iteration:560 \n",
      "Loss:0.041476693004369736\n",
      "Epoch:3 \n",
      "Iteration:561 \n",
      "Loss:0.0528879277408123\n",
      "Epoch:3 \n",
      "Iteration:562 \n",
      "Loss:0.056906808167696\n",
      "Epoch:3 \n",
      "Iteration:563 \n",
      "Loss:0.04545257240533829\n",
      "Epoch:3 \n",
      "Iteration:564 \n",
      "Loss:0.14434482157230377\n",
      "Epoch:3 \n",
      "Iteration:565 \n",
      "Loss:0.09007858484983444\n",
      "Epoch:3 \n",
      "Iteration:566 \n",
      "Loss:0.059502530843019485\n",
      "Epoch:3 \n",
      "Iteration:567 \n",
      "Loss:0.04964425042271614\n",
      "Epoch:3 \n",
      "Iteration:568 \n",
      "Loss:0.1237768679857254\n",
      "Epoch:3 \n",
      "Iteration:569 \n",
      "Loss:0.08240116387605667\n",
      "Epoch:3 \n",
      "Iteration:570 \n",
      "Loss:0.06299039721488953\n",
      "Epoch:3 \n",
      "Iteration:571 \n",
      "Loss:0.07882066816091537\n",
      "Epoch:3 \n",
      "Iteration:572 \n",
      "Loss:0.09825509786605835\n",
      "Epoch:3 \n",
      "Iteration:573 \n",
      "Loss:0.08380954712629318\n",
      "Epoch:3 \n",
      "Iteration:574 \n",
      "Loss:0.10015127062797546\n",
      "Epoch:3 \n",
      "Iteration:575 \n",
      "Loss:0.051494285464286804\n",
      "Epoch:3 \n",
      "Iteration:576 \n",
      "Loss:0.10279891639947891\n",
      "Epoch:3 \n",
      "Iteration:577 \n",
      "Loss:0.09098298847675323\n",
      "Epoch:3 \n",
      "Iteration:578 \n",
      "Loss:0.0633343905210495\n",
      "Epoch:3 \n",
      "Iteration:579 \n",
      "Loss:0.08485736697912216\n",
      "Epoch:3 \n",
      "Iteration:580 \n",
      "Loss:0.08493006974458694\n",
      "Epoch:3 \n",
      "Iteration:581 \n",
      "Loss:0.03729868680238724\n",
      "Epoch:3 \n",
      "Iteration:582 \n",
      "Loss:0.1048179417848587\n",
      "Epoch:3 \n",
      "Iteration:583 \n",
      "Loss:0.10830719769001007\n",
      "Epoch:3 \n",
      "Iteration:584 \n",
      "Loss:0.01557110995054245\n",
      "Epoch:3 \n",
      "Iteration:585 \n",
      "Loss:0.018897496163845062\n",
      "Epoch:3 \n",
      "Iteration:586 \n",
      "Loss:0.10859076678752899\n",
      "Epoch:3 \n",
      "Iteration:587 \n",
      "Loss:0.061787303537130356\n",
      "Epoch:3 \n",
      "Iteration:588 \n",
      "Loss:0.0567367747426033\n",
      "Epoch:3 \n",
      "Iteration:589 \n",
      "Loss:0.02698596566915512\n",
      "Epoch:3 \n",
      "Iteration:590 \n",
      "Loss:0.05311235040426254\n",
      "Epoch:3 \n",
      "Iteration:591 \n",
      "Loss:0.05536394193768501\n",
      "Epoch:3 \n",
      "Iteration:592 \n",
      "Loss:0.09901095181703568\n",
      "Epoch:3 \n",
      "Iteration:593 \n",
      "Loss:0.045028939843177795\n",
      "Epoch:3 \n",
      "Iteration:594 \n",
      "Loss:0.15335804224014282\n",
      "Epoch:3 \n",
      "Iteration:595 \n",
      "Loss:0.04694725573062897\n",
      "Epoch:3 \n",
      "Iteration:596 \n",
      "Loss:0.04241202771663666\n",
      "Epoch:3 \n",
      "Iteration:597 \n",
      "Loss:0.078656405210495\n",
      "Epoch:3 \n",
      "Iteration:598 \n",
      "Loss:0.0703522264957428\n",
      "Epoch:3 \n",
      "Iteration:599 \n",
      "Loss:0.05391570180654526\n",
      "Epoch:3 \n",
      "Iteration:600 \n",
      "Loss:0.08491485565900803\n",
      "\n",
      "Accuracy of network in epoch 3: 97.97833333333334\n",
      "Epoch:4 \n",
      "Iteration:1 \n",
      "Loss:0.01789376325905323\n",
      "Epoch:4 \n",
      "Iteration:2 \n",
      "Loss:0.07889601588249207\n",
      "Epoch:4 \n",
      "Iteration:3 \n",
      "Loss:0.04703843966126442\n",
      "Epoch:4 \n",
      "Iteration:4 \n",
      "Loss:0.04022621735930443\n",
      "Epoch:4 \n",
      "Iteration:5 \n",
      "Loss:0.008627141825854778\n",
      "Epoch:4 \n",
      "Iteration:6 \n",
      "Loss:0.01156854722648859\n",
      "Epoch:4 \n",
      "Iteration:7 \n",
      "Loss:0.01550103910267353\n",
      "Epoch:4 \n",
      "Iteration:8 \n",
      "Loss:0.07879588752985\n",
      "Epoch:4 \n",
      "Iteration:9 \n",
      "Loss:0.008173178881406784\n",
      "Epoch:4 \n",
      "Iteration:10 \n",
      "Loss:0.059393320232629776\n",
      "Epoch:4 \n",
      "Iteration:11 \n",
      "Loss:0.022475985810160637\n",
      "Epoch:4 \n",
      "Iteration:12 \n",
      "Loss:0.03332153707742691\n",
      "Epoch:4 \n",
      "Iteration:13 \n",
      "Loss:0.015555769205093384\n",
      "Epoch:4 \n",
      "Iteration:14 \n",
      "Loss:0.0357709601521492\n",
      "Epoch:4 \n",
      "Iteration:15 \n",
      "Loss:0.06433311104774475\n",
      "Epoch:4 \n",
      "Iteration:16 \n",
      "Loss:0.016791533678770065\n",
      "Epoch:4 \n",
      "Iteration:17 \n",
      "Loss:0.04974867403507233\n",
      "Epoch:4 \n",
      "Iteration:18 \n",
      "Loss:0.005947496276348829\n",
      "Epoch:4 \n",
      "Iteration:19 \n",
      "Loss:0.05682890862226486\n",
      "Epoch:4 \n",
      "Iteration:20 \n",
      "Loss:0.06448543816804886\n",
      "Epoch:4 \n",
      "Iteration:21 \n",
      "Loss:0.04339738190174103\n",
      "Epoch:4 \n",
      "Iteration:22 \n",
      "Loss:0.07480448484420776\n",
      "Epoch:4 \n",
      "Iteration:23 \n",
      "Loss:0.023384351283311844\n",
      "Epoch:4 \n",
      "Iteration:24 \n",
      "Loss:0.10775664448738098\n",
      "Epoch:4 \n",
      "Iteration:25 \n",
      "Loss:0.02458544261753559\n",
      "Epoch:4 \n",
      "Iteration:26 \n",
      "Loss:0.07701321691274643\n",
      "Epoch:4 \n",
      "Iteration:27 \n",
      "Loss:0.12433914095163345\n",
      "Epoch:4 \n",
      "Iteration:28 \n",
      "Loss:0.031119676306843758\n",
      "Epoch:4 \n",
      "Iteration:29 \n",
      "Loss:0.016854573041200638\n",
      "Epoch:4 \n",
      "Iteration:30 \n",
      "Loss:0.003978261258453131\n",
      "Epoch:4 \n",
      "Iteration:31 \n",
      "Loss:0.0025878397282212973\n",
      "Epoch:4 \n",
      "Iteration:32 \n",
      "Loss:0.09982051700353622\n",
      "Epoch:4 \n",
      "Iteration:33 \n",
      "Loss:0.08204580098390579\n",
      "Epoch:4 \n",
      "Iteration:34 \n",
      "Loss:0.01873853988945484\n",
      "Epoch:4 \n",
      "Iteration:35 \n",
      "Loss:0.05223340913653374\n",
      "Epoch:4 \n",
      "Iteration:36 \n",
      "Loss:0.01321937795728445\n",
      "Epoch:4 \n",
      "Iteration:37 \n",
      "Loss:0.023532243445515633\n",
      "Epoch:4 \n",
      "Iteration:38 \n",
      "Loss:0.0490330308675766\n",
      "Epoch:4 \n",
      "Iteration:39 \n",
      "Loss:0.026632240042090416\n",
      "Epoch:4 \n",
      "Iteration:40 \n",
      "Loss:0.03256245702505112\n",
      "Epoch:4 \n",
      "Iteration:41 \n",
      "Loss:0.03067956492304802\n",
      "Epoch:4 \n",
      "Iteration:42 \n",
      "Loss:0.024496756494045258\n",
      "Epoch:4 \n",
      "Iteration:43 \n",
      "Loss:0.020979812368750572\n",
      "Epoch:4 \n",
      "Iteration:44 \n",
      "Loss:0.04607443884015083\n",
      "Epoch:4 \n",
      "Iteration:45 \n",
      "Loss:0.00590901542454958\n",
      "Epoch:4 \n",
      "Iteration:46 \n",
      "Loss:0.03034631535410881\n",
      "Epoch:4 \n",
      "Iteration:47 \n",
      "Loss:0.08082685619592667\n",
      "Epoch:4 \n",
      "Iteration:48 \n",
      "Loss:0.06856287270784378\n",
      "Epoch:4 \n",
      "Iteration:49 \n",
      "Loss:0.04518041014671326\n",
      "Epoch:4 \n",
      "Iteration:50 \n",
      "Loss:0.016538048163056374\n",
      "Epoch:4 \n",
      "Iteration:51 \n",
      "Loss:0.007430723402649164\n",
      "Epoch:4 \n",
      "Iteration:52 \n",
      "Loss:0.017678290605545044\n",
      "Epoch:4 \n",
      "Iteration:53 \n",
      "Loss:0.0033998119179159403\n",
      "Epoch:4 \n",
      "Iteration:54 \n",
      "Loss:0.014363058842718601\n",
      "Epoch:4 \n",
      "Iteration:55 \n",
      "Loss:0.04996103793382645\n",
      "Epoch:4 \n",
      "Iteration:56 \n",
      "Loss:0.016824737191200256\n",
      "Epoch:4 \n",
      "Iteration:57 \n",
      "Loss:0.018151648342609406\n",
      "Epoch:4 \n",
      "Iteration:58 \n",
      "Loss:0.06474575400352478\n",
      "Epoch:4 \n",
      "Iteration:59 \n",
      "Loss:0.06597334146499634\n",
      "Epoch:4 \n",
      "Iteration:60 \n",
      "Loss:0.0074410829693078995\n",
      "Epoch:4 \n",
      "Iteration:61 \n",
      "Loss:0.029403066262602806\n",
      "Epoch:4 \n",
      "Iteration:62 \n",
      "Loss:0.014681065455079079\n",
      "Epoch:4 \n",
      "Iteration:63 \n",
      "Loss:0.021315770223736763\n",
      "Epoch:4 \n",
      "Iteration:64 \n",
      "Loss:0.00859622098505497\n",
      "Epoch:4 \n",
      "Iteration:65 \n",
      "Loss:0.00688653951510787\n",
      "Epoch:4 \n",
      "Iteration:66 \n",
      "Loss:0.04730942100286484\n",
      "Epoch:4 \n",
      "Iteration:67 \n",
      "Loss:0.02298618294298649\n",
      "Epoch:4 \n",
      "Iteration:68 \n",
      "Loss:0.06538284569978714\n",
      "Epoch:4 \n",
      "Iteration:69 \n",
      "Loss:0.011603139340877533\n",
      "Epoch:4 \n",
      "Iteration:70 \n",
      "Loss:0.047224294394254684\n",
      "Epoch:4 \n",
      "Iteration:71 \n",
      "Loss:0.008952029049396515\n",
      "Epoch:4 \n",
      "Iteration:72 \n",
      "Loss:0.02915778197348118\n",
      "Epoch:4 \n",
      "Iteration:73 \n",
      "Loss:0.01331786997616291\n",
      "Epoch:4 \n",
      "Iteration:74 \n",
      "Loss:0.006045385729521513\n",
      "Epoch:4 \n",
      "Iteration:75 \n",
      "Loss:0.013076921924948692\n",
      "Epoch:4 \n",
      "Iteration:76 \n",
      "Loss:0.02799094282090664\n",
      "Epoch:4 \n",
      "Iteration:77 \n",
      "Loss:0.03331561014056206\n",
      "Epoch:4 \n",
      "Iteration:78 \n",
      "Loss:0.005030190572142601\n",
      "Epoch:4 \n",
      "Iteration:79 \n",
      "Loss:0.021165695041418076\n",
      "Epoch:4 \n",
      "Iteration:80 \n",
      "Loss:0.016653193160891533\n",
      "Epoch:4 \n",
      "Iteration:81 \n",
      "Loss:0.029071908444166183\n",
      "Epoch:4 \n",
      "Iteration:82 \n",
      "Loss:0.002818274311721325\n",
      "Epoch:4 \n",
      "Iteration:83 \n",
      "Loss:0.09432516992092133\n",
      "Epoch:4 \n",
      "Iteration:84 \n",
      "Loss:0.03774137422442436\n",
      "Epoch:4 \n",
      "Iteration:85 \n",
      "Loss:0.07432451099157333\n",
      "Epoch:4 \n",
      "Iteration:86 \n",
      "Loss:0.06524107605218887\n",
      "Epoch:4 \n",
      "Iteration:87 \n",
      "Loss:0.007082641124725342\n",
      "Epoch:4 \n",
      "Iteration:88 \n",
      "Loss:0.050045523792505264\n",
      "Epoch:4 \n",
      "Iteration:89 \n",
      "Loss:0.042525481432676315\n",
      "Epoch:4 \n",
      "Iteration:90 \n",
      "Loss:0.09866044670343399\n",
      "Epoch:4 \n",
      "Iteration:91 \n",
      "Loss:0.08991432934999466\n",
      "Epoch:4 \n",
      "Iteration:92 \n",
      "Loss:0.08320696651935577\n",
      "Epoch:4 \n",
      "Iteration:93 \n",
      "Loss:0.010825909674167633\n",
      "Epoch:4 \n",
      "Iteration:94 \n",
      "Loss:0.05765775218605995\n",
      "Epoch:4 \n",
      "Iteration:95 \n",
      "Loss:0.08058343082666397\n",
      "Epoch:4 \n",
      "Iteration:96 \n",
      "Loss:0.006746064405888319\n",
      "Epoch:4 \n",
      "Iteration:97 \n",
      "Loss:0.022668354213237762\n",
      "Epoch:4 \n",
      "Iteration:98 \n",
      "Loss:0.003434357000514865\n",
      "Epoch:4 \n",
      "Iteration:99 \n",
      "Loss:0.044832728803157806\n",
      "Epoch:4 \n",
      "Iteration:100 \n",
      "Loss:0.009273026138544083\n",
      "Epoch:4 \n",
      "Iteration:101 \n",
      "Loss:0.011010735295712948\n",
      "Epoch:4 \n",
      "Iteration:102 \n",
      "Loss:0.030676711350679398\n",
      "Epoch:4 \n",
      "Iteration:103 \n",
      "Loss:0.013816446997225285\n",
      "Epoch:4 \n",
      "Iteration:104 \n",
      "Loss:0.08443519473075867\n",
      "Epoch:4 \n",
      "Iteration:105 \n",
      "Loss:0.027454061433672905\n",
      "Epoch:4 \n",
      "Iteration:106 \n",
      "Loss:0.03269563615322113\n",
      "Epoch:4 \n",
      "Iteration:107 \n",
      "Loss:0.038587652146816254\n",
      "Epoch:4 \n",
      "Iteration:108 \n",
      "Loss:0.036297816783189774\n",
      "Epoch:4 \n",
      "Iteration:109 \n",
      "Loss:0.031072886660695076\n",
      "Epoch:4 \n",
      "Iteration:110 \n",
      "Loss:0.03862094134092331\n",
      "Epoch:4 \n",
      "Iteration:111 \n",
      "Loss:0.1002841368317604\n",
      "Epoch:4 \n",
      "Iteration:112 \n",
      "Loss:0.030631277710199356\n",
      "Epoch:4 \n",
      "Iteration:113 \n",
      "Loss:0.07150768488645554\n",
      "Epoch:4 \n",
      "Iteration:114 \n",
      "Loss:0.030221426859498024\n",
      "Epoch:4 \n",
      "Iteration:115 \n",
      "Loss:0.011252054944634438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4 \n",
      "Iteration:116 \n",
      "Loss:0.07272277772426605\n",
      "Epoch:4 \n",
      "Iteration:117 \n",
      "Loss:0.03852635994553566\n",
      "Epoch:4 \n",
      "Iteration:118 \n",
      "Loss:0.04058416187763214\n",
      "Epoch:4 \n",
      "Iteration:119 \n",
      "Loss:0.007790723815560341\n",
      "Epoch:4 \n",
      "Iteration:120 \n",
      "Loss:0.015223665162920952\n",
      "Epoch:4 \n",
      "Iteration:121 \n",
      "Loss:0.1175655946135521\n",
      "Epoch:4 \n",
      "Iteration:122 \n",
      "Loss:0.10863318294286728\n",
      "Epoch:4 \n",
      "Iteration:123 \n",
      "Loss:0.04736940562725067\n",
      "Epoch:4 \n",
      "Iteration:124 \n",
      "Loss:0.024785034358501434\n",
      "Epoch:4 \n",
      "Iteration:125 \n",
      "Loss:0.02570423111319542\n",
      "Epoch:4 \n",
      "Iteration:126 \n",
      "Loss:0.012892677448689938\n",
      "Epoch:4 \n",
      "Iteration:127 \n",
      "Loss:0.007912449538707733\n",
      "Epoch:4 \n",
      "Iteration:128 \n",
      "Loss:0.041013747453689575\n",
      "Epoch:4 \n",
      "Iteration:129 \n",
      "Loss:0.018509484827518463\n",
      "Epoch:4 \n",
      "Iteration:130 \n",
      "Loss:0.019345315173268318\n",
      "Epoch:4 \n",
      "Iteration:131 \n",
      "Loss:0.10311723500490189\n",
      "Epoch:4 \n",
      "Iteration:132 \n",
      "Loss:0.05665489658713341\n",
      "Epoch:4 \n",
      "Iteration:133 \n",
      "Loss:0.10963281989097595\n",
      "Epoch:4 \n",
      "Iteration:134 \n",
      "Loss:0.043743062764406204\n",
      "Epoch:4 \n",
      "Iteration:135 \n",
      "Loss:0.057814355939626694\n",
      "Epoch:4 \n",
      "Iteration:136 \n",
      "Loss:0.020585181191563606\n",
      "Epoch:4 \n",
      "Iteration:137 \n",
      "Loss:0.11075325310230255\n",
      "Epoch:4 \n",
      "Iteration:138 \n",
      "Loss:0.02633899636566639\n",
      "Epoch:4 \n",
      "Iteration:139 \n",
      "Loss:0.01688660867512226\n",
      "Epoch:4 \n",
      "Iteration:140 \n",
      "Loss:0.03505046293139458\n",
      "Epoch:4 \n",
      "Iteration:141 \n",
      "Loss:0.06518782675266266\n",
      "Epoch:4 \n",
      "Iteration:142 \n",
      "Loss:0.02159339375793934\n",
      "Epoch:4 \n",
      "Iteration:143 \n",
      "Loss:0.03421924263238907\n",
      "Epoch:4 \n",
      "Iteration:144 \n",
      "Loss:0.020732436329126358\n",
      "Epoch:4 \n",
      "Iteration:145 \n",
      "Loss:0.04402105510234833\n",
      "Epoch:4 \n",
      "Iteration:146 \n",
      "Loss:0.06428989768028259\n",
      "Epoch:4 \n",
      "Iteration:147 \n",
      "Loss:0.03373410552740097\n",
      "Epoch:4 \n",
      "Iteration:148 \n",
      "Loss:0.013051302172243595\n",
      "Epoch:4 \n",
      "Iteration:149 \n",
      "Loss:0.057291179895401\n",
      "Epoch:4 \n",
      "Iteration:150 \n",
      "Loss:0.02513781376183033\n",
      "Epoch:4 \n",
      "Iteration:151 \n",
      "Loss:0.02907457761466503\n",
      "Epoch:4 \n",
      "Iteration:152 \n",
      "Loss:0.045794662088155746\n",
      "Epoch:4 \n",
      "Iteration:153 \n",
      "Loss:0.07223228365182877\n",
      "Epoch:4 \n",
      "Iteration:154 \n",
      "Loss:0.09580782055854797\n",
      "Epoch:4 \n",
      "Iteration:155 \n",
      "Loss:0.024940773844718933\n",
      "Epoch:4 \n",
      "Iteration:156 \n",
      "Loss:0.03560597822070122\n",
      "Epoch:4 \n",
      "Iteration:157 \n",
      "Loss:0.02950041927397251\n",
      "Epoch:4 \n",
      "Iteration:158 \n",
      "Loss:0.22930078208446503\n",
      "Epoch:4 \n",
      "Iteration:159 \n",
      "Loss:0.021253779530525208\n",
      "Epoch:4 \n",
      "Iteration:160 \n",
      "Loss:0.0204850435256958\n",
      "Epoch:4 \n",
      "Iteration:161 \n",
      "Loss:0.025805898010730743\n",
      "Epoch:4 \n",
      "Iteration:162 \n",
      "Loss:0.012012061662971973\n",
      "Epoch:4 \n",
      "Iteration:163 \n",
      "Loss:0.04875238239765167\n",
      "Epoch:4 \n",
      "Iteration:164 \n",
      "Loss:0.03282483294606209\n",
      "Epoch:4 \n",
      "Iteration:165 \n",
      "Loss:0.06108585000038147\n",
      "Epoch:4 \n",
      "Iteration:166 \n",
      "Loss:0.09782817959785461\n",
      "Epoch:4 \n",
      "Iteration:167 \n",
      "Loss:0.0010686555178835988\n",
      "Epoch:4 \n",
      "Iteration:168 \n",
      "Loss:0.012737821787595749\n",
      "Epoch:4 \n",
      "Iteration:169 \n",
      "Loss:0.053565338253974915\n",
      "Epoch:4 \n",
      "Iteration:170 \n",
      "Loss:0.0070737628266215324\n",
      "Epoch:4 \n",
      "Iteration:171 \n",
      "Loss:0.07067570090293884\n",
      "Epoch:4 \n",
      "Iteration:172 \n",
      "Loss:0.002382984384894371\n",
      "Epoch:4 \n",
      "Iteration:173 \n",
      "Loss:0.0251668319106102\n",
      "Epoch:4 \n",
      "Iteration:174 \n",
      "Loss:0.06688268482685089\n",
      "Epoch:4 \n",
      "Iteration:175 \n",
      "Loss:0.05048993602395058\n",
      "Epoch:4 \n",
      "Iteration:176 \n",
      "Loss:0.05242740735411644\n",
      "Epoch:4 \n",
      "Iteration:177 \n",
      "Loss:0.0658479705452919\n",
      "Epoch:4 \n",
      "Iteration:178 \n",
      "Loss:0.012247789651155472\n",
      "Epoch:4 \n",
      "Iteration:179 \n",
      "Loss:0.03499563783407211\n",
      "Epoch:4 \n",
      "Iteration:180 \n",
      "Loss:0.03592647984623909\n",
      "Epoch:4 \n",
      "Iteration:181 \n",
      "Loss:0.06284985691308975\n",
      "Epoch:4 \n",
      "Iteration:182 \n",
      "Loss:0.008889662101864815\n",
      "Epoch:4 \n",
      "Iteration:183 \n",
      "Loss:0.03530336171388626\n",
      "Epoch:4 \n",
      "Iteration:184 \n",
      "Loss:0.09110040962696075\n",
      "Epoch:4 \n",
      "Iteration:185 \n",
      "Loss:0.06151720881462097\n",
      "Epoch:4 \n",
      "Iteration:186 \n",
      "Loss:0.03167538717389107\n",
      "Epoch:4 \n",
      "Iteration:187 \n",
      "Loss:0.020943166688084602\n",
      "Epoch:4 \n",
      "Iteration:188 \n",
      "Loss:0.046729739755392075\n",
      "Epoch:4 \n",
      "Iteration:189 \n",
      "Loss:0.010419253259897232\n",
      "Epoch:4 \n",
      "Iteration:190 \n",
      "Loss:0.09722647070884705\n",
      "Epoch:4 \n",
      "Iteration:191 \n",
      "Loss:0.02741861157119274\n",
      "Epoch:4 \n",
      "Iteration:192 \n",
      "Loss:0.011463813483715057\n",
      "Epoch:4 \n",
      "Iteration:193 \n",
      "Loss:0.06587100774049759\n",
      "Epoch:4 \n",
      "Iteration:194 \n",
      "Loss:0.013265818357467651\n",
      "Epoch:4 \n",
      "Iteration:195 \n",
      "Loss:0.007137294393032789\n",
      "Epoch:4 \n",
      "Iteration:196 \n",
      "Loss:0.027204925194382668\n",
      "Epoch:4 \n",
      "Iteration:197 \n",
      "Loss:0.06652376055717468\n",
      "Epoch:4 \n",
      "Iteration:198 \n",
      "Loss:0.018772903829813004\n",
      "Epoch:4 \n",
      "Iteration:199 \n",
      "Loss:0.043611861765384674\n",
      "Epoch:4 \n",
      "Iteration:200 \n",
      "Loss:0.0396699495613575\n",
      "Epoch:4 \n",
      "Iteration:201 \n",
      "Loss:0.015717720612883568\n",
      "Epoch:4 \n",
      "Iteration:202 \n",
      "Loss:0.02865874022245407\n",
      "Epoch:4 \n",
      "Iteration:203 \n",
      "Loss:0.08666635304689407\n",
      "Epoch:4 \n",
      "Iteration:204 \n",
      "Loss:0.13036249577999115\n",
      "Epoch:4 \n",
      "Iteration:205 \n",
      "Loss:0.06751979887485504\n",
      "Epoch:4 \n",
      "Iteration:206 \n",
      "Loss:0.10360526293516159\n",
      "Epoch:4 \n",
      "Iteration:207 \n",
      "Loss:0.03300411254167557\n",
      "Epoch:4 \n",
      "Iteration:208 \n",
      "Loss:0.04355408996343613\n",
      "Epoch:4 \n",
      "Iteration:209 \n",
      "Loss:0.046397749334573746\n",
      "Epoch:4 \n",
      "Iteration:210 \n",
      "Loss:0.099107526242733\n",
      "Epoch:4 \n",
      "Iteration:211 \n",
      "Loss:0.005363555159419775\n",
      "Epoch:4 \n",
      "Iteration:212 \n",
      "Loss:0.016780894249677658\n",
      "Epoch:4 \n",
      "Iteration:213 \n",
      "Loss:0.034937020391225815\n",
      "Epoch:4 \n",
      "Iteration:214 \n",
      "Loss:0.032664209604263306\n",
      "Epoch:4 \n",
      "Iteration:215 \n",
      "Loss:0.021255025640130043\n",
      "Epoch:4 \n",
      "Iteration:216 \n",
      "Loss:0.0952213853597641\n",
      "Epoch:4 \n",
      "Iteration:217 \n",
      "Loss:0.026839274913072586\n",
      "Epoch:4 \n",
      "Iteration:218 \n",
      "Loss:0.02510874904692173\n",
      "Epoch:4 \n",
      "Iteration:219 \n",
      "Loss:0.0050577991642057896\n",
      "Epoch:4 \n",
      "Iteration:220 \n",
      "Loss:0.0354459248483181\n",
      "Epoch:4 \n",
      "Iteration:221 \n",
      "Loss:0.013688416220247746\n",
      "Epoch:4 \n",
      "Iteration:222 \n",
      "Loss:0.010984021238982677\n",
      "Epoch:4 \n",
      "Iteration:223 \n",
      "Loss:0.01632738672196865\n",
      "Epoch:4 \n",
      "Iteration:224 \n",
      "Loss:0.008102284744381905\n",
      "Epoch:4 \n",
      "Iteration:225 \n",
      "Loss:0.019166121259331703\n",
      "Epoch:4 \n",
      "Iteration:226 \n",
      "Loss:0.008138826116919518\n",
      "Epoch:4 \n",
      "Iteration:227 \n",
      "Loss:0.02244025655090809\n",
      "Epoch:4 \n",
      "Iteration:228 \n",
      "Loss:0.10369366407394409\n",
      "Epoch:4 \n",
      "Iteration:229 \n",
      "Loss:0.04329647496342659\n",
      "Epoch:4 \n",
      "Iteration:230 \n",
      "Loss:0.021417617797851562\n",
      "Epoch:4 \n",
      "Iteration:231 \n",
      "Loss:0.012163382954895496\n",
      "Epoch:4 \n",
      "Iteration:232 \n",
      "Loss:0.07784514129161835\n",
      "Epoch:4 \n",
      "Iteration:233 \n",
      "Loss:0.05458538979291916\n",
      "Epoch:4 \n",
      "Iteration:234 \n",
      "Loss:0.08573481440544128\n",
      "Epoch:4 \n",
      "Iteration:235 \n",
      "Loss:0.018282504752278328\n",
      "Epoch:4 \n",
      "Iteration:236 \n",
      "Loss:0.03855879232287407\n",
      "Epoch:4 \n",
      "Iteration:237 \n",
      "Loss:0.0955142080783844\n",
      "Epoch:4 \n",
      "Iteration:238 \n",
      "Loss:0.15187755227088928\n",
      "Epoch:4 \n",
      "Iteration:239 \n",
      "Loss:0.07280685752630234\n",
      "Epoch:4 \n",
      "Iteration:240 \n",
      "Loss:0.15878306329250336\n",
      "Epoch:4 \n",
      "Iteration:241 \n",
      "Loss:0.06972454488277435\n",
      "Epoch:4 \n",
      "Iteration:242 \n",
      "Loss:0.012178049422800541\n",
      "Epoch:4 \n",
      "Iteration:243 \n",
      "Loss:0.0801473930478096\n",
      "Epoch:4 \n",
      "Iteration:244 \n",
      "Loss:0.1338726282119751\n",
      "Epoch:4 \n",
      "Iteration:245 \n",
      "Loss:0.0067077274434268475\n",
      "Epoch:4 \n",
      "Iteration:246 \n",
      "Loss:0.034120168536901474\n",
      "Epoch:4 \n",
      "Iteration:247 \n",
      "Loss:0.052360281348228455\n",
      "Epoch:4 \n",
      "Iteration:248 \n",
      "Loss:0.057258643209934235\n",
      "Epoch:4 \n",
      "Iteration:249 \n",
      "Loss:0.12406441569328308\n",
      "Epoch:4 \n",
      "Iteration:250 \n",
      "Loss:0.05287754908204079\n",
      "Epoch:4 \n",
      "Iteration:251 \n",
      "Loss:0.1877429336309433\n",
      "Epoch:4 \n",
      "Iteration:252 \n",
      "Loss:0.08802897483110428\n",
      "Epoch:4 \n",
      "Iteration:253 \n",
      "Loss:0.06309767812490463\n",
      "Epoch:4 \n",
      "Iteration:254 \n",
      "Loss:0.03678484633564949\n",
      "Epoch:4 \n",
      "Iteration:255 \n",
      "Loss:0.014342695474624634\n",
      "Epoch:4 \n",
      "Iteration:256 \n",
      "Loss:0.044328488409519196\n",
      "Epoch:4 \n",
      "Iteration:257 \n",
      "Loss:0.053095147013664246\n",
      "Epoch:4 \n",
      "Iteration:258 \n",
      "Loss:0.04106579348444939\n",
      "Epoch:4 \n",
      "Iteration:259 \n",
      "Loss:0.04265962541103363\n",
      "Epoch:4 \n",
      "Iteration:260 \n",
      "Loss:0.03779234364628792\n",
      "Epoch:4 \n",
      "Iteration:261 \n",
      "Loss:0.048096828162670135\n",
      "Epoch:4 \n",
      "Iteration:262 \n",
      "Loss:0.05857823044061661\n",
      "Epoch:4 \n",
      "Iteration:263 \n",
      "Loss:0.14568962156772614\n",
      "Epoch:4 \n",
      "Iteration:264 \n",
      "Loss:0.010872261598706245\n",
      "Epoch:4 \n",
      "Iteration:265 \n",
      "Loss:0.030020492151379585\n",
      "Epoch:4 \n",
      "Iteration:266 \n",
      "Loss:0.03108099475502968\n",
      "Epoch:4 \n",
      "Iteration:267 \n",
      "Loss:0.060746122151613235\n",
      "Epoch:4 \n",
      "Iteration:268 \n",
      "Loss:0.010861274786293507\n",
      "Epoch:4 \n",
      "Iteration:269 \n",
      "Loss:0.041624534875154495\n",
      "Epoch:4 \n",
      "Iteration:270 \n",
      "Loss:0.04964950680732727\n",
      "Epoch:4 \n",
      "Iteration:271 \n",
      "Loss:0.02079024724662304\n",
      "Epoch:4 \n",
      "Iteration:272 \n",
      "Loss:0.039441172033548355\n",
      "Epoch:4 \n",
      "Iteration:273 \n",
      "Loss:0.03411968797445297\n",
      "Epoch:4 \n",
      "Iteration:274 \n",
      "Loss:0.026641465723514557\n",
      "Epoch:4 \n",
      "Iteration:275 \n",
      "Loss:0.02539953961968422\n",
      "Epoch:4 \n",
      "Iteration:276 \n",
      "Loss:0.047852907329797745\n",
      "Epoch:4 \n",
      "Iteration:277 \n",
      "Loss:0.04497009143233299\n",
      "Epoch:4 \n",
      "Iteration:278 \n",
      "Loss:0.01229097880423069\n",
      "Epoch:4 \n",
      "Iteration:279 \n",
      "Loss:0.0556337833404541\n",
      "Epoch:4 \n",
      "Iteration:280 \n",
      "Loss:0.058529749512672424\n",
      "Epoch:4 \n",
      "Iteration:281 \n",
      "Loss:0.09918298572301865\n",
      "Epoch:4 \n",
      "Iteration:282 \n",
      "Loss:0.030847666785120964\n",
      "Epoch:4 \n",
      "Iteration:283 \n",
      "Loss:0.053102895617485046\n",
      "Epoch:4 \n",
      "Iteration:284 \n",
      "Loss:0.04342872276902199\n",
      "Epoch:4 \n",
      "Iteration:285 \n",
      "Loss:0.029163140803575516\n",
      "Epoch:4 \n",
      "Iteration:286 \n",
      "Loss:0.1216549426317215\n",
      "Epoch:4 \n",
      "Iteration:287 \n",
      "Loss:0.049458179622888565\n",
      "Epoch:4 \n",
      "Iteration:288 \n",
      "Loss:0.019466133788228035\n",
      "Epoch:4 \n",
      "Iteration:289 \n",
      "Loss:0.015868451446294785\n",
      "Epoch:4 \n",
      "Iteration:290 \n",
      "Loss:0.026472298428416252\n",
      "Epoch:4 \n",
      "Iteration:291 \n",
      "Loss:0.008392277173697948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4 \n",
      "Iteration:292 \n",
      "Loss:0.1151043251156807\n",
      "Epoch:4 \n",
      "Iteration:293 \n",
      "Loss:0.03363701328635216\n",
      "Epoch:4 \n",
      "Iteration:294 \n",
      "Loss:0.0700664296746254\n",
      "Epoch:4 \n",
      "Iteration:295 \n",
      "Loss:0.03385183960199356\n",
      "Epoch:4 \n",
      "Iteration:296 \n",
      "Loss:0.0621645413339138\n",
      "Epoch:4 \n",
      "Iteration:297 \n",
      "Loss:0.030149735510349274\n",
      "Epoch:4 \n",
      "Iteration:298 \n",
      "Loss:0.017900250852108\n",
      "Epoch:4 \n",
      "Iteration:299 \n",
      "Loss:0.018759354948997498\n",
      "Epoch:4 \n",
      "Iteration:300 \n",
      "Loss:0.01161237433552742\n",
      "Epoch:4 \n",
      "Iteration:301 \n",
      "Loss:0.05781356990337372\n",
      "Epoch:4 \n",
      "Iteration:302 \n",
      "Loss:0.13541348278522491\n",
      "Epoch:4 \n",
      "Iteration:303 \n",
      "Loss:0.03125126659870148\n",
      "Epoch:4 \n",
      "Iteration:304 \n",
      "Loss:0.04410817101597786\n",
      "Epoch:4 \n",
      "Iteration:305 \n",
      "Loss:0.01324605941772461\n",
      "Epoch:4 \n",
      "Iteration:306 \n",
      "Loss:0.046619124710559845\n",
      "Epoch:4 \n",
      "Iteration:307 \n",
      "Loss:0.01618296466767788\n",
      "Epoch:4 \n",
      "Iteration:308 \n",
      "Loss:0.05036887153983116\n",
      "Epoch:4 \n",
      "Iteration:309 \n",
      "Loss:0.046777963638305664\n",
      "Epoch:4 \n",
      "Iteration:310 \n",
      "Loss:0.05538192763924599\n",
      "Epoch:4 \n",
      "Iteration:311 \n",
      "Loss:0.009595093317329884\n",
      "Epoch:4 \n",
      "Iteration:312 \n",
      "Loss:0.021324045956134796\n",
      "Epoch:4 \n",
      "Iteration:313 \n",
      "Loss:0.057597726583480835\n",
      "Epoch:4 \n",
      "Iteration:314 \n",
      "Loss:0.007440105080604553\n",
      "Epoch:4 \n",
      "Iteration:315 \n",
      "Loss:0.032388605177402496\n",
      "Epoch:4 \n",
      "Iteration:316 \n",
      "Loss:0.029006514698266983\n",
      "Epoch:4 \n",
      "Iteration:317 \n",
      "Loss:0.059292688965797424\n",
      "Epoch:4 \n",
      "Iteration:318 \n",
      "Loss:0.0158858485519886\n",
      "Epoch:4 \n",
      "Iteration:319 \n",
      "Loss:0.009078847244381905\n",
      "Epoch:4 \n",
      "Iteration:320 \n",
      "Loss:0.12016525119543076\n",
      "Epoch:4 \n",
      "Iteration:321 \n",
      "Loss:0.09501208364963531\n",
      "Epoch:4 \n",
      "Iteration:322 \n",
      "Loss:0.16298708319664001\n",
      "Epoch:4 \n",
      "Iteration:323 \n",
      "Loss:0.013584684580564499\n",
      "Epoch:4 \n",
      "Iteration:324 \n",
      "Loss:0.034604012966156006\n",
      "Epoch:4 \n",
      "Iteration:325 \n",
      "Loss:0.026373988017439842\n",
      "Epoch:4 \n",
      "Iteration:326 \n",
      "Loss:0.025165721774101257\n",
      "Epoch:4 \n",
      "Iteration:327 \n",
      "Loss:0.014634179882705212\n",
      "Epoch:4 \n",
      "Iteration:328 \n",
      "Loss:0.05756602808833122\n",
      "Epoch:4 \n",
      "Iteration:329 \n",
      "Loss:0.031187834218144417\n",
      "Epoch:4 \n",
      "Iteration:330 \n",
      "Loss:0.11075112223625183\n",
      "Epoch:4 \n",
      "Iteration:331 \n",
      "Loss:0.1315167397260666\n",
      "Epoch:4 \n",
      "Iteration:332 \n",
      "Loss:0.008925540372729301\n",
      "Epoch:4 \n",
      "Iteration:333 \n",
      "Loss:0.034147828817367554\n",
      "Epoch:4 \n",
      "Iteration:334 \n",
      "Loss:0.038945067673921585\n",
      "Epoch:4 \n",
      "Iteration:335 \n",
      "Loss:0.08711092919111252\n",
      "Epoch:4 \n",
      "Iteration:336 \n",
      "Loss:0.05929270386695862\n",
      "Epoch:4 \n",
      "Iteration:337 \n",
      "Loss:0.05204319581389427\n",
      "Epoch:4 \n",
      "Iteration:338 \n",
      "Loss:0.03917538374662399\n",
      "Epoch:4 \n",
      "Iteration:339 \n",
      "Loss:0.039570700377225876\n",
      "Epoch:4 \n",
      "Iteration:340 \n",
      "Loss:0.06836003065109253\n",
      "Epoch:4 \n",
      "Iteration:341 \n",
      "Loss:0.08269675076007843\n",
      "Epoch:4 \n",
      "Iteration:342 \n",
      "Loss:0.010118076577782631\n",
      "Epoch:4 \n",
      "Iteration:343 \n",
      "Loss:0.07666605710983276\n",
      "Epoch:4 \n",
      "Iteration:344 \n",
      "Loss:0.008852044120430946\n",
      "Epoch:4 \n",
      "Iteration:345 \n",
      "Loss:0.07971706986427307\n",
      "Epoch:4 \n",
      "Iteration:346 \n",
      "Loss:0.07816839963197708\n",
      "Epoch:4 \n",
      "Iteration:347 \n",
      "Loss:0.054274026304483414\n",
      "Epoch:4 \n",
      "Iteration:348 \n",
      "Loss:0.034354422241449356\n",
      "Epoch:4 \n",
      "Iteration:349 \n",
      "Loss:0.049411311745643616\n",
      "Epoch:4 \n",
      "Iteration:350 \n",
      "Loss:0.09869148582220078\n",
      "Epoch:4 \n",
      "Iteration:351 \n",
      "Loss:0.05709552764892578\n",
      "Epoch:4 \n",
      "Iteration:352 \n",
      "Loss:0.08586981892585754\n",
      "Epoch:4 \n",
      "Iteration:353 \n",
      "Loss:0.028442520648241043\n",
      "Epoch:4 \n",
      "Iteration:354 \n",
      "Loss:0.019120456650853157\n",
      "Epoch:4 \n",
      "Iteration:355 \n",
      "Loss:0.07481022924184799\n",
      "Epoch:4 \n",
      "Iteration:356 \n",
      "Loss:0.10527177155017853\n",
      "Epoch:4 \n",
      "Iteration:357 \n",
      "Loss:0.0619005523622036\n",
      "Epoch:4 \n",
      "Iteration:358 \n",
      "Loss:0.01030681747943163\n",
      "Epoch:4 \n",
      "Iteration:359 \n",
      "Loss:0.03534584864974022\n",
      "Epoch:4 \n",
      "Iteration:360 \n",
      "Loss:0.028305325657129288\n",
      "Epoch:4 \n",
      "Iteration:361 \n",
      "Loss:0.021814441308379173\n",
      "Epoch:4 \n",
      "Iteration:362 \n",
      "Loss:0.0297683198004961\n",
      "Epoch:4 \n",
      "Iteration:363 \n",
      "Loss:0.08374860137701035\n",
      "Epoch:4 \n",
      "Iteration:364 \n",
      "Loss:0.048098813742399216\n",
      "Epoch:4 \n",
      "Iteration:365 \n",
      "Loss:0.023173758760094643\n",
      "Epoch:4 \n",
      "Iteration:366 \n",
      "Loss:0.12156688421964645\n",
      "Epoch:4 \n",
      "Iteration:367 \n",
      "Loss:0.04499993845820427\n",
      "Epoch:4 \n",
      "Iteration:368 \n",
      "Loss:0.030780615285038948\n",
      "Epoch:4 \n",
      "Iteration:369 \n",
      "Loss:0.07484473288059235\n",
      "Epoch:4 \n",
      "Iteration:370 \n",
      "Loss:0.10344385355710983\n",
      "Epoch:4 \n",
      "Iteration:371 \n",
      "Loss:0.0921032726764679\n",
      "Epoch:4 \n",
      "Iteration:372 \n",
      "Loss:0.0654354840517044\n",
      "Epoch:4 \n",
      "Iteration:373 \n",
      "Loss:0.05005211383104324\n",
      "Epoch:4 \n",
      "Iteration:374 \n",
      "Loss:0.02038717269897461\n",
      "Epoch:4 \n",
      "Iteration:375 \n",
      "Loss:0.04223956540226936\n",
      "Epoch:4 \n",
      "Iteration:376 \n",
      "Loss:0.08283750712871552\n",
      "Epoch:4 \n",
      "Iteration:377 \n",
      "Loss:0.0768808051943779\n",
      "Epoch:4 \n",
      "Iteration:378 \n",
      "Loss:0.0410267636179924\n",
      "Epoch:4 \n",
      "Iteration:379 \n",
      "Loss:0.04930896311998367\n",
      "Epoch:4 \n",
      "Iteration:380 \n",
      "Loss:0.02938346192240715\n",
      "Epoch:4 \n",
      "Iteration:381 \n",
      "Loss:0.07919315993785858\n",
      "Epoch:4 \n",
      "Iteration:382 \n",
      "Loss:0.04901115596294403\n",
      "Epoch:4 \n",
      "Iteration:383 \n",
      "Loss:0.06069306284189224\n",
      "Epoch:4 \n",
      "Iteration:384 \n",
      "Loss:0.0152069590985775\n",
      "Epoch:4 \n",
      "Iteration:385 \n",
      "Loss:0.027033641934394836\n",
      "Epoch:4 \n",
      "Iteration:386 \n",
      "Loss:0.014176848344504833\n",
      "Epoch:4 \n",
      "Iteration:387 \n",
      "Loss:0.031892016530036926\n",
      "Epoch:4 \n",
      "Iteration:388 \n",
      "Loss:0.02979177050292492\n",
      "Epoch:4 \n",
      "Iteration:389 \n",
      "Loss:0.027069278061389923\n",
      "Epoch:4 \n",
      "Iteration:390 \n",
      "Loss:0.016626836732029915\n",
      "Epoch:4 \n",
      "Iteration:391 \n",
      "Loss:0.09286576509475708\n",
      "Epoch:4 \n",
      "Iteration:392 \n",
      "Loss:0.13967980444431305\n",
      "Epoch:4 \n",
      "Iteration:393 \n",
      "Loss:0.036345526576042175\n",
      "Epoch:4 \n",
      "Iteration:394 \n",
      "Loss:0.011369862593710423\n",
      "Epoch:4 \n",
      "Iteration:395 \n",
      "Loss:0.05397408455610275\n",
      "Epoch:4 \n",
      "Iteration:396 \n",
      "Loss:0.016800187528133392\n",
      "Epoch:4 \n",
      "Iteration:397 \n",
      "Loss:0.019090062007308006\n",
      "Epoch:4 \n",
      "Iteration:398 \n",
      "Loss:0.061925627291202545\n",
      "Epoch:4 \n",
      "Iteration:399 \n",
      "Loss:0.02510826848447323\n",
      "Epoch:4 \n",
      "Iteration:400 \n",
      "Loss:0.1153946965932846\n",
      "Epoch:4 \n",
      "Iteration:401 \n",
      "Loss:0.030572600662708282\n",
      "Epoch:4 \n",
      "Iteration:402 \n",
      "Loss:0.004262290894985199\n",
      "Epoch:4 \n",
      "Iteration:403 \n",
      "Loss:0.00553370825946331\n",
      "Epoch:4 \n",
      "Iteration:404 \n",
      "Loss:0.018416978418827057\n",
      "Epoch:4 \n",
      "Iteration:405 \n",
      "Loss:0.03513094782829285\n",
      "Epoch:4 \n",
      "Iteration:406 \n",
      "Loss:0.03506619110703468\n",
      "Epoch:4 \n",
      "Iteration:407 \n",
      "Loss:0.012176946736872196\n",
      "Epoch:4 \n",
      "Iteration:408 \n",
      "Loss:0.022709794342517853\n",
      "Epoch:4 \n",
      "Iteration:409 \n",
      "Loss:0.08302965760231018\n",
      "Epoch:4 \n",
      "Iteration:410 \n",
      "Loss:0.04280880093574524\n",
      "Epoch:4 \n",
      "Iteration:411 \n",
      "Loss:0.0362771637737751\n",
      "Epoch:4 \n",
      "Iteration:412 \n",
      "Loss:0.08058687299489975\n",
      "Epoch:4 \n",
      "Iteration:413 \n",
      "Loss:0.03942721709609032\n",
      "Epoch:4 \n",
      "Iteration:414 \n",
      "Loss:0.013724220916628838\n",
      "Epoch:4 \n",
      "Iteration:415 \n",
      "Loss:0.006410585716366768\n",
      "Epoch:4 \n",
      "Iteration:416 \n",
      "Loss:0.012704133987426758\n",
      "Epoch:4 \n",
      "Iteration:417 \n",
      "Loss:0.0651843249797821\n",
      "Epoch:4 \n",
      "Iteration:418 \n",
      "Loss:0.0464077852666378\n",
      "Epoch:4 \n",
      "Iteration:419 \n",
      "Loss:0.023719336837530136\n",
      "Epoch:4 \n",
      "Iteration:420 \n",
      "Loss:0.06330014020204544\n",
      "Epoch:4 \n",
      "Iteration:421 \n",
      "Loss:0.025844551622867584\n",
      "Epoch:4 \n",
      "Iteration:422 \n",
      "Loss:0.05381614714860916\n",
      "Epoch:4 \n",
      "Iteration:423 \n",
      "Loss:0.007979905232787132\n",
      "Epoch:4 \n",
      "Iteration:424 \n",
      "Loss:0.0760030746459961\n",
      "Epoch:4 \n",
      "Iteration:425 \n",
      "Loss:0.04318208247423172\n",
      "Epoch:4 \n",
      "Iteration:426 \n",
      "Loss:0.05404441058635712\n",
      "Epoch:4 \n",
      "Iteration:427 \n",
      "Loss:0.04757403954863548\n",
      "Epoch:4 \n",
      "Iteration:428 \n",
      "Loss:0.06503885984420776\n",
      "Epoch:4 \n",
      "Iteration:429 \n",
      "Loss:0.06557431071996689\n",
      "Epoch:4 \n",
      "Iteration:430 \n",
      "Loss:0.04281027242541313\n",
      "Epoch:4 \n",
      "Iteration:431 \n",
      "Loss:0.14199618995189667\n",
      "Epoch:4 \n",
      "Iteration:432 \n",
      "Loss:0.06995493173599243\n",
      "Epoch:4 \n",
      "Iteration:433 \n",
      "Loss:0.011810476891696453\n",
      "Epoch:4 \n",
      "Iteration:434 \n",
      "Loss:0.009954454377293587\n",
      "Epoch:4 \n",
      "Iteration:435 \n",
      "Loss:0.08230546861886978\n",
      "Epoch:4 \n",
      "Iteration:436 \n",
      "Loss:0.07516516745090485\n",
      "Epoch:4 \n",
      "Iteration:437 \n",
      "Loss:0.04371054098010063\n",
      "Epoch:4 \n",
      "Iteration:438 \n",
      "Loss:0.053136616945266724\n",
      "Epoch:4 \n",
      "Iteration:439 \n",
      "Loss:0.0999719649553299\n",
      "Epoch:4 \n",
      "Iteration:440 \n",
      "Loss:0.039290495216846466\n",
      "Epoch:4 \n",
      "Iteration:441 \n",
      "Loss:0.08188962936401367\n",
      "Epoch:4 \n",
      "Iteration:442 \n",
      "Loss:0.02714819833636284\n",
      "Epoch:4 \n",
      "Iteration:443 \n",
      "Loss:0.07733628898859024\n",
      "Epoch:4 \n",
      "Iteration:444 \n",
      "Loss:0.1293228417634964\n",
      "Epoch:4 \n",
      "Iteration:445 \n",
      "Loss:0.03363217040896416\n",
      "Epoch:4 \n",
      "Iteration:446 \n",
      "Loss:0.0377047173678875\n",
      "Epoch:4 \n",
      "Iteration:447 \n",
      "Loss:0.039709605276584625\n",
      "Epoch:4 \n",
      "Iteration:448 \n",
      "Loss:0.01902863010764122\n",
      "Epoch:4 \n",
      "Iteration:449 \n",
      "Loss:0.09580697864294052\n",
      "Epoch:4 \n",
      "Iteration:450 \n",
      "Loss:0.15124060213565826\n",
      "Epoch:4 \n",
      "Iteration:451 \n",
      "Loss:0.005545345135033131\n",
      "Epoch:4 \n",
      "Iteration:452 \n",
      "Loss:0.11923860758543015\n",
      "Epoch:4 \n",
      "Iteration:453 \n",
      "Loss:0.006934755481779575\n",
      "Epoch:4 \n",
      "Iteration:454 \n",
      "Loss:0.07091594487428665\n",
      "Epoch:4 \n",
      "Iteration:455 \n",
      "Loss:0.06721224635839462\n",
      "Epoch:4 \n",
      "Iteration:456 \n",
      "Loss:0.052284542471170425\n",
      "Epoch:4 \n",
      "Iteration:457 \n",
      "Loss:0.032030023634433746\n",
      "Epoch:4 \n",
      "Iteration:458 \n",
      "Loss:0.03915654122829437\n",
      "Epoch:4 \n",
      "Iteration:459 \n",
      "Loss:0.05819541960954666\n",
      "Epoch:4 \n",
      "Iteration:460 \n",
      "Loss:0.12053148448467255\n",
      "Epoch:4 \n",
      "Iteration:461 \n",
      "Loss:0.04694046452641487\n",
      "Epoch:4 \n",
      "Iteration:462 \n",
      "Loss:0.05128633975982666\n",
      "Epoch:4 \n",
      "Iteration:463 \n",
      "Loss:0.042960718274116516\n",
      "Epoch:4 \n",
      "Iteration:464 \n",
      "Loss:0.08014263212680817\n",
      "Epoch:4 \n",
      "Iteration:465 \n",
      "Loss:0.1014503762125969\n",
      "Epoch:4 \n",
      "Iteration:466 \n",
      "Loss:0.04885342717170715\n",
      "Epoch:4 \n",
      "Iteration:467 \n",
      "Loss:0.044103365391492844\n",
      "Epoch:4 \n",
      "Iteration:468 \n",
      "Loss:0.00812288373708725\n",
      "Epoch:4 \n",
      "Iteration:469 \n",
      "Loss:0.01095378864556551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4 \n",
      "Iteration:470 \n",
      "Loss:0.018764303997159004\n",
      "Epoch:4 \n",
      "Iteration:471 \n",
      "Loss:0.12191998213529587\n",
      "Epoch:4 \n",
      "Iteration:472 \n",
      "Loss:0.017470842227339745\n",
      "Epoch:4 \n",
      "Iteration:473 \n",
      "Loss:0.05841919034719467\n",
      "Epoch:4 \n",
      "Iteration:474 \n",
      "Loss:0.07395163178443909\n",
      "Epoch:4 \n",
      "Iteration:475 \n",
      "Loss:0.03661569952964783\n",
      "Epoch:4 \n",
      "Iteration:476 \n",
      "Loss:0.1069280207157135\n",
      "Epoch:4 \n",
      "Iteration:477 \n",
      "Loss:0.020640652626752853\n",
      "Epoch:4 \n",
      "Iteration:478 \n",
      "Loss:0.051563773304224014\n",
      "Epoch:4 \n",
      "Iteration:479 \n",
      "Loss:0.03260396420955658\n",
      "Epoch:4 \n",
      "Iteration:480 \n",
      "Loss:0.04502737149596214\n",
      "Epoch:4 \n",
      "Iteration:481 \n",
      "Loss:0.01069057360291481\n",
      "Epoch:4 \n",
      "Iteration:482 \n",
      "Loss:0.04583257809281349\n",
      "Epoch:4 \n",
      "Iteration:483 \n",
      "Loss:0.03359924629330635\n",
      "Epoch:4 \n",
      "Iteration:484 \n",
      "Loss:0.07638128101825714\n",
      "Epoch:4 \n",
      "Iteration:485 \n",
      "Loss:0.11755654215812683\n",
      "Epoch:4 \n",
      "Iteration:486 \n",
      "Loss:0.014886762946844101\n",
      "Epoch:4 \n",
      "Iteration:487 \n",
      "Loss:0.02165236882865429\n",
      "Epoch:4 \n",
      "Iteration:488 \n",
      "Loss:0.07066107541322708\n",
      "Epoch:4 \n",
      "Iteration:489 \n",
      "Loss:0.01928560808300972\n",
      "Epoch:4 \n",
      "Iteration:490 \n",
      "Loss:0.07626079767942429\n",
      "Epoch:4 \n",
      "Iteration:491 \n",
      "Loss:0.0028982048388570547\n",
      "Epoch:4 \n",
      "Iteration:492 \n",
      "Loss:0.031610194593667984\n",
      "Epoch:4 \n",
      "Iteration:493 \n",
      "Loss:0.02797190472483635\n",
      "Epoch:4 \n",
      "Iteration:494 \n",
      "Loss:0.044019054621458054\n",
      "Epoch:4 \n",
      "Iteration:495 \n",
      "Loss:0.07471911609172821\n",
      "Epoch:4 \n",
      "Iteration:496 \n",
      "Loss:0.1199987605214119\n",
      "Epoch:4 \n",
      "Iteration:497 \n",
      "Loss:0.05997126176953316\n",
      "Epoch:4 \n",
      "Iteration:498 \n",
      "Loss:0.11776549369096756\n",
      "Epoch:4 \n",
      "Iteration:499 \n",
      "Loss:0.015714412555098534\n",
      "Epoch:4 \n",
      "Iteration:500 \n",
      "Loss:0.01498736534267664\n",
      "Epoch:4 \n",
      "Iteration:501 \n",
      "Loss:0.005744211841374636\n",
      "Epoch:4 \n",
      "Iteration:502 \n",
      "Loss:0.10182604193687439\n",
      "Epoch:4 \n",
      "Iteration:503 \n",
      "Loss:0.011080545373260975\n",
      "Epoch:4 \n",
      "Iteration:504 \n",
      "Loss:0.032462649047374725\n",
      "Epoch:4 \n",
      "Iteration:505 \n",
      "Loss:0.03780407831072807\n",
      "Epoch:4 \n",
      "Iteration:506 \n",
      "Loss:0.015614187344908714\n",
      "Epoch:4 \n",
      "Iteration:507 \n",
      "Loss:0.06760770082473755\n",
      "Epoch:4 \n",
      "Iteration:508 \n",
      "Loss:0.016738038510084152\n",
      "Epoch:4 \n",
      "Iteration:509 \n",
      "Loss:0.01658971793949604\n",
      "Epoch:4 \n",
      "Iteration:510 \n",
      "Loss:0.024305012077093124\n",
      "Epoch:4 \n",
      "Iteration:511 \n",
      "Loss:0.04759565740823746\n",
      "Epoch:4 \n",
      "Iteration:512 \n",
      "Loss:0.06693946570158005\n",
      "Epoch:4 \n",
      "Iteration:513 \n",
      "Loss:0.022829845547676086\n",
      "Epoch:4 \n",
      "Iteration:514 \n",
      "Loss:0.06104227527976036\n",
      "Epoch:4 \n",
      "Iteration:515 \n",
      "Loss:0.0050639077089726925\n",
      "Epoch:4 \n",
      "Iteration:516 \n",
      "Loss:0.024346036836504936\n",
      "Epoch:4 \n",
      "Iteration:517 \n",
      "Loss:0.10049983859062195\n",
      "Epoch:4 \n",
      "Iteration:518 \n",
      "Loss:0.09313666075468063\n",
      "Epoch:4 \n",
      "Iteration:519 \n",
      "Loss:0.030021915212273598\n",
      "Epoch:4 \n",
      "Iteration:520 \n",
      "Loss:0.04179150238633156\n",
      "Epoch:4 \n",
      "Iteration:521 \n",
      "Loss:0.02456755004823208\n",
      "Epoch:4 \n",
      "Iteration:522 \n",
      "Loss:0.03723936900496483\n",
      "Epoch:4 \n",
      "Iteration:523 \n",
      "Loss:0.07391918450593948\n",
      "Epoch:4 \n",
      "Iteration:524 \n",
      "Loss:0.020798394456505775\n",
      "Epoch:4 \n",
      "Iteration:525 \n",
      "Loss:0.05160648748278618\n",
      "Epoch:4 \n",
      "Iteration:526 \n",
      "Loss:0.03905922919511795\n",
      "Epoch:4 \n",
      "Iteration:527 \n",
      "Loss:0.06144167110323906\n",
      "Epoch:4 \n",
      "Iteration:528 \n",
      "Loss:0.09771379828453064\n",
      "Epoch:4 \n",
      "Iteration:529 \n",
      "Loss:0.027142181992530823\n",
      "Epoch:4 \n",
      "Iteration:530 \n",
      "Loss:0.13097403943538666\n",
      "Epoch:4 \n",
      "Iteration:531 \n",
      "Loss:0.010433226823806763\n",
      "Epoch:4 \n",
      "Iteration:532 \n",
      "Loss:0.016229771077632904\n",
      "Epoch:4 \n",
      "Iteration:533 \n",
      "Loss:0.03643365204334259\n",
      "Epoch:4 \n",
      "Iteration:534 \n",
      "Loss:0.023893823847174644\n",
      "Epoch:4 \n",
      "Iteration:535 \n",
      "Loss:0.006459407042711973\n",
      "Epoch:4 \n",
      "Iteration:536 \n",
      "Loss:0.05996400862932205\n",
      "Epoch:4 \n",
      "Iteration:537 \n",
      "Loss:0.05098329856991768\n",
      "Epoch:4 \n",
      "Iteration:538 \n",
      "Loss:0.06003999710083008\n",
      "Epoch:4 \n",
      "Iteration:539 \n",
      "Loss:0.06455986201763153\n",
      "Epoch:4 \n",
      "Iteration:540 \n",
      "Loss:0.0018409814219921827\n",
      "Epoch:4 \n",
      "Iteration:541 \n",
      "Loss:0.03902461379766464\n",
      "Epoch:4 \n",
      "Iteration:542 \n",
      "Loss:0.04378477483987808\n",
      "Epoch:4 \n",
      "Iteration:543 \n",
      "Loss:0.0061819530092179775\n",
      "Epoch:4 \n",
      "Iteration:544 \n",
      "Loss:0.047182999551296234\n",
      "Epoch:4 \n",
      "Iteration:545 \n",
      "Loss:0.05746840313076973\n",
      "Epoch:4 \n",
      "Iteration:546 \n",
      "Loss:0.01906627230346203\n",
      "Epoch:4 \n",
      "Iteration:547 \n",
      "Loss:0.20936216413974762\n",
      "Epoch:4 \n",
      "Iteration:548 \n",
      "Loss:0.0839746966958046\n",
      "Epoch:4 \n",
      "Iteration:549 \n",
      "Loss:0.024327969178557396\n",
      "Epoch:4 \n",
      "Iteration:550 \n",
      "Loss:0.042461663484573364\n",
      "Epoch:4 \n",
      "Iteration:551 \n",
      "Loss:0.01974553056061268\n",
      "Epoch:4 \n",
      "Iteration:552 \n",
      "Loss:0.05275796353816986\n",
      "Epoch:4 \n",
      "Iteration:553 \n",
      "Loss:0.11748737096786499\n",
      "Epoch:4 \n",
      "Iteration:554 \n",
      "Loss:0.17028820514678955\n",
      "Epoch:4 \n",
      "Iteration:555 \n",
      "Loss:0.012800345197319984\n",
      "Epoch:4 \n",
      "Iteration:556 \n",
      "Loss:0.17666266858577728\n",
      "Epoch:4 \n",
      "Iteration:557 \n",
      "Loss:0.028835639357566833\n",
      "Epoch:4 \n",
      "Iteration:558 \n",
      "Loss:0.04012872651219368\n",
      "Epoch:4 \n",
      "Iteration:559 \n",
      "Loss:0.04176301136612892\n",
      "Epoch:4 \n",
      "Iteration:560 \n",
      "Loss:0.07548405975103378\n",
      "Epoch:4 \n",
      "Iteration:561 \n",
      "Loss:0.0319393165409565\n",
      "Epoch:4 \n",
      "Iteration:562 \n",
      "Loss:0.058027878403663635\n",
      "Epoch:4 \n",
      "Iteration:563 \n",
      "Loss:0.08902982622385025\n",
      "Epoch:4 \n",
      "Iteration:564 \n",
      "Loss:0.04373681917786598\n",
      "Epoch:4 \n",
      "Iteration:565 \n",
      "Loss:0.03629143163561821\n",
      "Epoch:4 \n",
      "Iteration:566 \n",
      "Loss:0.035276684910058975\n",
      "Epoch:4 \n",
      "Iteration:567 \n",
      "Loss:0.07741788774728775\n",
      "Epoch:4 \n",
      "Iteration:568 \n",
      "Loss:0.03359391540288925\n",
      "Epoch:4 \n",
      "Iteration:569 \n",
      "Loss:0.1394750475883484\n",
      "Epoch:4 \n",
      "Iteration:570 \n",
      "Loss:0.10001648217439651\n",
      "Epoch:4 \n",
      "Iteration:571 \n",
      "Loss:0.01608988642692566\n",
      "Epoch:4 \n",
      "Iteration:572 \n",
      "Loss:0.05794857069849968\n",
      "Epoch:4 \n",
      "Iteration:573 \n",
      "Loss:0.01524822972714901\n",
      "Epoch:4 \n",
      "Iteration:574 \n",
      "Loss:0.1591728925704956\n",
      "Epoch:4 \n",
      "Iteration:575 \n",
      "Loss:0.0415838398039341\n",
      "Epoch:4 \n",
      "Iteration:576 \n",
      "Loss:0.05721773952245712\n",
      "Epoch:4 \n",
      "Iteration:577 \n",
      "Loss:0.1552903801202774\n",
      "Epoch:4 \n",
      "Iteration:578 \n",
      "Loss:0.007739701773971319\n",
      "Epoch:4 \n",
      "Iteration:579 \n",
      "Loss:0.029678814113140106\n",
      "Epoch:4 \n",
      "Iteration:580 \n",
      "Loss:0.0794377475976944\n",
      "Epoch:4 \n",
      "Iteration:581 \n",
      "Loss:0.020843911916017532\n",
      "Epoch:4 \n",
      "Iteration:582 \n",
      "Loss:0.07401937991380692\n",
      "Epoch:4 \n",
      "Iteration:583 \n",
      "Loss:0.06163157522678375\n",
      "Epoch:4 \n",
      "Iteration:584 \n",
      "Loss:0.07000530511140823\n",
      "Epoch:4 \n",
      "Iteration:585 \n",
      "Loss:0.030937686562538147\n",
      "Epoch:4 \n",
      "Iteration:586 \n",
      "Loss:0.0781107246875763\n",
      "Epoch:4 \n",
      "Iteration:587 \n",
      "Loss:0.03676345571875572\n",
      "Epoch:4 \n",
      "Iteration:588 \n",
      "Loss:0.008249121718108654\n",
      "Epoch:4 \n",
      "Iteration:589 \n",
      "Loss:0.028176402673125267\n",
      "Epoch:4 \n",
      "Iteration:590 \n",
      "Loss:0.007778591010719538\n",
      "Epoch:4 \n",
      "Iteration:591 \n",
      "Loss:0.09243366122245789\n",
      "Epoch:4 \n",
      "Iteration:592 \n",
      "Loss:0.06417675316333771\n",
      "Epoch:4 \n",
      "Iteration:593 \n",
      "Loss:0.0426030196249485\n",
      "Epoch:4 \n",
      "Iteration:594 \n",
      "Loss:0.12897726893424988\n",
      "Epoch:4 \n",
      "Iteration:595 \n",
      "Loss:0.03154204413294792\n",
      "Epoch:4 \n",
      "Iteration:596 \n",
      "Loss:0.07647789269685745\n",
      "Epoch:4 \n",
      "Iteration:597 \n",
      "Loss:0.006134346127510071\n",
      "Epoch:4 \n",
      "Iteration:598 \n",
      "Loss:0.028373437002301216\n",
      "Epoch:4 \n",
      "Iteration:599 \n",
      "Loss:0.028972206637263298\n",
      "Epoch:4 \n",
      "Iteration:600 \n",
      "Loss:0.17726197838783264\n",
      "\n",
      "Accuracy of network in epoch 4: 98.48833333333333\n",
      "Epoch:5 \n",
      "Iteration:1 \n",
      "Loss:0.008354908786714077\n",
      "Epoch:5 \n",
      "Iteration:2 \n",
      "Loss:0.0021204808726906776\n",
      "Epoch:5 \n",
      "Iteration:3 \n",
      "Loss:0.009838227182626724\n",
      "Epoch:5 \n",
      "Iteration:4 \n",
      "Loss:0.04806382209062576\n",
      "Epoch:5 \n",
      "Iteration:5 \n",
      "Loss:0.016117582097649574\n",
      "Epoch:5 \n",
      "Iteration:6 \n",
      "Loss:0.014580371789634228\n",
      "Epoch:5 \n",
      "Iteration:7 \n",
      "Loss:0.014269322156906128\n",
      "Epoch:5 \n",
      "Iteration:8 \n",
      "Loss:0.019681314006447792\n",
      "Epoch:5 \n",
      "Iteration:9 \n",
      "Loss:0.027688927948474884\n",
      "Epoch:5 \n",
      "Iteration:10 \n",
      "Loss:0.060438722372055054\n",
      "Epoch:5 \n",
      "Iteration:11 \n",
      "Loss:0.0070455027744174\n",
      "Epoch:5 \n",
      "Iteration:12 \n",
      "Loss:0.003211061004549265\n",
      "Epoch:5 \n",
      "Iteration:13 \n",
      "Loss:0.009114285930991173\n",
      "Epoch:5 \n",
      "Iteration:14 \n",
      "Loss:0.17354173958301544\n",
      "Epoch:5 \n",
      "Iteration:15 \n",
      "Loss:0.016876043751835823\n",
      "Epoch:5 \n",
      "Iteration:16 \n",
      "Loss:0.013226073235273361\n",
      "Epoch:5 \n",
      "Iteration:17 \n",
      "Loss:0.0037711127661168575\n",
      "Epoch:5 \n",
      "Iteration:18 \n",
      "Loss:0.02288839779794216\n",
      "Epoch:5 \n",
      "Iteration:19 \n",
      "Loss:0.0270115714520216\n",
      "Epoch:5 \n",
      "Iteration:20 \n",
      "Loss:0.09235724806785583\n",
      "Epoch:5 \n",
      "Iteration:21 \n",
      "Loss:0.013838011771440506\n",
      "Epoch:5 \n",
      "Iteration:22 \n",
      "Loss:0.008335795253515244\n",
      "Epoch:5 \n",
      "Iteration:23 \n",
      "Loss:0.021612191572785378\n",
      "Epoch:5 \n",
      "Iteration:24 \n",
      "Loss:0.029852796345949173\n",
      "Epoch:5 \n",
      "Iteration:25 \n",
      "Loss:0.008187114261090755\n",
      "Epoch:5 \n",
      "Iteration:26 \n",
      "Loss:0.059249456971883774\n",
      "Epoch:5 \n",
      "Iteration:27 \n",
      "Loss:0.04806073009967804\n",
      "Epoch:5 \n",
      "Iteration:28 \n",
      "Loss:0.0015549063682556152\n",
      "Epoch:5 \n",
      "Iteration:29 \n",
      "Loss:0.03204211965203285\n",
      "Epoch:5 \n",
      "Iteration:30 \n",
      "Loss:0.00381019851192832\n",
      "Epoch:5 \n",
      "Iteration:31 \n",
      "Loss:0.024857446551322937\n",
      "Epoch:5 \n",
      "Iteration:32 \n",
      "Loss:0.09588378667831421\n",
      "Epoch:5 \n",
      "Iteration:33 \n",
      "Loss:0.023676997050642967\n",
      "Epoch:5 \n",
      "Iteration:34 \n",
      "Loss:0.005142894573509693\n",
      "Epoch:5 \n",
      "Iteration:35 \n",
      "Loss:0.02087865024805069\n",
      "Epoch:5 \n",
      "Iteration:36 \n",
      "Loss:0.0809541866183281\n",
      "Epoch:5 \n",
      "Iteration:37 \n",
      "Loss:0.005338664632290602\n",
      "Epoch:5 \n",
      "Iteration:38 \n",
      "Loss:0.06231238320469856\n",
      "Epoch:5 \n",
      "Iteration:39 \n",
      "Loss:0.004364416468888521\n",
      "Epoch:5 \n",
      "Iteration:40 \n",
      "Loss:0.006477198097854853\n",
      "Epoch:5 \n",
      "Iteration:41 \n",
      "Loss:0.023357994854450226\n",
      "Epoch:5 \n",
      "Iteration:42 \n",
      "Loss:0.020256878808140755\n",
      "Epoch:5 \n",
      "Iteration:43 \n",
      "Loss:0.021849319338798523\n",
      "Epoch:5 \n",
      "Iteration:44 \n",
      "Loss:0.03112613968551159\n",
      "Epoch:5 \n",
      "Iteration:45 \n",
      "Loss:0.0739545151591301\n",
      "Epoch:5 \n",
      "Iteration:46 \n",
      "Loss:0.04631819203495979\n",
      "Epoch:5 \n",
      "Iteration:47 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.0501219742000103\n",
      "Epoch:5 \n",
      "Iteration:48 \n",
      "Loss:0.036879584193229675\n",
      "Epoch:5 \n",
      "Iteration:49 \n",
      "Loss:0.041280701756477356\n",
      "Epoch:5 \n",
      "Iteration:50 \n",
      "Loss:0.0423673540353775\n",
      "Epoch:5 \n",
      "Iteration:51 \n",
      "Loss:0.09156283736228943\n",
      "Epoch:5 \n",
      "Iteration:52 \n",
      "Loss:0.021895036101341248\n",
      "Epoch:5 \n",
      "Iteration:53 \n",
      "Loss:0.0639810636639595\n",
      "Epoch:5 \n",
      "Iteration:54 \n",
      "Loss:0.05017377808690071\n",
      "Epoch:5 \n",
      "Iteration:55 \n",
      "Loss:0.027040056884288788\n",
      "Epoch:5 \n",
      "Iteration:56 \n",
      "Loss:0.05508287996053696\n",
      "Epoch:5 \n",
      "Iteration:57 \n",
      "Loss:0.02778727561235428\n",
      "Epoch:5 \n",
      "Iteration:58 \n",
      "Loss:0.03794340789318085\n",
      "Epoch:5 \n",
      "Iteration:59 \n",
      "Loss:0.024515200406312943\n",
      "Epoch:5 \n",
      "Iteration:60 \n",
      "Loss:0.01581365242600441\n",
      "Epoch:5 \n",
      "Iteration:61 \n",
      "Loss:0.04929263889789581\n",
      "Epoch:5 \n",
      "Iteration:62 \n",
      "Loss:0.018748585134744644\n",
      "Epoch:5 \n",
      "Iteration:63 \n",
      "Loss:0.017937609925866127\n",
      "Epoch:5 \n",
      "Iteration:64 \n",
      "Loss:0.044574931263923645\n",
      "Epoch:5 \n",
      "Iteration:65 \n",
      "Loss:0.05748237669467926\n",
      "Epoch:5 \n",
      "Iteration:66 \n",
      "Loss:0.022236211225390434\n",
      "Epoch:5 \n",
      "Iteration:67 \n",
      "Loss:0.006080453284084797\n",
      "Epoch:5 \n",
      "Iteration:68 \n",
      "Loss:0.025870464742183685\n",
      "Epoch:5 \n",
      "Iteration:69 \n",
      "Loss:0.003083490766584873\n",
      "Epoch:5 \n",
      "Iteration:70 \n",
      "Loss:0.07590921223163605\n",
      "Epoch:5 \n",
      "Iteration:71 \n",
      "Loss:0.033865828067064285\n",
      "Epoch:5 \n",
      "Iteration:72 \n",
      "Loss:0.015424821525812149\n",
      "Epoch:5 \n",
      "Iteration:73 \n",
      "Loss:0.014791633002460003\n",
      "Epoch:5 \n",
      "Iteration:74 \n",
      "Loss:0.0077199786901474\n",
      "Epoch:5 \n",
      "Iteration:75 \n",
      "Loss:0.012633204460144043\n",
      "Epoch:5 \n",
      "Iteration:76 \n",
      "Loss:0.022363949567079544\n",
      "Epoch:5 \n",
      "Iteration:77 \n",
      "Loss:0.020985035225749016\n",
      "Epoch:5 \n",
      "Iteration:78 \n",
      "Loss:0.0016782167367637157\n",
      "Epoch:5 \n",
      "Iteration:79 \n",
      "Loss:0.08685922622680664\n",
      "Epoch:5 \n",
      "Iteration:80 \n",
      "Loss:0.07231438905000687\n",
      "Epoch:5 \n",
      "Iteration:81 \n",
      "Loss:0.041871003806591034\n",
      "Epoch:5 \n",
      "Iteration:82 \n",
      "Loss:0.00772493053227663\n",
      "Epoch:5 \n",
      "Iteration:83 \n",
      "Loss:0.056029416620731354\n",
      "Epoch:5 \n",
      "Iteration:84 \n",
      "Loss:0.005039834417402744\n",
      "Epoch:5 \n",
      "Iteration:85 \n",
      "Loss:0.0009520672028884292\n",
      "Epoch:5 \n",
      "Iteration:86 \n",
      "Loss:0.0017091198824346066\n",
      "Epoch:5 \n",
      "Iteration:87 \n",
      "Loss:0.02866588905453682\n",
      "Epoch:5 \n",
      "Iteration:88 \n",
      "Loss:0.0071078138425946236\n",
      "Epoch:5 \n",
      "Iteration:89 \n",
      "Loss:0.06789544224739075\n",
      "Epoch:5 \n",
      "Iteration:90 \n",
      "Loss:0.0069314269348979\n",
      "Epoch:5 \n",
      "Iteration:91 \n",
      "Loss:0.017479751259088516\n",
      "Epoch:5 \n",
      "Iteration:92 \n",
      "Loss:0.011844546534121037\n",
      "Epoch:5 \n",
      "Iteration:93 \n",
      "Loss:0.11414621025323868\n",
      "Epoch:5 \n",
      "Iteration:94 \n",
      "Loss:0.11900026351213455\n",
      "Epoch:5 \n",
      "Iteration:95 \n",
      "Loss:0.0513598769903183\n",
      "Epoch:5 \n",
      "Iteration:96 \n",
      "Loss:0.02156989835202694\n",
      "Epoch:5 \n",
      "Iteration:97 \n",
      "Loss:0.10540921241044998\n",
      "Epoch:5 \n",
      "Iteration:98 \n",
      "Loss:0.004759255796670914\n",
      "Epoch:5 \n",
      "Iteration:99 \n",
      "Loss:0.0017172759398818016\n",
      "Epoch:5 \n",
      "Iteration:100 \n",
      "Loss:0.018207835033535957\n",
      "Epoch:5 \n",
      "Iteration:101 \n",
      "Loss:0.05174718797206879\n",
      "Epoch:5 \n",
      "Iteration:102 \n",
      "Loss:0.0614677295088768\n",
      "Epoch:5 \n",
      "Iteration:103 \n",
      "Loss:0.11644528061151505\n",
      "Epoch:5 \n",
      "Iteration:104 \n",
      "Loss:0.062433015555143356\n",
      "Epoch:5 \n",
      "Iteration:105 \n",
      "Loss:0.008429351262748241\n",
      "Epoch:5 \n",
      "Iteration:106 \n",
      "Loss:0.01928906887769699\n",
      "Epoch:5 \n",
      "Iteration:107 \n",
      "Loss:0.015428189188241959\n",
      "Epoch:5 \n",
      "Iteration:108 \n",
      "Loss:0.029354562982916832\n",
      "Epoch:5 \n",
      "Iteration:109 \n",
      "Loss:0.025937790051102638\n",
      "Epoch:5 \n",
      "Iteration:110 \n",
      "Loss:0.056165728718042374\n",
      "Epoch:5 \n",
      "Iteration:111 \n",
      "Loss:0.05902191251516342\n",
      "Epoch:5 \n",
      "Iteration:112 \n",
      "Loss:0.008383464068174362\n",
      "Epoch:5 \n",
      "Iteration:113 \n",
      "Loss:0.007031195797026157\n",
      "Epoch:5 \n",
      "Iteration:114 \n",
      "Loss:0.029090743511915207\n",
      "Epoch:5 \n",
      "Iteration:115 \n",
      "Loss:0.09299366176128387\n",
      "Epoch:5 \n",
      "Iteration:116 \n",
      "Loss:0.021445002406835556\n",
      "Epoch:5 \n",
      "Iteration:117 \n",
      "Loss:0.011151851154863834\n",
      "Epoch:5 \n",
      "Iteration:118 \n",
      "Loss:0.049888186156749725\n",
      "Epoch:5 \n",
      "Iteration:119 \n",
      "Loss:0.05277197062969208\n",
      "Epoch:5 \n",
      "Iteration:120 \n",
      "Loss:0.012252547778189182\n",
      "Epoch:5 \n",
      "Iteration:121 \n",
      "Loss:0.05729968845844269\n",
      "Epoch:5 \n",
      "Iteration:122 \n",
      "Loss:0.013007535599172115\n",
      "Epoch:5 \n",
      "Iteration:123 \n",
      "Loss:0.030860135331749916\n",
      "Epoch:5 \n",
      "Iteration:124 \n",
      "Loss:0.010219590738415718\n",
      "Epoch:5 \n",
      "Iteration:125 \n",
      "Loss:0.01604657992720604\n",
      "Epoch:5 \n",
      "Iteration:126 \n",
      "Loss:0.014644069597125053\n",
      "Epoch:5 \n",
      "Iteration:127 \n",
      "Loss:0.03249460831284523\n",
      "Epoch:5 \n",
      "Iteration:128 \n",
      "Loss:0.02780020423233509\n",
      "Epoch:5 \n",
      "Iteration:129 \n",
      "Loss:0.013386514969170094\n",
      "Epoch:5 \n",
      "Iteration:130 \n",
      "Loss:0.05896317958831787\n",
      "Epoch:5 \n",
      "Iteration:131 \n",
      "Loss:0.007848761044442654\n",
      "Epoch:5 \n",
      "Iteration:132 \n",
      "Loss:0.00525230448693037\n",
      "Epoch:5 \n",
      "Iteration:133 \n",
      "Loss:0.025292033329606056\n",
      "Epoch:5 \n",
      "Iteration:134 \n",
      "Loss:0.09321035444736481\n",
      "Epoch:5 \n",
      "Iteration:135 \n",
      "Loss:0.014823350124061108\n",
      "Epoch:5 \n",
      "Iteration:136 \n",
      "Loss:0.03575638681650162\n",
      "Epoch:5 \n",
      "Iteration:137 \n",
      "Loss:0.09268287569284439\n",
      "Epoch:5 \n",
      "Iteration:138 \n",
      "Loss:0.00418532220646739\n",
      "Epoch:5 \n",
      "Iteration:139 \n",
      "Loss:0.005711697973310947\n",
      "Epoch:5 \n",
      "Iteration:140 \n",
      "Loss:0.017858222126960754\n",
      "Epoch:5 \n",
      "Iteration:141 \n",
      "Loss:0.008881667628884315\n",
      "Epoch:5 \n",
      "Iteration:142 \n",
      "Loss:0.023469991981983185\n",
      "Epoch:5 \n",
      "Iteration:143 \n",
      "Loss:0.031435318291187286\n",
      "Epoch:5 \n",
      "Iteration:144 \n",
      "Loss:0.047724198549985886\n",
      "Epoch:5 \n",
      "Iteration:145 \n",
      "Loss:0.0122687928378582\n",
      "Epoch:5 \n",
      "Iteration:146 \n",
      "Loss:0.01102321594953537\n",
      "Epoch:5 \n",
      "Iteration:147 \n",
      "Loss:0.0547788143157959\n",
      "Epoch:5 \n",
      "Iteration:148 \n",
      "Loss:0.019800670444965363\n",
      "Epoch:5 \n",
      "Iteration:149 \n",
      "Loss:0.007703104056417942\n",
      "Epoch:5 \n",
      "Iteration:150 \n",
      "Loss:0.03032558225095272\n",
      "Epoch:5 \n",
      "Iteration:151 \n",
      "Loss:0.007082965224981308\n",
      "Epoch:5 \n",
      "Iteration:152 \n",
      "Loss:0.08963114768266678\n",
      "Epoch:5 \n",
      "Iteration:153 \n",
      "Loss:0.011886557564139366\n",
      "Epoch:5 \n",
      "Iteration:154 \n",
      "Loss:0.005273689050227404\n",
      "Epoch:5 \n",
      "Iteration:155 \n",
      "Loss:0.0060187410563230515\n",
      "Epoch:5 \n",
      "Iteration:156 \n",
      "Loss:0.0047499011270701885\n",
      "Epoch:5 \n",
      "Iteration:157 \n",
      "Loss:0.001847783220000565\n",
      "Epoch:5 \n",
      "Iteration:158 \n",
      "Loss:0.006017403211444616\n",
      "Epoch:5 \n",
      "Iteration:159 \n",
      "Loss:0.03088475950062275\n",
      "Epoch:5 \n",
      "Iteration:160 \n",
      "Loss:0.022606918588280678\n",
      "Epoch:5 \n",
      "Iteration:161 \n",
      "Loss:0.04354383051395416\n",
      "Epoch:5 \n",
      "Iteration:162 \n",
      "Loss:0.053142450749874115\n",
      "Epoch:5 \n",
      "Iteration:163 \n",
      "Loss:0.0013626903528347611\n",
      "Epoch:5 \n",
      "Iteration:164 \n",
      "Loss:0.0022888618987053633\n",
      "Epoch:5 \n",
      "Iteration:165 \n",
      "Loss:0.10601752996444702\n",
      "Epoch:5 \n",
      "Iteration:166 \n",
      "Loss:0.027201468124985695\n",
      "Epoch:5 \n",
      "Iteration:167 \n",
      "Loss:0.029203757643699646\n",
      "Epoch:5 \n",
      "Iteration:168 \n",
      "Loss:0.003371194237843156\n",
      "Epoch:5 \n",
      "Iteration:169 \n",
      "Loss:0.1367059350013733\n",
      "Epoch:5 \n",
      "Iteration:170 \n",
      "Loss:0.0333084836602211\n",
      "Epoch:5 \n",
      "Iteration:171 \n",
      "Loss:0.0029286888893693686\n",
      "Epoch:5 \n",
      "Iteration:172 \n",
      "Loss:0.043764401227235794\n",
      "Epoch:5 \n",
      "Iteration:173 \n",
      "Loss:0.022074062377214432\n",
      "Epoch:5 \n",
      "Iteration:174 \n",
      "Loss:0.03626113757491112\n",
      "Epoch:5 \n",
      "Iteration:175 \n",
      "Loss:0.027319753542542458\n",
      "Epoch:5 \n",
      "Iteration:176 \n",
      "Loss:0.10259804874658585\n",
      "Epoch:5 \n",
      "Iteration:177 \n",
      "Loss:0.07937386631965637\n",
      "Epoch:5 \n",
      "Iteration:178 \n",
      "Loss:0.0366901233792305\n",
      "Epoch:5 \n",
      "Iteration:179 \n",
      "Loss:0.01942426711320877\n",
      "Epoch:5 \n",
      "Iteration:180 \n",
      "Loss:0.021640989929437637\n",
      "Epoch:5 \n",
      "Iteration:181 \n",
      "Loss:0.021282339468598366\n",
      "Epoch:5 \n",
      "Iteration:182 \n",
      "Loss:0.12226741760969162\n",
      "Epoch:5 \n",
      "Iteration:183 \n",
      "Loss:0.0194413885474205\n",
      "Epoch:5 \n",
      "Iteration:184 \n",
      "Loss:0.022255633026361465\n",
      "Epoch:5 \n",
      "Iteration:185 \n",
      "Loss:0.04461973160505295\n",
      "Epoch:5 \n",
      "Iteration:186 \n",
      "Loss:0.05878574401140213\n",
      "Epoch:5 \n",
      "Iteration:187 \n",
      "Loss:0.10168582946062088\n",
      "Epoch:5 \n",
      "Iteration:188 \n",
      "Loss:0.15556085109710693\n",
      "Epoch:5 \n",
      "Iteration:189 \n",
      "Loss:0.04161043092608452\n",
      "Epoch:5 \n",
      "Iteration:190 \n",
      "Loss:0.02827397733926773\n",
      "Epoch:5 \n",
      "Iteration:191 \n",
      "Loss:0.006266253534704447\n",
      "Epoch:5 \n",
      "Iteration:192 \n",
      "Loss:0.03830084949731827\n",
      "Epoch:5 \n",
      "Iteration:193 \n",
      "Loss:0.01181009691208601\n",
      "Epoch:5 \n",
      "Iteration:194 \n",
      "Loss:0.00519329309463501\n",
      "Epoch:5 \n",
      "Iteration:195 \n",
      "Loss:0.09154728055000305\n",
      "Epoch:5 \n",
      "Iteration:196 \n",
      "Loss:0.011460389941930771\n",
      "Epoch:5 \n",
      "Iteration:197 \n",
      "Loss:0.0032536129001528025\n",
      "Epoch:5 \n",
      "Iteration:198 \n",
      "Loss:0.04917233809828758\n",
      "Epoch:5 \n",
      "Iteration:199 \n",
      "Loss:0.06639426201581955\n",
      "Epoch:5 \n",
      "Iteration:200 \n",
      "Loss:0.03480757400393486\n",
      "Epoch:5 \n",
      "Iteration:201 \n",
      "Loss:0.0013720226706936955\n",
      "Epoch:5 \n",
      "Iteration:202 \n",
      "Loss:0.0681237056851387\n",
      "Epoch:5 \n",
      "Iteration:203 \n",
      "Loss:0.01267420407384634\n",
      "Epoch:5 \n",
      "Iteration:204 \n",
      "Loss:0.05429048836231232\n",
      "Epoch:5 \n",
      "Iteration:205 \n",
      "Loss:0.1858130395412445\n",
      "Epoch:5 \n",
      "Iteration:206 \n",
      "Loss:0.07387594878673553\n",
      "Epoch:5 \n",
      "Iteration:207 \n",
      "Loss:0.0027198835741728544\n",
      "Epoch:5 \n",
      "Iteration:208 \n",
      "Loss:0.026393337175250053\n",
      "Epoch:5 \n",
      "Iteration:209 \n",
      "Loss:0.007987418211996555\n",
      "Epoch:5 \n",
      "Iteration:210 \n",
      "Loss:0.06281453371047974\n",
      "Epoch:5 \n",
      "Iteration:211 \n",
      "Loss:0.04198765382170677\n",
      "Epoch:5 \n",
      "Iteration:212 \n",
      "Loss:0.015796007588505745\n",
      "Epoch:5 \n",
      "Iteration:213 \n",
      "Loss:0.025341691449284554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5 \n",
      "Iteration:214 \n",
      "Loss:0.04837237298488617\n",
      "Epoch:5 \n",
      "Iteration:215 \n",
      "Loss:0.05099866911768913\n",
      "Epoch:5 \n",
      "Iteration:216 \n",
      "Loss:0.10531184077262878\n",
      "Epoch:5 \n",
      "Iteration:217 \n",
      "Loss:0.020571578294038773\n",
      "Epoch:5 \n",
      "Iteration:218 \n",
      "Loss:0.0203409343957901\n",
      "Epoch:5 \n",
      "Iteration:219 \n",
      "Loss:0.002193605527281761\n",
      "Epoch:5 \n",
      "Iteration:220 \n",
      "Loss:0.01738307811319828\n",
      "Epoch:5 \n",
      "Iteration:221 \n",
      "Loss:0.02719496563076973\n",
      "Epoch:5 \n",
      "Iteration:222 \n",
      "Loss:0.011695698834955692\n",
      "Epoch:5 \n",
      "Iteration:223 \n",
      "Loss:0.045908451080322266\n",
      "Epoch:5 \n",
      "Iteration:224 \n",
      "Loss:0.056271396577358246\n",
      "Epoch:5 \n",
      "Iteration:225 \n",
      "Loss:0.0629812702536583\n",
      "Epoch:5 \n",
      "Iteration:226 \n",
      "Loss:0.005832304246723652\n",
      "Epoch:5 \n",
      "Iteration:227 \n",
      "Loss:0.01732846349477768\n",
      "Epoch:5 \n",
      "Iteration:228 \n",
      "Loss:0.04719158634543419\n",
      "Epoch:5 \n",
      "Iteration:229 \n",
      "Loss:0.02598273567855358\n",
      "Epoch:5 \n",
      "Iteration:230 \n",
      "Loss:0.020170358940958977\n",
      "Epoch:5 \n",
      "Iteration:231 \n",
      "Loss:0.009102598764002323\n",
      "Epoch:5 \n",
      "Iteration:232 \n",
      "Loss:0.004015134647488594\n",
      "Epoch:5 \n",
      "Iteration:233 \n",
      "Loss:0.05322112888097763\n",
      "Epoch:5 \n",
      "Iteration:234 \n",
      "Loss:0.0249424297362566\n",
      "Epoch:5 \n",
      "Iteration:235 \n",
      "Loss:0.008256280794739723\n",
      "Epoch:5 \n",
      "Iteration:236 \n",
      "Loss:0.11711681634187698\n",
      "Epoch:5 \n",
      "Iteration:237 \n",
      "Loss:0.012890856713056564\n",
      "Epoch:5 \n",
      "Iteration:238 \n",
      "Loss:0.021939408034086227\n",
      "Epoch:5 \n",
      "Iteration:239 \n",
      "Loss:0.0330989845097065\n",
      "Epoch:5 \n",
      "Iteration:240 \n",
      "Loss:0.014909918420016766\n",
      "Epoch:5 \n",
      "Iteration:241 \n",
      "Loss:0.042488984763622284\n",
      "Epoch:5 \n",
      "Iteration:242 \n",
      "Loss:0.0034189862199127674\n",
      "Epoch:5 \n",
      "Iteration:243 \n",
      "Loss:0.027727391570806503\n",
      "Epoch:5 \n",
      "Iteration:244 \n",
      "Loss:0.005130380392074585\n",
      "Epoch:5 \n",
      "Iteration:245 \n",
      "Loss:0.025456123054027557\n",
      "Epoch:5 \n",
      "Iteration:246 \n",
      "Loss:0.03983313962817192\n",
      "Epoch:5 \n",
      "Iteration:247 \n",
      "Loss:0.04004927724599838\n",
      "Epoch:5 \n",
      "Iteration:248 \n",
      "Loss:0.10825648158788681\n",
      "Epoch:5 \n",
      "Iteration:249 \n",
      "Loss:0.018194502219557762\n",
      "Epoch:5 \n",
      "Iteration:250 \n",
      "Loss:0.019829368218779564\n",
      "Epoch:5 \n",
      "Iteration:251 \n",
      "Loss:0.15166132152080536\n",
      "Epoch:5 \n",
      "Iteration:252 \n",
      "Loss:0.007216374855488539\n",
      "Epoch:5 \n",
      "Iteration:253 \n",
      "Loss:0.021638037636876106\n",
      "Epoch:5 \n",
      "Iteration:254 \n",
      "Loss:0.0780341699719429\n",
      "Epoch:5 \n",
      "Iteration:255 \n",
      "Loss:0.02714572474360466\n",
      "Epoch:5 \n",
      "Iteration:256 \n",
      "Loss:0.010995647870004177\n",
      "Epoch:5 \n",
      "Iteration:257 \n",
      "Loss:0.016640473157167435\n",
      "Epoch:5 \n",
      "Iteration:258 \n",
      "Loss:0.024537472054362297\n",
      "Epoch:5 \n",
      "Iteration:259 \n",
      "Loss:0.056220997124910355\n",
      "Epoch:5 \n",
      "Iteration:260 \n",
      "Loss:0.014782808721065521\n",
      "Epoch:5 \n",
      "Iteration:261 \n",
      "Loss:0.04132203012704849\n",
      "Epoch:5 \n",
      "Iteration:262 \n",
      "Loss:0.05179990828037262\n",
      "Epoch:5 \n",
      "Iteration:263 \n",
      "Loss:0.04839722067117691\n",
      "Epoch:5 \n",
      "Iteration:264 \n",
      "Loss:0.08021727204322815\n",
      "Epoch:5 \n",
      "Iteration:265 \n",
      "Loss:0.22404035925865173\n",
      "Epoch:5 \n",
      "Iteration:266 \n",
      "Loss:0.004304721485823393\n",
      "Epoch:5 \n",
      "Iteration:267 \n",
      "Loss:0.009333536028862\n",
      "Epoch:5 \n",
      "Iteration:268 \n",
      "Loss:0.04572006314992905\n",
      "Epoch:5 \n",
      "Iteration:269 \n",
      "Loss:0.0029876308981329203\n",
      "Epoch:5 \n",
      "Iteration:270 \n",
      "Loss:0.06083708256483078\n",
      "Epoch:5 \n",
      "Iteration:271 \n",
      "Loss:0.06141042709350586\n",
      "Epoch:5 \n",
      "Iteration:272 \n",
      "Loss:0.09740705788135529\n",
      "Epoch:5 \n",
      "Iteration:273 \n",
      "Loss:0.04087788984179497\n",
      "Epoch:5 \n",
      "Iteration:274 \n",
      "Loss:0.04284539818763733\n",
      "Epoch:5 \n",
      "Iteration:275 \n",
      "Loss:0.08911549299955368\n",
      "Epoch:5 \n",
      "Iteration:276 \n",
      "Loss:0.054368551820516586\n",
      "Epoch:5 \n",
      "Iteration:277 \n",
      "Loss:0.051383983343839645\n",
      "Epoch:5 \n",
      "Iteration:278 \n",
      "Loss:0.043430395424366\n",
      "Epoch:5 \n",
      "Iteration:279 \n",
      "Loss:0.021310005336999893\n",
      "Epoch:5 \n",
      "Iteration:280 \n",
      "Loss:0.023348279297351837\n",
      "Epoch:5 \n",
      "Iteration:281 \n",
      "Loss:0.04596295952796936\n",
      "Epoch:5 \n",
      "Iteration:282 \n",
      "Loss:0.006715267896652222\n",
      "Epoch:5 \n",
      "Iteration:283 \n",
      "Loss:0.004026536364108324\n",
      "Epoch:5 \n",
      "Iteration:284 \n",
      "Loss:0.028221596032381058\n",
      "Epoch:5 \n",
      "Iteration:285 \n",
      "Loss:0.080050989985466\n",
      "Epoch:5 \n",
      "Iteration:286 \n",
      "Loss:0.0425339974462986\n",
      "Epoch:5 \n",
      "Iteration:287 \n",
      "Loss:0.006741777062416077\n",
      "Epoch:5 \n",
      "Iteration:288 \n",
      "Loss:0.023670291528105736\n",
      "Epoch:5 \n",
      "Iteration:289 \n",
      "Loss:0.022853979840874672\n",
      "Epoch:5 \n",
      "Iteration:290 \n",
      "Loss:0.0547621063888073\n",
      "Epoch:5 \n",
      "Iteration:291 \n",
      "Loss:0.03657141700387001\n",
      "Epoch:5 \n",
      "Iteration:292 \n",
      "Loss:0.028854265809059143\n",
      "Epoch:5 \n",
      "Iteration:293 \n",
      "Loss:0.014157739467918873\n",
      "Epoch:5 \n",
      "Iteration:294 \n",
      "Loss:0.048996973782777786\n",
      "Epoch:5 \n",
      "Iteration:295 \n",
      "Loss:0.07205921411514282\n",
      "Epoch:5 \n",
      "Iteration:296 \n",
      "Loss:0.07057604938745499\n",
      "Epoch:5 \n",
      "Iteration:297 \n",
      "Loss:0.03098134696483612\n",
      "Epoch:5 \n",
      "Iteration:298 \n",
      "Loss:0.014943258836865425\n",
      "Epoch:5 \n",
      "Iteration:299 \n",
      "Loss:0.005203001666814089\n",
      "Epoch:5 \n",
      "Iteration:300 \n",
      "Loss:0.09293577075004578\n",
      "Epoch:5 \n",
      "Iteration:301 \n",
      "Loss:0.179232656955719\n",
      "Epoch:5 \n",
      "Iteration:302 \n",
      "Loss:0.0018876743270084262\n",
      "Epoch:5 \n",
      "Iteration:303 \n",
      "Loss:0.010010268539190292\n",
      "Epoch:5 \n",
      "Iteration:304 \n",
      "Loss:0.014469531364738941\n",
      "Epoch:5 \n",
      "Iteration:305 \n",
      "Loss:0.025282375514507294\n",
      "Epoch:5 \n",
      "Iteration:306 \n",
      "Loss:0.015078680589795113\n",
      "Epoch:5 \n",
      "Iteration:307 \n",
      "Loss:0.019649401307106018\n",
      "Epoch:5 \n",
      "Iteration:308 \n",
      "Loss:0.007615830283612013\n",
      "Epoch:5 \n",
      "Iteration:309 \n",
      "Loss:0.003454160410910845\n",
      "Epoch:5 \n",
      "Iteration:310 \n",
      "Loss:0.048382796347141266\n",
      "Epoch:5 \n",
      "Iteration:311 \n",
      "Loss:0.04584045708179474\n",
      "Epoch:5 \n",
      "Iteration:312 \n",
      "Loss:0.043960265815258026\n",
      "Epoch:5 \n",
      "Iteration:313 \n",
      "Loss:0.05136370658874512\n",
      "Epoch:5 \n",
      "Iteration:314 \n",
      "Loss:0.014082192443311214\n",
      "Epoch:5 \n",
      "Iteration:315 \n",
      "Loss:0.047573547810316086\n",
      "Epoch:5 \n",
      "Iteration:316 \n",
      "Loss:0.012408256530761719\n",
      "Epoch:5 \n",
      "Iteration:317 \n",
      "Loss:0.06963067501783371\n",
      "Epoch:5 \n",
      "Iteration:318 \n",
      "Loss:0.02974516898393631\n",
      "Epoch:5 \n",
      "Iteration:319 \n",
      "Loss:0.05822201818227768\n",
      "Epoch:5 \n",
      "Iteration:320 \n",
      "Loss:0.08304069191217422\n",
      "Epoch:5 \n",
      "Iteration:321 \n",
      "Loss:0.030409004539251328\n",
      "Epoch:5 \n",
      "Iteration:322 \n",
      "Loss:0.07946910709142685\n",
      "Epoch:5 \n",
      "Iteration:323 \n",
      "Loss:0.04857013374567032\n",
      "Epoch:5 \n",
      "Iteration:324 \n",
      "Loss:0.10957178473472595\n",
      "Epoch:5 \n",
      "Iteration:325 \n",
      "Loss:0.009823182597756386\n",
      "Epoch:5 \n",
      "Iteration:326 \n",
      "Loss:0.005253791343420744\n",
      "Epoch:5 \n",
      "Iteration:327 \n",
      "Loss:0.10247427970170975\n",
      "Epoch:5 \n",
      "Iteration:328 \n",
      "Loss:0.05277135223150253\n",
      "Epoch:5 \n",
      "Iteration:329 \n",
      "Loss:0.007127128075808287\n",
      "Epoch:5 \n",
      "Iteration:330 \n",
      "Loss:0.04397985339164734\n",
      "Epoch:5 \n",
      "Iteration:331 \n",
      "Loss:0.016715778037905693\n",
      "Epoch:5 \n",
      "Iteration:332 \n",
      "Loss:0.04088862985372543\n",
      "Epoch:5 \n",
      "Iteration:333 \n",
      "Loss:0.17076756060123444\n",
      "Epoch:5 \n",
      "Iteration:334 \n",
      "Loss:0.010469253174960613\n",
      "Epoch:5 \n",
      "Iteration:335 \n",
      "Loss:0.00333763868547976\n",
      "Epoch:5 \n",
      "Iteration:336 \n",
      "Loss:0.05751198157668114\n",
      "Epoch:5 \n",
      "Iteration:337 \n",
      "Loss:0.0816745012998581\n",
      "Epoch:5 \n",
      "Iteration:338 \n",
      "Loss:0.0029237940907478333\n",
      "Epoch:5 \n",
      "Iteration:339 \n",
      "Loss:0.05111284554004669\n",
      "Epoch:5 \n",
      "Iteration:340 \n",
      "Loss:0.09387700259685516\n",
      "Epoch:5 \n",
      "Iteration:341 \n",
      "Loss:0.03929232060909271\n",
      "Epoch:5 \n",
      "Iteration:342 \n",
      "Loss:0.00708002271130681\n",
      "Epoch:5 \n",
      "Iteration:343 \n",
      "Loss:0.062146686017513275\n",
      "Epoch:5 \n",
      "Iteration:344 \n",
      "Loss:0.013858905993402004\n",
      "Epoch:5 \n",
      "Iteration:345 \n",
      "Loss:0.09932363778352737\n",
      "Epoch:5 \n",
      "Iteration:346 \n",
      "Loss:0.03540554642677307\n",
      "Epoch:5 \n",
      "Iteration:347 \n",
      "Loss:0.013138684444129467\n",
      "Epoch:5 \n",
      "Iteration:348 \n",
      "Loss:0.12957780063152313\n",
      "Epoch:5 \n",
      "Iteration:349 \n",
      "Loss:0.015373778529465199\n",
      "Epoch:5 \n",
      "Iteration:350 \n",
      "Loss:0.05264896899461746\n",
      "Epoch:5 \n",
      "Iteration:351 \n",
      "Loss:0.08456496149301529\n",
      "Epoch:5 \n",
      "Iteration:352 \n",
      "Loss:0.006332049146294594\n",
      "Epoch:5 \n",
      "Iteration:353 \n",
      "Loss:0.046433161944150925\n",
      "Epoch:5 \n",
      "Iteration:354 \n",
      "Loss:0.043002448976039886\n",
      "Epoch:5 \n",
      "Iteration:355 \n",
      "Loss:0.028477050364017487\n",
      "Epoch:5 \n",
      "Iteration:356 \n",
      "Loss:0.12587451934814453\n",
      "Epoch:5 \n",
      "Iteration:357 \n",
      "Loss:0.10533547401428223\n",
      "Epoch:5 \n",
      "Iteration:358 \n",
      "Loss:0.06571564823389053\n",
      "Epoch:5 \n",
      "Iteration:359 \n",
      "Loss:0.019195809960365295\n",
      "Epoch:5 \n",
      "Iteration:360 \n",
      "Loss:0.045375071465969086\n",
      "Epoch:5 \n",
      "Iteration:361 \n",
      "Loss:0.10289410501718521\n",
      "Epoch:5 \n",
      "Iteration:362 \n",
      "Loss:0.028914734721183777\n",
      "Epoch:5 \n",
      "Iteration:363 \n",
      "Loss:0.008770766668021679\n",
      "Epoch:5 \n",
      "Iteration:364 \n",
      "Loss:0.1241755411028862\n",
      "Epoch:5 \n",
      "Iteration:365 \n",
      "Loss:0.00492456741631031\n",
      "Epoch:5 \n",
      "Iteration:366 \n",
      "Loss:0.06998497992753983\n",
      "Epoch:5 \n",
      "Iteration:367 \n",
      "Loss:0.025064587593078613\n",
      "Epoch:5 \n",
      "Iteration:368 \n",
      "Loss:0.10906746983528137\n",
      "Epoch:5 \n",
      "Iteration:369 \n",
      "Loss:0.03760945424437523\n",
      "Epoch:5 \n",
      "Iteration:370 \n",
      "Loss:0.008554023690521717\n",
      "Epoch:5 \n",
      "Iteration:371 \n",
      "Loss:0.1067260280251503\n",
      "Epoch:5 \n",
      "Iteration:372 \n",
      "Loss:0.02043270133435726\n",
      "Epoch:5 \n",
      "Iteration:373 \n",
      "Loss:0.042824264615774155\n",
      "Epoch:5 \n",
      "Iteration:374 \n",
      "Loss:0.07117931544780731\n",
      "Epoch:5 \n",
      "Iteration:375 \n",
      "Loss:0.023089684545993805\n",
      "Epoch:5 \n",
      "Iteration:376 \n",
      "Loss:0.028763189911842346\n",
      "Epoch:5 \n",
      "Iteration:377 \n",
      "Loss:0.024829117581248283\n",
      "Epoch:5 \n",
      "Iteration:378 \n",
      "Loss:0.0016516378382220864\n",
      "Epoch:5 \n",
      "Iteration:379 \n",
      "Loss:0.02502274513244629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:5 \n",
      "Iteration:380 \n",
      "Loss:0.17466101050376892\n",
      "Epoch:5 \n",
      "Iteration:381 \n",
      "Loss:0.023437142372131348\n",
      "Epoch:5 \n",
      "Iteration:382 \n",
      "Loss:0.03697950392961502\n",
      "Epoch:5 \n",
      "Iteration:383 \n",
      "Loss:0.1192232295870781\n",
      "Epoch:5 \n",
      "Iteration:384 \n",
      "Loss:0.10674715042114258\n",
      "Epoch:5 \n",
      "Iteration:385 \n",
      "Loss:0.06372912228107452\n",
      "Epoch:5 \n",
      "Iteration:386 \n",
      "Loss:0.029676303267478943\n",
      "Epoch:5 \n",
      "Iteration:387 \n",
      "Loss:0.039401840418577194\n",
      "Epoch:5 \n",
      "Iteration:388 \n",
      "Loss:0.04313652962446213\n",
      "Epoch:5 \n",
      "Iteration:389 \n",
      "Loss:0.04242105409502983\n",
      "Epoch:5 \n",
      "Iteration:390 \n",
      "Loss:0.046509504318237305\n",
      "Epoch:5 \n",
      "Iteration:391 \n",
      "Loss:0.012387670576572418\n",
      "Epoch:5 \n",
      "Iteration:392 \n",
      "Loss:0.004667161498218775\n",
      "Epoch:5 \n",
      "Iteration:393 \n",
      "Loss:0.00928991287946701\n",
      "Epoch:5 \n",
      "Iteration:394 \n",
      "Loss:0.08838345855474472\n",
      "Epoch:5 \n",
      "Iteration:395 \n",
      "Loss:0.024059347808361053\n",
      "Epoch:5 \n",
      "Iteration:396 \n",
      "Loss:0.05232882499694824\n",
      "Epoch:5 \n",
      "Iteration:397 \n",
      "Loss:0.07347489148378372\n",
      "Epoch:5 \n",
      "Iteration:398 \n",
      "Loss:0.04029689356684685\n",
      "Epoch:5 \n",
      "Iteration:399 \n",
      "Loss:0.051091793924570084\n",
      "Epoch:5 \n",
      "Iteration:400 \n",
      "Loss:0.08678067475557327\n",
      "Epoch:5 \n",
      "Iteration:401 \n",
      "Loss:0.005495588295161724\n",
      "Epoch:5 \n",
      "Iteration:402 \n",
      "Loss:0.05553276836872101\n",
      "Epoch:5 \n",
      "Iteration:403 \n",
      "Loss:0.03457173332571983\n",
      "Epoch:5 \n",
      "Iteration:404 \n",
      "Loss:0.031582266092300415\n",
      "Epoch:5 \n",
      "Iteration:405 \n",
      "Loss:0.04680706933140755\n",
      "Epoch:5 \n",
      "Iteration:406 \n",
      "Loss:0.054624851793050766\n",
      "Epoch:5 \n",
      "Iteration:407 \n",
      "Loss:0.05530458316206932\n",
      "Epoch:5 \n",
      "Iteration:408 \n",
      "Loss:0.06197701394557953\n",
      "Epoch:5 \n",
      "Iteration:409 \n",
      "Loss:0.0868837758898735\n",
      "Epoch:5 \n",
      "Iteration:410 \n",
      "Loss:0.11450186371803284\n",
      "Epoch:5 \n",
      "Iteration:411 \n",
      "Loss:0.03578885272145271\n",
      "Epoch:5 \n",
      "Iteration:412 \n",
      "Loss:0.0013503247173503041\n",
      "Epoch:5 \n",
      "Iteration:413 \n",
      "Loss:0.031604208052158356\n",
      "Epoch:5 \n",
      "Iteration:414 \n",
      "Loss:0.0172459464520216\n",
      "Epoch:5 \n",
      "Iteration:415 \n",
      "Loss:0.12543347477912903\n",
      "Epoch:5 \n",
      "Iteration:416 \n",
      "Loss:0.0702885314822197\n",
      "Epoch:5 \n",
      "Iteration:417 \n",
      "Loss:0.03554667532444\n",
      "Epoch:5 \n",
      "Iteration:418 \n",
      "Loss:0.08244433999061584\n",
      "Epoch:5 \n",
      "Iteration:419 \n",
      "Loss:0.04671969264745712\n",
      "Epoch:5 \n",
      "Iteration:420 \n",
      "Loss:0.05437220633029938\n",
      "Epoch:5 \n",
      "Iteration:421 \n",
      "Loss:0.05023599788546562\n",
      "Epoch:5 \n",
      "Iteration:422 \n",
      "Loss:0.06934364140033722\n",
      "Epoch:5 \n",
      "Iteration:423 \n",
      "Loss:0.041576795279979706\n",
      "Epoch:5 \n",
      "Iteration:424 \n",
      "Loss:0.013442312367260456\n",
      "Epoch:5 \n",
      "Iteration:425 \n",
      "Loss:0.06820803880691528\n",
      "Epoch:5 \n",
      "Iteration:426 \n",
      "Loss:0.0948287770152092\n",
      "Epoch:5 \n",
      "Iteration:427 \n",
      "Loss:0.08503484725952148\n",
      "Epoch:5 \n",
      "Iteration:428 \n",
      "Loss:0.04229574277997017\n",
      "Epoch:5 \n",
      "Iteration:429 \n",
      "Loss:0.025745704770088196\n",
      "Epoch:5 \n",
      "Iteration:430 \n",
      "Loss:0.032654061913490295\n",
      "Epoch:5 \n",
      "Iteration:431 \n",
      "Loss:0.01710253767669201\n",
      "Epoch:5 \n",
      "Iteration:432 \n",
      "Loss:0.020856276154518127\n",
      "Epoch:5 \n",
      "Iteration:433 \n",
      "Loss:0.0195842906832695\n",
      "Epoch:5 \n",
      "Iteration:434 \n",
      "Loss:0.04392298310995102\n",
      "Epoch:5 \n",
      "Iteration:435 \n",
      "Loss:0.07840470224618912\n",
      "Epoch:5 \n",
      "Iteration:436 \n",
      "Loss:0.038389235734939575\n",
      "Epoch:5 \n",
      "Iteration:437 \n",
      "Loss:0.03839346393942833\n",
      "Epoch:5 \n",
      "Iteration:438 \n",
      "Loss:0.08290597051382065\n",
      "Epoch:5 \n",
      "Iteration:439 \n",
      "Loss:0.02240535244345665\n",
      "Epoch:5 \n",
      "Iteration:440 \n",
      "Loss:0.055692605674266815\n",
      "Epoch:5 \n",
      "Iteration:441 \n",
      "Loss:0.027674470096826553\n",
      "Epoch:5 \n",
      "Iteration:442 \n",
      "Loss:0.022056199610233307\n",
      "Epoch:5 \n",
      "Iteration:443 \n",
      "Loss:0.026656026020646095\n",
      "Epoch:5 \n",
      "Iteration:444 \n",
      "Loss:0.028085336089134216\n",
      "Epoch:5 \n",
      "Iteration:445 \n",
      "Loss:0.011384980753064156\n",
      "Epoch:5 \n",
      "Iteration:446 \n",
      "Loss:0.05817548185586929\n",
      "Epoch:5 \n",
      "Iteration:447 \n",
      "Loss:0.04625573009252548\n",
      "Epoch:5 \n",
      "Iteration:448 \n",
      "Loss:0.03665824979543686\n",
      "Epoch:5 \n",
      "Iteration:449 \n",
      "Loss:0.043323710560798645\n",
      "Epoch:5 \n",
      "Iteration:450 \n",
      "Loss:0.017426228150725365\n",
      "Epoch:5 \n",
      "Iteration:451 \n",
      "Loss:0.0365583710372448\n",
      "Epoch:5 \n",
      "Iteration:452 \n",
      "Loss:0.05465990677475929\n",
      "Epoch:5 \n",
      "Iteration:453 \n",
      "Loss:0.07669070363044739\n",
      "Epoch:5 \n",
      "Iteration:454 \n",
      "Loss:0.09443493187427521\n",
      "Epoch:5 \n",
      "Iteration:455 \n",
      "Loss:0.08030182123184204\n",
      "Epoch:5 \n",
      "Iteration:456 \n",
      "Loss:0.04013628140091896\n",
      "Epoch:5 \n",
      "Iteration:457 \n",
      "Loss:0.06333023309707642\n",
      "Epoch:5 \n",
      "Iteration:458 \n",
      "Loss:0.041470542550086975\n",
      "Epoch:5 \n",
      "Iteration:459 \n",
      "Loss:0.010485414415597916\n",
      "Epoch:5 \n",
      "Iteration:460 \n",
      "Loss:0.024206167086958885\n",
      "Epoch:5 \n",
      "Iteration:461 \n",
      "Loss:0.007060531992465258\n",
      "Epoch:5 \n",
      "Iteration:462 \n",
      "Loss:0.01706262119114399\n",
      "Epoch:5 \n",
      "Iteration:463 \n",
      "Loss:0.19753897190093994\n",
      "Epoch:5 \n",
      "Iteration:464 \n",
      "Loss:0.04516153410077095\n",
      "Epoch:5 \n",
      "Iteration:465 \n",
      "Loss:0.06909158825874329\n",
      "Epoch:5 \n",
      "Iteration:466 \n",
      "Loss:0.03989846631884575\n",
      "Epoch:5 \n",
      "Iteration:467 \n",
      "Loss:0.03725603222846985\n",
      "Epoch:5 \n",
      "Iteration:468 \n",
      "Loss:0.03270873799920082\n",
      "Epoch:5 \n",
      "Iteration:469 \n",
      "Loss:0.12758269906044006\n",
      "Epoch:5 \n",
      "Iteration:470 \n",
      "Loss:0.02676538936793804\n",
      "Epoch:5 \n",
      "Iteration:471 \n",
      "Loss:0.14908429980278015\n",
      "Epoch:5 \n",
      "Iteration:472 \n",
      "Loss:0.036945320665836334\n",
      "Epoch:5 \n",
      "Iteration:473 \n",
      "Loss:0.031125718727707863\n",
      "Epoch:5 \n",
      "Iteration:474 \n",
      "Loss:0.03110155276954174\n",
      "Epoch:5 \n",
      "Iteration:475 \n",
      "Loss:0.024923808872699738\n",
      "Epoch:5 \n",
      "Iteration:476 \n",
      "Loss:0.008140339516103268\n",
      "Epoch:5 \n",
      "Iteration:477 \n",
      "Loss:0.013475959189236164\n",
      "Epoch:5 \n",
      "Iteration:478 \n",
      "Loss:0.022068817168474197\n",
      "Epoch:5 \n",
      "Iteration:479 \n",
      "Loss:0.021191684529185295\n",
      "Epoch:5 \n",
      "Iteration:480 \n",
      "Loss:0.10442979633808136\n",
      "Epoch:5 \n",
      "Iteration:481 \n",
      "Loss:0.0060240221209824085\n",
      "Epoch:5 \n",
      "Iteration:482 \n",
      "Loss:0.022857988253235817\n",
      "Epoch:5 \n",
      "Iteration:483 \n",
      "Loss:0.04332411661744118\n",
      "Epoch:5 \n",
      "Iteration:484 \n",
      "Loss:0.11777931451797485\n",
      "Epoch:5 \n",
      "Iteration:485 \n",
      "Loss:0.12377697974443436\n",
      "Epoch:5 \n",
      "Iteration:486 \n",
      "Loss:0.04110099375247955\n",
      "Epoch:5 \n",
      "Iteration:487 \n",
      "Loss:0.039203811436891556\n",
      "Epoch:5 \n",
      "Iteration:488 \n",
      "Loss:0.05967998132109642\n",
      "Epoch:5 \n",
      "Iteration:489 \n",
      "Loss:0.003850245149806142\n",
      "Epoch:5 \n",
      "Iteration:490 \n",
      "Loss:0.04961865767836571\n",
      "Epoch:5 \n",
      "Iteration:491 \n",
      "Loss:0.02007894590497017\n",
      "Epoch:5 \n",
      "Iteration:492 \n",
      "Loss:0.0012080129235982895\n",
      "Epoch:5 \n",
      "Iteration:493 \n",
      "Loss:0.06124259531497955\n",
      "Epoch:5 \n",
      "Iteration:494 \n",
      "Loss:0.02832639031112194\n",
      "Epoch:5 \n",
      "Iteration:495 \n",
      "Loss:0.050372667610645294\n",
      "Epoch:5 \n",
      "Iteration:496 \n",
      "Loss:0.009329910390079021\n",
      "Epoch:5 \n",
      "Iteration:497 \n",
      "Loss:0.058764442801475525\n",
      "Epoch:5 \n",
      "Iteration:498 \n",
      "Loss:0.06463534384965897\n",
      "Epoch:5 \n",
      "Iteration:499 \n",
      "Loss:0.030944380909204483\n",
      "Epoch:5 \n",
      "Iteration:500 \n",
      "Loss:0.026387058198451996\n",
      "Epoch:5 \n",
      "Iteration:501 \n",
      "Loss:0.07025323808193207\n",
      "Epoch:5 \n",
      "Iteration:502 \n",
      "Loss:0.0074350880458951\n",
      "Epoch:5 \n",
      "Iteration:503 \n",
      "Loss:0.031213246285915375\n",
      "Epoch:5 \n",
      "Iteration:504 \n",
      "Loss:0.009925741702318192\n",
      "Epoch:5 \n",
      "Iteration:505 \n",
      "Loss:0.1001056581735611\n",
      "Epoch:5 \n",
      "Iteration:506 \n",
      "Loss:0.16858603060245514\n",
      "Epoch:5 \n",
      "Iteration:507 \n",
      "Loss:0.011763614602386951\n",
      "Epoch:5 \n",
      "Iteration:508 \n",
      "Loss:0.07400573045015335\n",
      "Epoch:5 \n",
      "Iteration:509 \n",
      "Loss:0.0965205579996109\n",
      "Epoch:5 \n",
      "Iteration:510 \n",
      "Loss:0.05325038731098175\n",
      "Epoch:5 \n",
      "Iteration:511 \n",
      "Loss:0.10820859670639038\n",
      "Epoch:5 \n",
      "Iteration:512 \n",
      "Loss:0.013834051787853241\n",
      "Epoch:5 \n",
      "Iteration:513 \n",
      "Loss:0.00935860350728035\n",
      "Epoch:5 \n",
      "Iteration:514 \n",
      "Loss:0.03320690989494324\n",
      "Epoch:5 \n",
      "Iteration:515 \n",
      "Loss:0.05758867785334587\n",
      "Epoch:5 \n",
      "Iteration:516 \n",
      "Loss:0.0857243537902832\n",
      "Epoch:5 \n",
      "Iteration:517 \n",
      "Loss:0.007965043187141418\n",
      "Epoch:5 \n",
      "Iteration:518 \n",
      "Loss:0.09572593867778778\n",
      "Epoch:5 \n",
      "Iteration:519 \n",
      "Loss:0.04329109564423561\n",
      "Epoch:5 \n",
      "Iteration:520 \n",
      "Loss:0.012259460985660553\n",
      "Epoch:5 \n",
      "Iteration:521 \n",
      "Loss:0.02906518243253231\n",
      "Epoch:5 \n",
      "Iteration:522 \n",
      "Loss:0.03238600119948387\n",
      "Epoch:5 \n",
      "Iteration:523 \n",
      "Loss:0.07829958945512772\n",
      "Epoch:5 \n",
      "Iteration:524 \n",
      "Loss:0.08853375166654587\n",
      "Epoch:5 \n",
      "Iteration:525 \n",
      "Loss:0.03739073872566223\n",
      "Epoch:5 \n",
      "Iteration:526 \n",
      "Loss:0.009950990788638592\n",
      "Epoch:5 \n",
      "Iteration:527 \n",
      "Loss:0.028542401269078255\n",
      "Epoch:5 \n",
      "Iteration:528 \n",
      "Loss:0.04543047398328781\n",
      "Epoch:5 \n",
      "Iteration:529 \n",
      "Loss:0.036084841936826706\n",
      "Epoch:5 \n",
      "Iteration:530 \n",
      "Loss:0.020066287368535995\n",
      "Epoch:5 \n",
      "Iteration:531 \n",
      "Loss:0.01566131040453911\n",
      "Epoch:5 \n",
      "Iteration:532 \n",
      "Loss:0.06999991834163666\n",
      "Epoch:5 \n",
      "Iteration:533 \n",
      "Loss:0.035319503396749496\n",
      "Epoch:5 \n",
      "Iteration:534 \n",
      "Loss:0.13384738564491272\n",
      "Epoch:5 \n",
      "Iteration:535 \n",
      "Loss:0.05361543223261833\n",
      "Epoch:5 \n",
      "Iteration:536 \n",
      "Loss:0.041168030351400375\n",
      "Epoch:5 \n",
      "Iteration:537 \n",
      "Loss:0.09533627331256866\n",
      "Epoch:5 \n",
      "Iteration:538 \n",
      "Loss:0.02034333162009716\n",
      "Epoch:5 \n",
      "Iteration:539 \n",
      "Loss:0.023726437240839005\n",
      "Epoch:5 \n",
      "Iteration:540 \n",
      "Loss:0.02835550159215927\n",
      "Epoch:5 \n",
      "Iteration:541 \n",
      "Loss:0.009913735091686249\n",
      "Epoch:5 \n",
      "Iteration:542 \n",
      "Loss:0.03806377574801445\n",
      "Epoch:5 \n",
      "Iteration:543 \n",
      "Loss:0.06259583681821823\n",
      "Epoch:5 \n",
      "Iteration:544 \n",
      "Loss:0.031196046620607376\n",
      "Epoch:5 \n",
      "Iteration:545 \n",
      "Loss:0.031106457114219666\n",
      "Epoch:5 \n",
      "Iteration:546 \n",
      "Loss:0.10589409619569778\n",
      "Epoch:5 \n",
      "Iteration:547 \n",
      "Loss:0.08020887523889542\n",
      "Epoch:5 \n",
      "Iteration:548 \n",
      "Loss:0.045092593878507614\n",
      "Epoch:5 \n",
      "Iteration:549 \n",
      "Loss:0.05215921998023987\n",
      "Epoch:5 \n",
      "Iteration:550 \n",
      "Loss:0.016143472865223885\n",
      "Epoch:5 \n",
      "Iteration:551 \n",
      "Loss:0.00945336651057005\n",
      "Epoch:5 \n",
      "Iteration:552 \n",
      "Loss:0.06134295463562012\n",
      "Epoch:5 \n",
      "Iteration:553 \n",
      "Loss:0.034022457897663116\n",
      "Epoch:5 \n",
      "Iteration:554 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.010873165912926197\n",
      "Epoch:5 \n",
      "Iteration:555 \n",
      "Loss:0.018323291093111038\n",
      "Epoch:5 \n",
      "Iteration:556 \n",
      "Loss:0.020297009497880936\n",
      "Epoch:5 \n",
      "Iteration:557 \n",
      "Loss:0.010244459845125675\n",
      "Epoch:5 \n",
      "Iteration:558 \n",
      "Loss:0.037558481097221375\n",
      "Epoch:5 \n",
      "Iteration:559 \n",
      "Loss:0.01061056274920702\n",
      "Epoch:5 \n",
      "Iteration:560 \n",
      "Loss:0.04536796733736992\n",
      "Epoch:5 \n",
      "Iteration:561 \n",
      "Loss:0.08902322500944138\n",
      "Epoch:5 \n",
      "Iteration:562 \n",
      "Loss:0.04351144656538963\n",
      "Epoch:5 \n",
      "Iteration:563 \n",
      "Loss:0.04123517870903015\n",
      "Epoch:5 \n",
      "Iteration:564 \n",
      "Loss:0.03861682862043381\n",
      "Epoch:5 \n",
      "Iteration:565 \n",
      "Loss:0.04593491554260254\n",
      "Epoch:5 \n",
      "Iteration:566 \n",
      "Loss:0.1014302670955658\n",
      "Epoch:5 \n",
      "Iteration:567 \n",
      "Loss:0.052195657044649124\n",
      "Epoch:5 \n",
      "Iteration:568 \n",
      "Loss:0.015729490667581558\n",
      "Epoch:5 \n",
      "Iteration:569 \n",
      "Loss:0.010034353472292423\n",
      "Epoch:5 \n",
      "Iteration:570 \n",
      "Loss:0.11708416044712067\n",
      "Epoch:5 \n",
      "Iteration:571 \n",
      "Loss:0.1108635738492012\n",
      "Epoch:5 \n",
      "Iteration:572 \n",
      "Loss:0.056082434952259064\n",
      "Epoch:5 \n",
      "Iteration:573 \n",
      "Loss:0.012722469866275787\n",
      "Epoch:5 \n",
      "Iteration:574 \n",
      "Loss:0.06322310119867325\n",
      "Epoch:5 \n",
      "Iteration:575 \n",
      "Loss:0.019840845838189125\n",
      "Epoch:5 \n",
      "Iteration:576 \n",
      "Loss:0.06076742708683014\n",
      "Epoch:5 \n",
      "Iteration:577 \n",
      "Loss:0.02260960079729557\n",
      "Epoch:5 \n",
      "Iteration:578 \n",
      "Loss:0.032762885093688965\n",
      "Epoch:5 \n",
      "Iteration:579 \n",
      "Loss:0.0068029360845685005\n",
      "Epoch:5 \n",
      "Iteration:580 \n",
      "Loss:0.05213358998298645\n",
      "Epoch:5 \n",
      "Iteration:581 \n",
      "Loss:0.013620147481560707\n",
      "Epoch:5 \n",
      "Iteration:582 \n",
      "Loss:0.04285508021712303\n",
      "Epoch:5 \n",
      "Iteration:583 \n",
      "Loss:0.04328902065753937\n",
      "Epoch:5 \n",
      "Iteration:584 \n",
      "Loss:0.0504206120967865\n",
      "Epoch:5 \n",
      "Iteration:585 \n",
      "Loss:0.03718406707048416\n",
      "Epoch:5 \n",
      "Iteration:586 \n",
      "Loss:0.0032064556144177914\n",
      "Epoch:5 \n",
      "Iteration:587 \n",
      "Loss:0.00622851587831974\n",
      "Epoch:5 \n",
      "Iteration:588 \n",
      "Loss:0.08114097267389297\n",
      "Epoch:5 \n",
      "Iteration:589 \n",
      "Loss:0.03547229617834091\n",
      "Epoch:5 \n",
      "Iteration:590 \n",
      "Loss:0.006339258514344692\n",
      "Epoch:5 \n",
      "Iteration:591 \n",
      "Loss:0.1124071404337883\n",
      "Epoch:5 \n",
      "Iteration:592 \n",
      "Loss:0.07256704568862915\n",
      "Epoch:5 \n",
      "Iteration:593 \n",
      "Loss:0.09187868982553482\n",
      "Epoch:5 \n",
      "Iteration:594 \n",
      "Loss:0.04057544097304344\n",
      "Epoch:5 \n",
      "Iteration:595 \n",
      "Loss:0.11772045493125916\n",
      "Epoch:5 \n",
      "Iteration:596 \n",
      "Loss:0.052355241030454636\n",
      "Epoch:5 \n",
      "Iteration:597 \n",
      "Loss:0.008064932189881802\n",
      "Epoch:5 \n",
      "Iteration:598 \n",
      "Loss:0.017516866326332092\n",
      "Epoch:5 \n",
      "Iteration:599 \n",
      "Loss:0.0536358505487442\n",
      "Epoch:5 \n",
      "Iteration:600 \n",
      "Loss:0.0071176690980792046\n",
      "\n",
      "Accuracy of network in epoch 5: 98.695\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, test_loader, criterion, optimizer, epochs = 5):\n",
    "    accuraccy_list = []\n",
    "    for epoch in range(epochs):\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for i, data in enumerate(train_loader):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            writer.add_scalar(\"Loss/train\", loss, epoch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f'Epoch:{epoch + 1} \\nIteration:{i + 1} \\nLoss:{loss}')\n",
    "            with torch.no_grad():\n",
    "                total += labels.size(0)\n",
    "                _,prediction = torch.max(outputs, 1)\n",
    "                correct += (prediction == labels).sum().item()\n",
    "        print(f'\\nAccuracy of network in epoch {epoch + 1}: {100 * correct / total}')\n",
    "    writer.flush()\n",
    "\n",
    "train_model(model, train_loader, test_loader, criterion, optimizer)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network:98.02\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "for data, labels in test_loader:\n",
    "    data = data.to(torch.device(\"cuda:0\"))\n",
    "    with torch.no_grad():\n",
    "        validation = model(data)\n",
    "        _,prediction = torch.max(validation, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (prediction.cpu() == labels).sum().item()\n",
    "    \n",
    "print(f'Accuracy of the network:{100 * correct / total}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2.0",
   "language": "python",
   "name": "pytorch2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
